//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_75
.address_size 64

	// .globl	fill
.global .align 4 .u32 SharedMemorySize = 32768;
.global .align 4 .u32 BLOCK_DIM = 32;
.const .align 2 .b8 sh[26];
// _ZZ12MatMulKernelE8blockElt has been demoted
// _ZZ12MatMulKernelE9blockxInd has been demoted
// _ZZ12MatMulKernelE9blockyInd has been demoted
// _ZZ12MatMulKernelE1b has been demoted
// _ZZ13MatMulKernelTE8blockElt has been demoted
// _ZZ13MatMulKernelTE9blockxInd has been demoted
// _ZZ13MatMulKernelTE9blockyInd has been demoted
// _ZZ13MatMulKernelTE1b has been demoted
// _ZZ19sharedMem_transposeE8M_Shared has been demoted
// _ZZ33matrixTransposeSolveBankConflictsE3mat has been demoted
// _ZZ11transposeV3E3s_A has been demoted

.visible .entry fill(
	.param .u64 fill_param_0,
	.param .align 2 .b8 fill_param_1[2],
	.param .u32 fill_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs1, [fill_param_1];
	ld.param.u64 	%rd1, [fill_param_0];
	ld.param.u32 	%r2, [fill_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u16 	[%rd4], %rs1;

$L__BB0_2:
	ret;

}
	// .globl	gelu
.visible .entry gelu(
	.param .u64 gelu_param_0,
	.param .u64 gelu_param_1,
	.param .u32 gelu_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<28>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [gelu_param_0];
	ld.param.u64 	%rd3, [gelu_param_1];
	ld.param.u32 	%r2, [gelu_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	ld.const.u16 	%rs3, [sh+2];
	// begin inline asm
	{mul.f16 %rs2,%rs3,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs5,%rs1,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs8,%rs5,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs12, [sh+4];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs8;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs2,%rs11;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs14;}

	// end inline asm
	abs.ftz.f32 	%f2, %f6;
	setp.ltu.ftz.f32 	%p2, %f2, 0f3F19999A;
	@%p2 bra 	$L__BB1_3;
	bra.uni 	$L__BB1_2;

$L__BB1_3:
	mul.ftz.f32 	%f15, %f6, %f6;
	mov.f32 	%f16, 0fBD563CAE;
	mov.f32 	%f17, 0f3C80F082;
	fma.rn.ftz.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0f3E085941;
	fma.rn.ftz.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f00000000;
	fma.rn.ftz.f32 	%f24, %f22, %f15, %f23;
	fma.rn.ftz.f32 	%f26, %f24, %f6, %f6;
	bra.uni 	$L__BB1_4;

$L__BB1_2:
	mul.ftz.f32 	%f7, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f8, %f7;
	add.ftz.f32 	%f9, %f8, 0f3F800000;
	mov.f32 	%f10, 0f3F800000;
	rcp.approx.ftz.f32 	%f11, %f9;
	mov.f32 	%f12, 0fC0000000;
	fma.rn.ftz.f32 	%f13, %f11, %f12, %f10;
	setp.ge.ftz.f32 	%p3, %f2, 0f41102CB4;
	selp.f32 	%f14, 0f3F800000, %f13, %p3;
	mov.b32 	%r6, %f14;
	mov.b32 	%r7, %f6;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f26, %r9;

$L__BB1_4:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f26;}

	// end inline asm
	ld.const.u16 	%rs20, [sh+6];
	// begin inline asm
	{mul.f16 %rs19,%rs20,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs23, [sh+8];
	// begin inline asm
	{add.f16 %rs22,%rs23,%rs18;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs25;

$L__BB1_5:
	ret;

}
	// .globl	set
.visible .entry set(
	.param .u64 set_param_0,
	.param .u32 set_param_1,
	.param .align 2 .b8 set_param_2[2]
)
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs1, [set_param_2];
	ld.param.u64 	%rd1, [set_param_0];
	ld.param.u32 	%r1, [set_param_1];
	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u16 	[%rd4], %rs1;
	ret;

}
	// .globl	MatAdd
.visible .entry MatAdd(
	.param .u64 MatAdd_param_0,
	.param .u64 MatAdd_param_1,
	.param .u32 MatAdd_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [MatAdd_param_0];
	ld.param.u64 	%rd2, [MatAdd_param_1];
	ld.param.u32 	%r2, [MatAdd_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u32 	%r8, [%rd7];
	// begin inline asm
	{add.f16x2 %r6,%r7,%r8;
}
	// end inline asm
	st.global.u32 	[%rd5], %r6;

$L__BB3_2:
	ret;

}
	// .globl	imageVector
.visible .entry imageVector(
	.param .u64 imageVector_param_0,
	.param .u64 imageVector_param_1,
	.param .u32 imageVector_param_2,
	.param .u32 imageVector_param_3,
	.param .u32 imageVector_param_4,
	.param .u32 imageVector_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [imageVector_param_0];
	ld.param.u64 	%rd14, [imageVector_param_1];
	ld.param.u32 	%r29, [imageVector_param_2];
	ld.param.u32 	%r26, [imageVector_param_3];
	ld.param.u32 	%r27, [imageVector_param_4];
	ld.param.u32 	%r28, [imageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r33, %r31, %r30, %r32;
	mul.lo.s32 	%r1, %r33, %r28;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r36, %tid.y;
	mad.lo.s32 	%r37, %r35, %r34, %r36;
	mul.lo.s32 	%r2, %r37, %r28;
	mov.u32 	%r38, %ctaid.z;
	mov.u32 	%r39, %ntid.z;
	mov.u32 	%r40, %tid.z;
	mad.lo.s32 	%r3, %r39, %r38, %r40;
	setp.ge.s32 	%p1, %r1, %r29;
	setp.ge.s32 	%p2, %r2, %r26;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r28;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB4_12;

	setp.lt.s32 	%p6, %r28, 1;
	@%p6 bra 	$L__BB4_12;

	mul.lo.s32 	%r41, %r28, %r27;
	mul.lo.s32 	%r42, %r41, %r28;
	add.s32 	%r43, %r1, %r3;
	mad.lo.s32 	%r4, %r43, %r26, %r2;
	setp.lt.s32 	%p7, %r27, 1;
	div.s32 	%r44, %r1, %r28;
	mul.lo.s32 	%r45, %r42, %r26;
	div.s32 	%r46, %r45, %r28;
	div.s32 	%r47, %r2, %r28;
	mul.lo.s32 	%r48, %r41, %r3;
	mad.lo.s32 	%r49, %r47, %r42, %r48;
	mad.lo.s32 	%r61, %r44, %r46, %r49;
	@%p7 bra 	$L__BB4_12;

	add.s32 	%r6, %r27, -1;
	and.b32  	%r7, %r27, 3;
	sub.s32 	%r8, %r27, %r7;
	add.s64 	%rd3, %rd1, 4;
	add.s64 	%rd4, %rd2, 4;
	mov.u32 	%r53, 0;

$L__BB4_4:
	add.s32 	%r52, %r4, %r53;
	mul.lo.s32 	%r59, %r52, %r27;
	setp.lt.u32 	%p8, %r6, 3;
	@%p8 bra 	$L__BB4_7;

	mul.wide.s32 	%rd15, %r61, 2;
	add.s64 	%rd20, %rd3, %rd15;
	mul.wide.s32 	%rd16, %r59, 2;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r57, %r8;

$L__BB4_6:
	ld.global.nc.u16 	%rs1, [%rd19+-4];
	st.global.u16 	[%rd20+-4], %rs1;
	ld.global.nc.u16 	%rs2, [%rd19+-2];
	st.global.u16 	[%rd20+-2], %rs2;
	ld.global.nc.u16 	%rs3, [%rd19];
	st.global.u16 	[%rd20], %rs3;
	ld.global.nc.u16 	%rs4, [%rd19+2];
	st.global.u16 	[%rd20+2], %rs4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd20, %rd20, 8;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r57, %r57, -4;
	setp.ne.s32 	%p9, %r57, 0;
	@%p9 bra 	$L__BB4_6;

$L__BB4_7:
	mov.u32 	%r20, %r61;
	setp.eq.s32 	%p10, %r7, 0;
	@%p10 bra 	$L__BB4_11;

	setp.eq.s32 	%p11, %r7, 1;
	mul.wide.s32 	%rd17, %r59, 2;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.u16 	%rs5, [%rd11];
	mul.wide.s32 	%rd18, %r20, 2;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.u16 	[%rd12], %rs5;
	add.s32 	%r61, %r20, 1;
	@%p11 bra 	$L__BB4_11;

	setp.eq.s32 	%p12, %r7, 2;
	ld.global.nc.u16 	%rs6, [%rd11+2];
	st.global.u16 	[%rd12+2], %rs6;
	add.s32 	%r61, %r20, 2;
	@%p12 bra 	$L__BB4_11;

	ld.global.nc.u16 	%rs7, [%rd11+4];
	st.global.u16 	[%rd12+4], %rs7;
	add.s32 	%r61, %r20, 3;

$L__BB4_11:
	add.s32 	%r53, %r53, 1;
	setp.lt.s32 	%p13, %r53, %r28;
	@%p13 bra 	$L__BB4_4;

$L__BB4_12:
	ret;

}
	// .globl	backImageVector
.visible .entry backImageVector(
	.param .u64 backImageVector_param_0,
	.param .u64 backImageVector_param_1,
	.param .u32 backImageVector_param_2,
	.param .u32 backImageVector_param_3,
	.param .u32 backImageVector_param_4,
	.param .u32 backImageVector_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [backImageVector_param_0];
	ld.param.u64 	%rd14, [backImageVector_param_1];
	ld.param.u32 	%r29, [backImageVector_param_2];
	ld.param.u32 	%r26, [backImageVector_param_3];
	ld.param.u32 	%r27, [backImageVector_param_4];
	ld.param.u32 	%r28, [backImageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r33, %r31, %r30, %r32;
	mul.lo.s32 	%r1, %r33, %r28;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r36, %tid.y;
	mad.lo.s32 	%r37, %r35, %r34, %r36;
	mul.lo.s32 	%r2, %r37, %r28;
	mov.u32 	%r38, %ctaid.z;
	mov.u32 	%r39, %ntid.z;
	mov.u32 	%r40, %tid.z;
	mad.lo.s32 	%r3, %r39, %r38, %r40;
	setp.ge.s32 	%p1, %r1, %r29;
	setp.ge.s32 	%p2, %r2, %r26;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r28;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB5_12;

	setp.lt.s32 	%p6, %r28, 1;
	@%p6 bra 	$L__BB5_12;

	mul.lo.s32 	%r41, %r28, %r27;
	mul.lo.s32 	%r42, %r41, %r28;
	add.s32 	%r43, %r1, %r3;
	mad.lo.s32 	%r4, %r43, %r26, %r2;
	setp.lt.s32 	%p7, %r27, 1;
	div.s32 	%r44, %r1, %r28;
	mul.lo.s32 	%r45, %r42, %r26;
	div.s32 	%r46, %r45, %r28;
	div.s32 	%r47, %r2, %r28;
	mul.lo.s32 	%r48, %r41, %r3;
	mad.lo.s32 	%r49, %r47, %r42, %r48;
	mad.lo.s32 	%r61, %r44, %r46, %r49;
	@%p7 bra 	$L__BB5_12;

	add.s32 	%r6, %r27, -1;
	and.b32  	%r7, %r27, 3;
	sub.s32 	%r8, %r27, %r7;
	add.s64 	%rd3, %rd2, 4;
	add.s64 	%rd4, %rd1, 4;
	mov.u32 	%r53, 0;

$L__BB5_4:
	add.s32 	%r52, %r4, %r53;
	mul.lo.s32 	%r59, %r52, %r27;
	setp.lt.u32 	%p8, %r6, 3;
	@%p8 bra 	$L__BB5_7;

	mul.wide.s32 	%rd15, %r61, 2;
	add.s64 	%rd20, %rd3, %rd15;
	mul.wide.s32 	%rd16, %r59, 2;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r57, %r8;

$L__BB5_6:
	ld.global.nc.u16 	%rs1, [%rd20+-4];
	st.global.u16 	[%rd19+-4], %rs1;
	ld.global.nc.u16 	%rs2, [%rd20+-2];
	st.global.u16 	[%rd19+-2], %rs2;
	ld.global.nc.u16 	%rs3, [%rd20];
	st.global.u16 	[%rd19], %rs3;
	ld.global.nc.u16 	%rs4, [%rd20+2];
	st.global.u16 	[%rd19+2], %rs4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd20, %rd20, 8;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r57, %r57, -4;
	setp.ne.s32 	%p9, %r57, 0;
	@%p9 bra 	$L__BB5_6;

$L__BB5_7:
	mov.u32 	%r20, %r61;
	setp.eq.s32 	%p10, %r7, 0;
	@%p10 bra 	$L__BB5_11;

	setp.eq.s32 	%p11, %r7, 1;
	mul.wide.s32 	%rd17, %r20, 2;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.u16 	%rs5, [%rd11];
	mul.wide.s32 	%rd18, %r59, 2;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.u16 	[%rd12], %rs5;
	add.s32 	%r61, %r20, 1;
	@%p11 bra 	$L__BB5_11;

	setp.eq.s32 	%p12, %r7, 2;
	ld.global.nc.u16 	%rs6, [%rd11+2];
	st.global.u16 	[%rd12+2], %rs6;
	add.s32 	%r61, %r20, 2;
	@%p12 bra 	$L__BB5_11;

	ld.global.nc.u16 	%rs7, [%rd11+4];
	st.global.u16 	[%rd12+4], %rs7;
	add.s32 	%r61, %r20, 3;

$L__BB5_11:
	add.s32 	%r53, %r53, 1;
	setp.lt.s32 	%p13, %r53, %r28;
	@%p13 bra 	$L__BB5_4;

$L__BB5_12:
	ret;

}
	// .globl	add3
.visible .entry add3(
	.param .u64 add3_param_0,
	.param .u64 add3_param_1,
	.param .u32 add3_param_2,
	.param .u32 add3_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [add3_param_0];
	ld.param.u64 	%rd2, [add3_param_1];
	ld.param.u32 	%r4, [add3_param_2];
	ld.param.u32 	%r5, [add3_param_3];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r2, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r3, %r2, %r9, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r3, %r5;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r11, %nctaid.y;
	mul.lo.s32 	%r12, %r11, %r2;
	mad.lo.s32 	%r13, %r12, %r1, %r3;
	mul.wide.s32 	%rd4, %r13, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r3, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u16 	%rs3, [%rd8];
	// begin inline asm
	{add.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs1;

$L__BB6_2:
	ret;

}
	// .globl	dot_VectorAndMatrix
.visible .entry dot_VectorAndMatrix(
	.param .u64 dot_VectorAndMatrix_param_0,
	.param .u64 dot_VectorAndMatrix_param_1,
	.param .u64 dot_VectorAndMatrix_param_2,
	.param .u32 dot_VectorAndMatrix_param_3,
	.param .u32 dot_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd14, [dot_VectorAndMatrix_param_0];
	ld.param.u64 	%rd15, [dot_VectorAndMatrix_param_1];
	ld.param.u64 	%rd16, [dot_VectorAndMatrix_param_2];
	ld.param.u32 	%r12, [dot_VectorAndMatrix_param_3];
	ld.param.u32 	%r11, [dot_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB7_9;

	ld.const.u16 	%rs44, [sh];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB7_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r24, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r23, 0;
	@%p3 bra 	$L__BB7_5;

	sub.s32 	%r22, %r11, %r24;
	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd17, %r19, 2;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd25, %rd18, 4;
	mov.u64 	%rd24, %rd2;

$L__BB7_4:
	ld.global.nc.u16 	%rs11, [%rd24];
	ld.global.nc.u16 	%rs12, [%rd25+-4];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs12;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs44,%rs10;
}
	// end inline asm
	ld.global.nc.u16 	%rs17, [%rd24+2];
	ld.global.nc.u16 	%rs18, [%rd25+-2];
	// begin inline asm
	{mul.f16 %rs16,%rs17,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs19,%rs13,%rs16;
}
	// end inline asm
	ld.global.nc.u16 	%rs23, [%rd24+4];
	ld.global.nc.u16 	%rs24, [%rd25];
	// begin inline asm
	{mul.f16 %rs22,%rs23,%rs24;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd24+6];
	ld.global.nc.u16 	%rs30, [%rd25+2];
	// begin inline asm
	{mul.f16 %rs28,%rs29,%rs30;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs25,%rs28;
}
	// end inline asm
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd25, %rd25, 8;
	add.s64 	%rd24, %rd24, 8;
	add.s32 	%r22, %r22, -4;
	setp.ne.s32 	%p4, %r22, 0;
	@%p4 bra 	$L__BB7_4;

$L__BB7_5:
	setp.eq.s32 	%p5, %r24, 0;
	@%p5 bra 	$L__BB7_8;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd19, %r20, 2;
	add.s64 	%rd27, %rd1, %rd19;
	mul.wide.s32 	%rd20, %r23, 2;
	add.s64 	%rd26, %rd2, %rd20;

$L__BB7_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs35, [%rd26];
	ld.global.nc.u16 	%rs36, [%rd27];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs36;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs44,%rs34;
}
	// end inline asm
	add.s64 	%rd27, %rd27, 2;
	add.s64 	%rd26, %rd26, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB7_7;

$L__BB7_8:
	cvta.to.global.u64 	%rd21, %rd14;
	mul.wide.s32 	%rd22, %r1, 2;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u16 	[%rd23], %rs44;

$L__BB7_9:
	ret;

}
	// .globl	MatMulKernel
.visible .entry MatMulKernel(
	.param .u64 MatMulKernel_param_0,
	.param .u64 MatMulKernel_param_1,
	.param .u64 MatMulKernel_param_2,
	.param .u32 MatMulKernel_param_3,
	.param .u32 MatMulKernel_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<48>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<27>;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE8blockElt;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE9blockxInd;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE9blockyInd;
	// demoted variable
	.shared .align 2 .b8 _ZZ12MatMulKernelE1b[128];

	ld.param.u64 	%rd11, [MatMulKernel_param_0];
	ld.param.u64 	%rd12, [MatMulKernel_param_1];
	ld.param.u64 	%rd13, [MatMulKernel_param_2];
	ld.param.u32 	%r26, [MatMulKernel_param_3];
	ld.param.u32 	%r27, [MatMulKernel_param_4];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB8_4;

	mov.u32 	%r29, %ctaid.x;
	shl.b32 	%r2, %r29, 6;
	add.s32 	%r30, %r2, 64;
	mov.u32 	%r54, 64;
	setp.le.u32 	%p2, %r30, %r27;
	@%p2 bra 	$L__BB8_3;

	shr.s32 	%r31, %r27, 31;
	shr.u32 	%r32, %r31, 26;
	add.s32 	%r33, %r27, %r32;
	and.b32  	%r34, %r33, -64;
	sub.s32 	%r54, %r27, %r34;

$L__BB8_3:
	st.shared.u32 	[_ZZ12MatMulKernelE8blockElt], %r54;
	st.shared.u32 	[_ZZ12MatMulKernelE9blockxInd], %r2;
	mov.u32 	%r35, %ctaid.y;
	shl.b32 	%r36, %r35, 10;
	st.shared.u32 	[_ZZ12MatMulKernelE9blockyInd], %r36;

$L__BB8_4:
	bar.sync 	0;
	ld.shared.u32 	%r37, [_ZZ12MatMulKernelE8blockElt];
	setp.ge.u32 	%p3, %r1, %r37;
	@%p3 bra 	$L__BB8_6;

	ld.shared.u32 	%r38, [_ZZ12MatMulKernelE9blockxInd];
	add.s32 	%r39, %r38, %r1;
	cvta.to.global.u64 	%rd14, %rd12;
	mul.wide.u32 	%rd15, %r39, 2;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u16 	%rs9, [%rd16];
	shl.b32 	%r40, %r1, 1;
	mov.u32 	%r41, _ZZ12MatMulKernelE1b;
	add.s32 	%r42, %r41, %r40;
	st.shared.u16 	[%r42], %rs9;

$L__BB8_6:
	bar.sync 	0;
	ld.const.u16 	%rs47, [sh];
	ld.shared.u32 	%r43, [_ZZ12MatMulKernelE9blockyInd];
	add.s32 	%r5, %r43, %r1;
	setp.ge.s32 	%p4, %r5, %r26;
	@%p4 bra 	$L__BB8_15;

	ld.shared.u32 	%r6, [_ZZ12MatMulKernelE8blockElt];
	setp.lt.s32 	%p5, %r6, 1;
	@%p5 bra 	$L__BB8_14;

	ld.shared.u32 	%r7, [_ZZ12MatMulKernelE9blockxInd];
	mov.u32 	%r59, 0;
	and.b32  	%r61, %r6, 3;
	add.s32 	%r45, %r6, -1;
	setp.lt.u32 	%p6, %r45, 3;
	@%p6 bra 	$L__BB8_11;

	sub.s32 	%r58, %r6, %r61;
	add.s32 	%r48, %r7, 1;
	mad.lo.s32 	%r55, %r26, %r48, %r5;
	mad.lo.s32 	%r49, %r7, %r26, %r5;
	mul.wide.s32 	%rd17, %r49, 2;
	add.s64 	%rd25, %rd1, %rd17;
	shl.b32 	%r11, %r26, 2;
	mul.wide.s32 	%rd3, %r11, 2;
	mul.wide.s32 	%rd4, %r26, 2;
	mov.u32 	%r56, _ZZ12MatMulKernelE1b;

$L__BB8_10:
	ld.shared.u16 	%rs12, [%r56];
	ld.global.u16 	%rs13, [%rd25];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs13;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs47,%rs11;
}
	// end inline asm
	ld.shared.u16 	%rs18, [%r56+2];
	mul.wide.s32 	%rd18, %r55, 2;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.u16 	%rs19, [%rd19];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs14,%rs17;
}
	// end inline asm
	ld.shared.u16 	%rs24, [%r56+4];
	add.s64 	%rd20, %rd19, %rd4;
	ld.global.u16 	%rs25, [%rd20];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs20,%rs23;
}
	// end inline asm
	ld.shared.u16 	%rs30, [%r56+6];
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.u16 	%rs31, [%rd21];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs26,%rs29;
}
	// end inline asm
	add.s32 	%r59, %r59, 4;
	add.s32 	%r56, %r56, 8;
	add.s32 	%r55, %r55, %r11;
	add.s64 	%rd25, %rd25, %rd3;
	add.s32 	%r58, %r58, -4;
	setp.ne.s32 	%p7, %r58, 0;
	@%p7 bra 	$L__BB8_10;

$L__BB8_11:
	setp.eq.s32 	%p8, %r61, 0;
	@%p8 bra 	$L__BB8_14;

	shl.b32 	%r50, %r59, 1;
	mov.u32 	%r51, _ZZ12MatMulKernelE1b;
	add.s32 	%r60, %r51, %r50;
	add.s32 	%r52, %r59, %r7;
	mad.lo.s32 	%r53, %r26, %r52, %r5;
	mul.wide.s32 	%rd22, %r53, 2;
	add.s64 	%rd26, %rd1, %rd22;
	mul.wide.s32 	%rd8, %r26, 2;

$L__BB8_13:
	.pragma "nounroll";
	ld.shared.u16 	%rs36, [%r60];
	ld.global.u16 	%rs37, [%rd26];
	// begin inline asm
	{mul.f16 %rs35,%rs36,%rs37;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs47,%rs35;
}
	// end inline asm
	add.s32 	%r60, %r60, 2;
	add.s64 	%rd26, %rd26, %rd8;
	add.s32 	%r61, %r61, -1;
	setp.ne.s32 	%p9, %r61, 0;
	@%p9 bra 	$L__BB8_13;

$L__BB8_14:
	mul.wide.s32 	%rd24, %r5, 2;
	add.s64 	%rd23, %rd11, %rd24;
	// begin inline asm
	{ atom.add.noftz.f16 %rs41,[%rd23],%rs47; }

	// end inline asm

$L__BB8_15:
	ret;

}
	// .globl	MatMulKernelT
.visible .entry MatMulKernelT(
	.param .u64 MatMulKernelT_param_0,
	.param .u64 MatMulKernelT_param_1,
	.param .u64 MatMulKernelT_param_2,
	.param .u32 MatMulKernelT_param_3,
	.param .u32 MatMulKernelT_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<48>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<21>;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE8blockElt;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE9blockxInd;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE9blockyInd;
	// demoted variable
	.shared .align 2 .b8 _ZZ13MatMulKernelTE1b[128];

	ld.param.u64 	%rd8, [MatMulKernelT_param_0];
	ld.param.u64 	%rd9, [MatMulKernelT_param_1];
	ld.param.u64 	%rd10, [MatMulKernelT_param_2];
	ld.param.u32 	%r22, [MatMulKernelT_param_3];
	ld.param.u32 	%r23, [MatMulKernelT_param_4];
	cvta.to.global.u64 	%rd1, %rd10;
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB9_4;

	mov.u32 	%r25, %ctaid.y;
	shl.b32 	%r2, %r25, 6;
	add.s32 	%r26, %r2, 64;
	mov.u32 	%r49, 64;
	setp.le.u32 	%p2, %r26, %r22;
	@%p2 bra 	$L__BB9_3;

	shr.s32 	%r27, %r22, 31;
	shr.u32 	%r28, %r27, 26;
	add.s32 	%r29, %r22, %r28;
	and.b32  	%r30, %r29, -64;
	sub.s32 	%r49, %r22, %r30;

$L__BB9_3:
	st.shared.u32 	[_ZZ13MatMulKernelTE8blockElt], %r49;
	mov.u32 	%r31, %ctaid.x;
	shl.b32 	%r32, %r31, 10;
	st.shared.u32 	[_ZZ13MatMulKernelTE9blockxInd], %r32;
	st.shared.u32 	[_ZZ13MatMulKernelTE9blockyInd], %r2;

$L__BB9_4:
	bar.sync 	0;
	ld.shared.u32 	%r33, [_ZZ13MatMulKernelTE8blockElt];
	setp.ge.u32 	%p3, %r1, %r33;
	@%p3 bra 	$L__BB9_6;

	ld.shared.u32 	%r34, [_ZZ13MatMulKernelTE9blockyInd];
	add.s32 	%r35, %r34, %r1;
	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r35, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u16 	%rs9, [%rd13];
	shl.b32 	%r36, %r1, 1;
	mov.u32 	%r37, _ZZ13MatMulKernelTE1b;
	add.s32 	%r38, %r37, %r36;
	st.shared.u16 	[%r38], %rs9;

$L__BB9_6:
	bar.sync 	0;
	ld.const.u16 	%rs47, [sh];
	ld.shared.u32 	%r39, [_ZZ13MatMulKernelTE9blockxInd];
	add.s32 	%r5, %r39, %r1;
	setp.ge.s32 	%p4, %r5, %r23;
	@%p4 bra 	$L__BB9_15;

	ld.shared.u32 	%r6, [_ZZ13MatMulKernelTE8blockElt];
	setp.lt.s32 	%p5, %r6, 1;
	@%p5 bra 	$L__BB9_14;

	ld.shared.u32 	%r7, [_ZZ13MatMulKernelTE9blockyInd];
	mov.u32 	%r53, 0;
	and.b32  	%r55, %r6, 3;
	add.s32 	%r41, %r6, -1;
	setp.lt.u32 	%p6, %r41, 3;
	@%p6 bra 	$L__BB9_11;

	sub.s32 	%r52, %r6, %r55;
	mad.lo.s32 	%r44, %r22, %r5, %r7;
	mul.wide.s32 	%rd14, %r44, 2;
	add.s64 	%rd15, %rd1, %rd14;
	add.s64 	%rd19, %rd15, 4;
	mov.u32 	%r50, _ZZ13MatMulKernelTE1b;

$L__BB9_10:
	ld.shared.u16 	%rs12, [%r50];
	ld.global.u16 	%rs13, [%rd19+-4];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs13;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs47,%rs11;
}
	// end inline asm
	ld.shared.u16 	%rs18, [%r50+2];
	ld.global.u16 	%rs19, [%rd19+-2];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs14,%rs17;
}
	// end inline asm
	ld.shared.u16 	%rs24, [%r50+4];
	ld.global.u16 	%rs25, [%rd19];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs20,%rs23;
}
	// end inline asm
	ld.shared.u16 	%rs30, [%r50+6];
	ld.global.u16 	%rs31, [%rd19+2];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs26,%rs29;
}
	// end inline asm
	add.s32 	%r53, %r53, 4;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r50, %r50, 8;
	add.s32 	%r52, %r52, -4;
	setp.ne.s32 	%p7, %r52, 0;
	@%p7 bra 	$L__BB9_10;

$L__BB9_11:
	setp.eq.s32 	%p8, %r55, 0;
	@%p8 bra 	$L__BB9_14;

	shl.b32 	%r45, %r53, 1;
	mov.u32 	%r46, _ZZ13MatMulKernelTE1b;
	add.s32 	%r54, %r46, %r45;
	add.s32 	%r47, %r53, %r7;
	mad.lo.s32 	%r48, %r22, %r5, %r47;
	mul.wide.s32 	%rd16, %r48, 2;
	add.s64 	%rd20, %rd1, %rd16;

$L__BB9_13:
	.pragma "nounroll";
	ld.shared.u16 	%rs36, [%r54];
	ld.global.u16 	%rs37, [%rd20];
	// begin inline asm
	{mul.f16 %rs35,%rs36,%rs37;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs47,%rs35;
}
	// end inline asm
	add.s32 	%r54, %r54, 2;
	add.s64 	%rd20, %rd20, 2;
	add.s32 	%r55, %r55, -1;
	setp.ne.s32 	%p9, %r55, 0;
	@%p9 bra 	$L__BB9_13;

$L__BB9_14:
	mul.wide.s32 	%rd18, %r5, 2;
	add.s64 	%rd17, %rd8, %rd18;
	// begin inline asm
	{ atom.add.noftz.f16 %rs41,[%rd17],%rs47; }

	// end inline asm

$L__BB9_15:
	ret;

}
	// .globl	dotT_VectorAndMatrix
.visible .entry dotT_VectorAndMatrix(
	.param .u64 dotT_VectorAndMatrix_param_0,
	.param .u64 dotT_VectorAndMatrix_param_1,
	.param .u64 dotT_VectorAndMatrix_param_2,
	.param .u32 dotT_VectorAndMatrix_param_3,
	.param .u32 dotT_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd17, [dotT_VectorAndMatrix_param_0];
	ld.param.u64 	%rd18, [dotT_VectorAndMatrix_param_1];
	ld.param.u64 	%rd16, [dotT_VectorAndMatrix_param_2];
	ld.param.u32 	%r11, [dotT_VectorAndMatrix_param_3];
	ld.param.u32 	%r12, [dotT_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd17;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB10_9;

	ld.const.u16 	%rs44, [sh];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB10_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB10_5;

	sub.s32 	%r21, %r11, %r23;
	mul.wide.s32 	%rd19, %r1, 2;
	add.s64 	%rd29, %rd1, %rd19;
	mul.wide.s32 	%rd4, %r12, 2;
	mov.u64 	%rd28, %rd2;

$L__BB10_4:
	ld.global.nc.u16 	%rs11, [%rd28];
	ld.global.nc.u16 	%rs12, [%rd29];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs12;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs44,%rs10;
}
	// end inline asm
	ld.global.nc.u16 	%rs17, [%rd28+2];
	add.s64 	%rd20, %rd29, %rd4;
	ld.global.nc.u16 	%rs18, [%rd20];
	// begin inline asm
	{mul.f16 %rs16,%rs17,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs19,%rs13,%rs16;
}
	// end inline asm
	ld.global.nc.u16 	%rs23, [%rd28+4];
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.nc.u16 	%rs24, [%rd21];
	// begin inline asm
	{mul.f16 %rs22,%rs23,%rs24;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd28+6];
	add.s64 	%rd22, %rd21, %rd4;
	add.s64 	%rd29, %rd22, %rd4;
	ld.global.nc.u16 	%rs30, [%rd22];
	// begin inline asm
	{mul.f16 %rs28,%rs29,%rs30;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs25,%rs28;
}
	// end inline asm
	add.s32 	%r22, %r22, 4;
	add.s64 	%rd28, %rd28, 8;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB10_4;

$L__BB10_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB10_8;

	mul.wide.s32 	%rd23, %r22, 2;
	add.s64 	%rd31, %rd2, %rd23;
	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd24, %r19, 2;
	add.s64 	%rd30, %rd1, %rd24;
	mul.wide.s32 	%rd11, %r12, 2;

$L__BB10_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs35, [%rd31];
	ld.global.nc.u16 	%rs36, [%rd30];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs36;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs44,%rs34;
}
	// end inline asm
	add.s64 	%rd31, %rd31, 2;
	add.s64 	%rd30, %rd30, %rd11;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB10_7;

$L__BB10_8:
	cvta.to.global.u64 	%rd25, %rd16;
	mul.wide.s32 	%rd26, %r1, 2;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u16 	[%rd27], %rs44;

$L__BB10_9:
	ret;

}
	// .globl	derivativeWeight
.visible .entry derivativeWeight(
	.param .u64 derivativeWeight_param_0,
	.param .u64 derivativeWeight_param_1,
	.param .u64 derivativeWeight_param_2,
	.param .u32 derivativeWeight_param_3,
	.param .u32 derivativeWeight_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [derivativeWeight_param_0];
	ld.param.u64 	%rd2, [derivativeWeight_param_1];
	ld.param.u64 	%rd3, [derivativeWeight_param_2];
	ld.param.u32 	%r4, [derivativeWeight_param_3];
	ld.param.u32 	%r3, [derivativeWeight_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB11_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs2, [%rd7];
	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.s32 	%rd9, %r2, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs3, [%rd10];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	mul.wide.s32 	%rd11, %r11, 2;
	add.s64 	%rd12, %rd4, %rd11;
	ld.global.u16 	%rs5, [%rd12];
	// begin inline asm
	{add.f16 %rs4,%rs5,%rs1;
}
	// end inline asm
	st.global.u16 	[%rd12], %rs4;

$L__BB11_2:
	ret;

}
	// .globl	findMean_part
.visible .entry findMean_part(
	.param .u64 findMean_part_param_0,
	.param .u64 findMean_part_param_1,
	.param .u32 findMean_part_param_2,
	.param .u32 findMean_part_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<30>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd9, [findMean_part_param_0];
	ld.param.u64 	%rd8, [findMean_part_param_1];
	ld.param.u32 	%r13, [findMean_part_param_2];
	ld.param.u32 	%r12, [findMean_part_param_3];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r15, %r14, %r16;
	setp.ge.s32 	%p1, %r1, %r13;
	@%p1 bra 	$L__BB12_9;

	ld.const.u16 	%rs29, [sh];
	mul.lo.s32 	%r20, %r1, %r12;
	setp.lt.s32 	%p2, %r12, 1;
	@%p2 bra 	$L__BB12_8;

	add.s32 	%r17, %r12, -1;
	and.b32  	%r21, %r12, 3;
	setp.lt.u32 	%p3, %r17, 3;
	@%p3 bra 	$L__BB12_5;

	sub.s32 	%r19, %r12, %r21;
	mul.wide.s32 	%rd10, %r20, 2;
	add.s64 	%rd11, %rd1, %rd10;
	add.s64 	%rd16, %rd11, 4;

$L__BB12_4:
	ld.global.nc.u16 	%rs12, [%rd16+-4];
	// begin inline asm
	{add.f16 %rs10,%rs29,%rs12;
}
	// end inline asm
	ld.global.nc.u16 	%rs15, [%rd16+-2];
	// begin inline asm
	{add.f16 %rs13,%rs10,%rs15;
}
	// end inline asm
	ld.global.nc.u16 	%rs18, [%rd16];
	// begin inline asm
	{add.f16 %rs16,%rs13,%rs18;
}
	// end inline asm
	ld.global.nc.u16 	%rs21, [%rd16+2];
	// begin inline asm
	{add.f16 %rs29,%rs16,%rs21;
}
	// end inline asm
	add.s32 	%r20, %r20, 4;
	add.s64 	%rd16, %rd16, 8;
	add.s32 	%r19, %r19, -4;
	setp.ne.s32 	%p4, %r19, 0;
	@%p4 bra 	$L__BB12_4;

$L__BB12_5:
	setp.eq.s32 	%p5, %r21, 0;
	@%p5 bra 	$L__BB12_8;

	mul.wide.s32 	%rd12, %r20, 2;
	add.s64 	%rd17, %rd1, %rd12;

$L__BB12_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs24, [%rd17];
	// begin inline asm
	{add.f16 %rs29,%rs29,%rs24;
}
	// end inline asm
	add.s64 	%rd17, %rd17, 2;
	add.s32 	%r21, %r21, -1;
	setp.ne.s32 	%p6, %r21, 0;
	@%p6 bra 	$L__BB12_7;

$L__BB12_8:
	cvta.to.global.u64 	%rd13, %rd8;
	mul.wide.s32 	%rd14, %r1, 2;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u16 	[%rd15], %rs29;

$L__BB12_9:
	ret;

}
	// .globl	generateErrorNorm
.visible .entry generateErrorNorm(
	.param .u64 generateErrorNorm_param_0,
	.param .u64 generateErrorNorm_param_1,
	.param .u64 generateErrorNorm_param_2,
	.param .u32 generateErrorNorm_param_3,
	.param .u32 generateErrorNorm_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [generateErrorNorm_param_0];
	ld.param.u64 	%rd2, [generateErrorNorm_param_1];
	ld.param.u64 	%rd3, [generateErrorNorm_param_2];
	ld.param.u32 	%r4, [generateErrorNorm_param_3];
	ld.param.u32 	%r3, [generateErrorNorm_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB13_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd5, %r11, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r2, 2;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.u16 	%rs3, [%rd9];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.u16 	[%rd11], %rs1;

$L__BB13_2:
	ret;

}
	// .globl	derVar_part_2
.visible .entry derVar_part_2(
	.param .u64 derVar_part_2_param_0,
	.param .u64 derVar_part_2_param_1,
	.param .u64 derVar_part_2_param_2,
	.param .u64 derVar_part_2_param_3,
	.param .u32 derVar_part_2_param_4,
	.param .u32 derVar_part_2_param_5
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<61>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<33>;


	ld.param.u64 	%rd18, [derVar_part_2_param_0];
	ld.param.u64 	%rd19, [derVar_part_2_param_1];
	ld.param.u64 	%rd16, [derVar_part_2_param_2];
	ld.param.u64 	%rd17, [derVar_part_2_param_3];
	ld.param.u32 	%r13, [derVar_part_2_param_4];
	ld.param.u32 	%r12, [derVar_part_2_param_5];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd19;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r15, %r14, %r16;
	setp.ge.s32 	%p1, %r1, %r13;
	@%p1 bra 	$L__BB14_9;

	cvta.to.global.u64 	%rd20, %rd16;
	mul.lo.s32 	%r20, %r1, %r12;
	cvt.s64.s32 	%rd3, %r1;
	mul.wide.s32 	%rd21, %r1, 2;
	add.s64 	%rd22, %rd20, %rd21;
	ld.global.nc.u16 	%rs1, [%rd22];
	ld.const.u16 	%rs60, [sh];
	setp.lt.s32 	%p2, %r12, 1;
	@%p2 bra 	$L__BB14_8;

	add.s32 	%r17, %r12, -1;
	and.b32  	%r21, %r12, 3;
	setp.lt.u32 	%p3, %r17, 3;
	@%p3 bra 	$L__BB14_5;

	sub.s32 	%r19, %r12, %r21;
	mul.wide.s32 	%rd23, %r20, 2;
	add.s64 	%rd24, %rd23, 4;
	add.s64 	%rd30, %rd1, %rd24;
	add.s64 	%rd29, %rd2, %rd24;

$L__BB14_4:
	ld.global.nc.u16 	%rs12, [%rd29+-4];
	// begin inline asm
	{sub.f16 %rs11,%rs12,%rs1;
}
	// end inline asm
	ld.global.nc.u16 	%rs15, [%rd30+-4];
	// begin inline asm
	{mul.f16 %rs14,%rs15,%rs11;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs17,%rs60,%rs14;
}
	// end inline asm
	ld.global.nc.u16 	%rs21, [%rd29+-2];
	// begin inline asm
	{sub.f16 %rs20,%rs21,%rs1;
}
	// end inline asm
	ld.global.nc.u16 	%rs24, [%rd30+-2];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs20;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs17,%rs23;
}
	// end inline asm
	ld.global.nc.u16 	%rs30, [%rd29];
	// begin inline asm
	{sub.f16 %rs29,%rs30,%rs1;
}
	// end inline asm
	ld.global.nc.u16 	%rs33, [%rd30];
	// begin inline asm
	{mul.f16 %rs32,%rs33,%rs29;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs35,%rs26,%rs32;
}
	// end inline asm
	ld.global.nc.u16 	%rs39, [%rd29+2];
	// begin inline asm
	{sub.f16 %rs38,%rs39,%rs1;
}
	// end inline asm
	ld.global.nc.u16 	%rs42, [%rd30+2];
	// begin inline asm
	{mul.f16 %rs41,%rs42,%rs38;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs60,%rs35,%rs41;
}
	// end inline asm
	add.s32 	%r20, %r20, 4;
	add.s64 	%rd30, %rd30, 8;
	add.s64 	%rd29, %rd29, 8;
	add.s32 	%r19, %r19, -4;
	setp.ne.s32 	%p4, %r19, 0;
	@%p4 bra 	$L__BB14_4;

$L__BB14_5:
	setp.eq.s32 	%p5, %r21, 0;
	@%p5 bra 	$L__BB14_8;

	mul.wide.s32 	%rd25, %r20, 2;
	add.s64 	%rd32, %rd1, %rd25;
	add.s64 	%rd31, %rd2, %rd25;

$L__BB14_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs48, [%rd31];
	// begin inline asm
	{sub.f16 %rs47,%rs48,%rs1;
}
	// end inline asm
	ld.global.nc.u16 	%rs51, [%rd32];
	// begin inline asm
	{mul.f16 %rs50,%rs51,%rs47;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs60,%rs60,%rs50;
}
	// end inline asm
	add.s64 	%rd32, %rd32, 2;
	add.s64 	%rd31, %rd31, 2;
	add.s32 	%r21, %r21, -1;
	setp.ne.s32 	%p6, %r21, 0;
	@%p6 bra 	$L__BB14_7;

$L__BB14_8:
	cvta.to.global.u64 	%rd26, %rd17;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd26, %rd27;
	st.global.u16 	[%rd28], %rs60;

$L__BB14_9:
	ret;

}
	// .globl	derMean_part_2
.visible .entry derMean_part_2(
	.param .u64 derMean_part_2_param_0,
	.param .u64 derMean_part_2_param_1,
	.param .u64 derMean_part_2_param_2,
	.param .u64 derMean_part_2_param_3,
	.param .u64 derMean_part_2_param_4,
	.param .u32 derMean_part_2_param_5,
	.param .u32 derMean_part_2_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<73>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd19, [derMean_part_2_param_0];
	ld.param.u64 	%rd20, [derMean_part_2_param_1];
	ld.param.u64 	%rd16, [derMean_part_2_param_2];
	ld.param.u64 	%rd17, [derMean_part_2_param_3];
	ld.param.u64 	%rd18, [derMean_part_2_param_4];
	ld.param.u32 	%r12, [derMean_part_2_param_5];
	ld.param.u32 	%r11, [derMean_part_2_param_6];
	cvta.to.global.u64 	%rd1, %rd20;
	cvta.to.global.u64 	%rd2, %rd19;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB15_9;

	cvta.to.global.u64 	%rd21, %rd16;
	ld.const.u16 	%rs71, [sh];
	cvt.s64.s32 	%rd3, %r1;
	mul.wide.s32 	%rd22, %r1, 2;
	add.s64 	%rd23, %rd21, %rd22;
	ld.global.nc.u16 	%rs2, [%rd23];
	setp.lt.s32 	%p2, %r11, 1;
	mov.u16 	%rs72, %rs71;
	@%p2 bra 	$L__BB15_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r24, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r23, 0;
	mov.u16 	%rs72, %rs71;
	@%p3 bra 	$L__BB15_5;

	sub.s32 	%r22, %r11, %r24;
	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd24, %r19, 2;
	add.s64 	%rd25, %rd24, 4;
	add.s64 	%rd33, %rd2, %rd25;
	add.s64 	%rd32, %rd1, %rd25;
	mov.u16 	%rs72, %rs71;

$L__BB15_4:
	ld.global.nc.u16 	%rs20, [%rd33+-4];
	// begin inline asm
	{add.f16 %rs18,%rs72,%rs20;
}
	// end inline asm
	ld.global.nc.u16 	%rs22, [%rd32+-4];
	// begin inline asm
	{sub.f16 %rs21,%rs22,%rs2;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs24,%rs71,%rs21;
}
	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd33+-2];
	// begin inline asm
	{add.f16 %rs27,%rs18,%rs29;
}
	// end inline asm
	ld.global.nc.u16 	%rs31, [%rd32+-2];
	// begin inline asm
	{sub.f16 %rs30,%rs31,%rs2;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs33,%rs24,%rs30;
}
	// end inline asm
	ld.global.nc.u16 	%rs38, [%rd33];
	// begin inline asm
	{add.f16 %rs36,%rs27,%rs38;
}
	// end inline asm
	ld.global.nc.u16 	%rs40, [%rd32];
	// begin inline asm
	{sub.f16 %rs39,%rs40,%rs2;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs42,%rs33,%rs39;
}
	// end inline asm
	ld.global.nc.u16 	%rs47, [%rd33+2];
	// begin inline asm
	{add.f16 %rs72,%rs36,%rs47;
}
	// end inline asm
	ld.global.nc.u16 	%rs49, [%rd32+2];
	// begin inline asm
	{sub.f16 %rs48,%rs49,%rs2;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs71,%rs42,%rs48;
}
	// end inline asm
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd33, %rd33, 8;
	add.s64 	%rd32, %rd32, 8;
	add.s32 	%r22, %r22, -4;
	setp.ne.s32 	%p4, %r22, 0;
	@%p4 bra 	$L__BB15_4;

$L__BB15_5:
	setp.eq.s32 	%p5, %r24, 0;
	@%p5 bra 	$L__BB15_8;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd26, %r20, 2;
	add.s64 	%rd35, %rd1, %rd26;
	add.s64 	%rd34, %rd2, %rd26;

$L__BB15_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs56, [%rd34];
	// begin inline asm
	{add.f16 %rs72,%rs72,%rs56;
}
	// end inline asm
	ld.global.nc.u16 	%rs58, [%rd35];
	// begin inline asm
	{sub.f16 %rs57,%rs58,%rs2;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs71,%rs71,%rs57;
}
	// end inline asm
	add.s64 	%rd35, %rd35, 2;
	add.s64 	%rd34, %rd34, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB15_7;

$L__BB15_8:
	cvta.to.global.u64 	%rd27, %rd17;
	shl.b64 	%rd28, %rd3, 1;
	add.s64 	%rd29, %rd27, %rd28;
	st.global.u16 	[%rd29], %rs72;
	cvta.to.global.u64 	%rd30, %rd18;
	add.s64 	%rd31, %rd30, %rd28;
	st.global.u16 	[%rd31], %rs71;

$L__BB15_9:
	ret;

}
	// .globl	derNorm_part_2
.visible .entry derNorm_part_2(
	.param .u64 derNorm_part_2_param_0,
	.param .u64 derNorm_part_2_param_1,
	.param .u64 derNorm_part_2_param_2,
	.param .u64 derNorm_part_2_param_3,
	.param .u64 derNorm_part_2_param_4,
	.param .u64 derNorm_part_2_param_5,
	.param .u64 derNorm_part_2_param_6,
	.param .u32 derNorm_part_2_param_7,
	.param .u32 derNorm_part_2_param_8
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<16>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd1, [derNorm_part_2_param_0];
	ld.param.u64 	%rd2, [derNorm_part_2_param_1];
	ld.param.u64 	%rd3, [derNorm_part_2_param_2];
	ld.param.u64 	%rd4, [derNorm_part_2_param_3];
	ld.param.u64 	%rd5, [derNorm_part_2_param_4];
	ld.param.u64 	%rd6, [derNorm_part_2_param_5];
	ld.param.u64 	%rd7, [derNorm_part_2_param_6];
	ld.param.u32 	%r4, [derNorm_part_2_param_7];
	ld.param.u32 	%r3, [derNorm_part_2_param_8];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB16_2;

	cvta.to.global.u64 	%rd8, %rd1;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd9, %r11, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs2, [%rd10];
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.nc.u16 	%rs3, [%rd13];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	cvta.to.global.u64 	%rd14, %rd4;
	add.s64 	%rd15, %rd14, %rd9;
	ld.global.nc.u16 	%rs5, [%rd15];
	cvta.to.global.u64 	%rd16, %rd5;
	add.s64 	%rd17, %rd16, %rd12;
	ld.global.nc.u16 	%rs6, [%rd17];
	// begin inline asm
	{sub.f16 %rs4,%rs5,%rs6;
}
	// end inline asm
	cvta.to.global.u64 	%rd18, %rd3;
	add.s64 	%rd19, %rd18, %rd12;
	ld.global.nc.u16 	%rs8, [%rd19];
	// begin inline asm
	{mul.f16 %rs7,%rs8,%rs4;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs10,%rs1,%rs7;
}
	// end inline asm
	cvta.to.global.u64 	%rd20, %rd6;
	add.s64 	%rd21, %rd20, %rd12;
	ld.global.nc.u16 	%rs15, [%rd21];
	// begin inline asm
	{add.f16 %rs13,%rs10,%rs15;
}
	// end inline asm
	cvta.to.global.u64 	%rd22, %rd7;
	add.s64 	%rd23, %rd22, %rd9;
	st.global.u16 	[%rd23], %rs13;

$L__BB16_2:
	ret;

}
	// .globl	derivativeWeight_2
.visible .entry derivativeWeight_2(
	.param .u64 derivativeWeight_2_param_0,
	.param .u64 derivativeWeight_2_param_1,
	.param .u64 derivativeWeight_2_param_2,
	.param .u64 derivativeWeight_2_param_3,
	.param .u64 derivativeWeight_2_param_4,
	.param .u64 derivativeWeight_2_param_5,
	.param .u32 derivativeWeight_2_param_6,
	.param .u32 derivativeWeight_2_param_7
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<191>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<45>;


	ld.param.u64 	%rd30, [derivativeWeight_2_param_0];
	ld.param.u64 	%rd31, [derivativeWeight_2_param_1];
	ld.param.u64 	%rd26, [derivativeWeight_2_param_2];
	ld.param.u64 	%rd27, [derivativeWeight_2_param_3];
	ld.param.u64 	%rd28, [derivativeWeight_2_param_4];
	ld.param.u64 	%rd29, [derivativeWeight_2_param_5];
	ld.param.u32 	%r11, [derivativeWeight_2_param_6];
	ld.param.u32 	%r12, [derivativeWeight_2_param_7];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB17_24;

	cvta.to.global.u64 	%rd32, %rd28;
	cvt.s64.s32 	%rd3, %r1;
	mul.wide.s32 	%rd33, %r1, 2;
	add.s64 	%rd4, %rd32, %rd33;
	ld.global.u16 	%rs190, [%rd4];
	cvta.to.global.u64 	%rd34, %rd29;
	add.s64 	%rd5, %rd34, %rd33;
	ld.global.u16 	%rs189, [%rd5];
	cvta.to.global.u64 	%rd35, %rd27;
	add.s64 	%rd36, %rd35, %rd33;
	ld.global.nc.u16 	%rs3, [%rd36];
	cvta.to.global.u64 	%rd37, %rd26;
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.u16 	%rs4, [%rd38];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB17_23;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB17_17;

	sub.s32 	%r21, %r11, %r23;
	shl.b64 	%rd39, %rd3, 1;
	add.s64 	%rd41, %rd2, %rd39;
	mul.wide.s32 	%rd7, %r12, 2;
	add.s64 	%rd42, %rd1, %rd39;
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs3;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f23, %f22;
}
	// end inline asm
	neg.ftz.f32 	%f28, %f22;

$L__BB17_4:
	ld.global.nc.u16 	%rs7, [%rd41];
	// begin inline asm
	{add.f16 %rs51,%rs190,%rs7;
}
	// end inline asm
	ld.global.nc.u16 	%rs55, [%rd42];
	// begin inline asm
	{sub.f16 %rs54,%rs55,%rs4;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs54;}

	// end inline asm
	mul.ftz.f32 	%f25, %f21, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f25;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs60,%rs178;
}
	// end inline asm
	mov.u16 	%rs64, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs60, %rs64;
  selp.u16 %rs62, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs62, 0;
	@%p4 bra 	$L__BB17_7;

	mov.f32 	%f26, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f26;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs65, %rs60;
  selp.u16 %rs66, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p5, %rs66, 0;
	@%p5 bra 	$L__BB17_7;

	fma.rn.ftz.f32 	%f29, %f28, %f25, %f21;
	fma.rn.ftz.f32 	%f27, %f23, %f29, %f25;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f27;}

	// end inline asm

$L__BB17_7:
	// begin inline asm
	{mul.f16 %rs70,%rs7,%rs178;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs73,%rs189,%rs70;
}
	// end inline asm
	add.s64 	%rd11, %rd41, %rd7;
	ld.global.nc.u16 	%rs14, [%rd11];
	// begin inline asm
	{add.f16 %rs76,%rs51,%rs14;
}
	// end inline asm
	add.s64 	%rd12, %rd42, %rd7;
	ld.global.nc.u16 	%rs80, [%rd12];
	// begin inline asm
	{sub.f16 %rs79,%rs80,%rs4;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs79;}

	// end inline asm
	mul.ftz.f32 	%f34, %f30, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs179, %f34;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs85,%rs179;
}
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs85, %rs64;
  selp.u16 %rs87, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p6, %rs87, 0;
	@%p6 bra 	$L__BB17_10;

	mov.f32 	%f35, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f35;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs90, %rs85;
  selp.u16 %rs91, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p7, %rs91, 0;
	@%p7 bra 	$L__BB17_10;

	fma.rn.ftz.f32 	%f38, %f28, %f34, %f30;
	fma.rn.ftz.f32 	%f36, %f23, %f38, %f34;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs179, %f36;}

	// end inline asm

$L__BB17_10:
	// begin inline asm
	{mul.f16 %rs95,%rs14,%rs179;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs98,%rs73,%rs95;
}
	// end inline asm
	add.s64 	%rd13, %rd11, %rd7;
	ld.global.nc.u16 	%rs21, [%rd13];
	// begin inline asm
	{add.f16 %rs101,%rs76,%rs21;
}
	// end inline asm
	add.s64 	%rd14, %rd12, %rd7;
	ld.global.nc.u16 	%rs105, [%rd14];
	// begin inline asm
	{sub.f16 %rs104,%rs105,%rs4;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f39, %rs104;}

	// end inline asm
	mul.ftz.f32 	%f43, %f39, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs180, %f43;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs110,%rs180;
}
	// end inline asm
	mov.u16 	%rs114, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs110, %rs114;
  selp.u16 %rs112, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p8, %rs112, 0;
	@%p8 bra 	$L__BB17_13;

	mov.f32 	%f44, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs115, %f44;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs115, %rs110;
  selp.u16 %rs116, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p9, %rs116, 0;
	@%p9 bra 	$L__BB17_13;

	fma.rn.ftz.f32 	%f47, %f28, %f43, %f39;
	fma.rn.ftz.f32 	%f45, %f23, %f47, %f43;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs180, %f45;}

	// end inline asm

$L__BB17_13:
	// begin inline asm
	{mul.f16 %rs120,%rs21,%rs180;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs123,%rs98,%rs120;
}
	// end inline asm
	add.s64 	%rd15, %rd13, %rd7;
	ld.global.nc.u16 	%rs28, [%rd15];
	// begin inline asm
	{add.f16 %rs190,%rs101,%rs28;
}
	// end inline asm
	add.s64 	%rd16, %rd14, %rd7;
	ld.global.nc.u16 	%rs130, [%rd16];
	// begin inline asm
	{sub.f16 %rs129,%rs130,%rs4;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f48, %rs129;}

	// end inline asm
	mul.ftz.f32 	%f52, %f48, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs181, %f52;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs135,%rs181;
}
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs135, %rs114;
  selp.u16 %rs137, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p10, %rs137, 0;
	@%p10 bra 	$L__BB17_16;

	mov.f32 	%f53, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs140, %f53;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs140, %rs135;
  selp.u16 %rs141, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p11, %rs141, 0;
	@%p11 bra 	$L__BB17_16;

	fma.rn.ftz.f32 	%f56, %f28, %f52, %f48;
	fma.rn.ftz.f32 	%f54, %f23, %f56, %f52;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs181, %f54;}

	// end inline asm

$L__BB17_16:
	// begin inline asm
	{mul.f16 %rs145,%rs28,%rs181;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs189,%rs123,%rs145;
}
	// end inline asm
	add.s32 	%r22, %r22, 4;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p12, %r21, 0;
	add.s64 	%rd41, %rd15, %rd7;
	add.s64 	%rd42, %rd16, %rd7;
	@%p12 bra 	$L__BB17_4;

$L__BB17_17:
	setp.eq.s32 	%p13, %r23, 0;
	@%p13 bra 	$L__BB17_23;

	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd40, %r19, 2;
	add.s64 	%rd44, %rd1, %rd40;
	mul.wide.s32 	%rd20, %r12, 2;
	add.s64 	%rd43, %rd2, %rd40;
	// begin inline asm
	{  cvt.f32.f16 %f58, %rs3;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f59, %f58;
}
	// end inline asm
	neg.ftz.f32 	%f64, %f58;

$L__BB17_19:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs41, [%rd43];
	// begin inline asm
	{add.f16 %rs190,%rs190,%rs41;
}
	// end inline asm
	ld.global.nc.u16 	%rs155, [%rd44];
	// begin inline asm
	{sub.f16 %rs154,%rs155,%rs4;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f57, %rs154;}

	// end inline asm
	mul.ftz.f32 	%f61, %f57, %f59;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs188, %f61;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs160,%rs188;
}
	// end inline asm
	mov.u16 	%rs164, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs160, %rs164;
  selp.u16 %rs162, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p14, %rs162, 0;
	@%p14 bra 	$L__BB17_22;

	mov.f32 	%f62, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs165, %f62;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs165, %rs160;
  selp.u16 %rs166, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p15, %rs166, 0;
	@%p15 bra 	$L__BB17_22;

	fma.rn.ftz.f32 	%f65, %f64, %f61, %f57;
	fma.rn.ftz.f32 	%f63, %f59, %f65, %f61;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs188, %f63;}

	// end inline asm

$L__BB17_22:
	// begin inline asm
	{mul.f16 %rs170,%rs41,%rs188;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs189,%rs189,%rs170;
}
	// end inline asm
	add.s64 	%rd44, %rd44, %rd20;
	add.s64 	%rd43, %rd43, %rd20;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p16, %r23, 0;
	@%p16 bra 	$L__BB17_19;

$L__BB17_23:
	st.global.u16 	[%rd4], %rs190;
	st.global.u16 	[%rd5], %rs189;

$L__BB17_24:
	ret;

}
	// .globl	findVariance_part
.visible .entry findVariance_part(
	.param .u64 findVariance_part_param_0,
	.param .u64 findVariance_part_param_1,
	.param .u64 findVariance_part_param_2,
	.param .u32 findVariance_part_param_3,
	.param .u32 findVariance_part_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<61>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd11, [findVariance_part_param_0];
	ld.param.u64 	%rd9, [findVariance_part_param_1];
	ld.param.u64 	%rd10, [findVariance_part_param_2];
	ld.param.u32 	%r13, [findVariance_part_param_3];
	ld.param.u32 	%r12, [findVariance_part_param_4];
	cvta.to.global.u64 	%rd1, %rd11;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r1, %r15, %r14, %r16;
	setp.ge.s32 	%p1, %r1, %r13;
	@%p1 bra 	$L__BB18_9;

	cvta.to.global.u64 	%rd12, %rd9;
	ld.const.u16 	%rs60, [sh];
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd13, %r1, 2;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u16 	%rs2, [%rd14];
	mul.lo.s32 	%r20, %r1, %r12;
	setp.lt.s32 	%p2, %r12, 1;
	@%p2 bra 	$L__BB18_8;

	add.s32 	%r17, %r12, -1;
	and.b32  	%r21, %r12, 3;
	setp.lt.u32 	%p3, %r17, 3;
	@%p3 bra 	$L__BB18_5;

	sub.s32 	%r19, %r12, %r21;
	mul.wide.s32 	%rd15, %r20, 2;
	add.s64 	%rd16, %rd1, %rd15;
	add.s64 	%rd21, %rd16, 4;

$L__BB18_4:
	ld.global.nc.u16 	%rs12, [%rd21+-4];
	// begin inline asm
	{sub.f16 %rs11,%rs12,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs14,%rs11,%rs11;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs17,%rs60,%rs14;
}
	// end inline asm
	ld.global.nc.u16 	%rs21, [%rd21+-2];
	// begin inline asm
	{sub.f16 %rs20,%rs21,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs23,%rs20,%rs20;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs17,%rs23;
}
	// end inline asm
	ld.global.nc.u16 	%rs30, [%rd21];
	// begin inline asm
	{sub.f16 %rs29,%rs30,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs32,%rs29,%rs29;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs35,%rs26,%rs32;
}
	// end inline asm
	ld.global.nc.u16 	%rs39, [%rd21+2];
	// begin inline asm
	{sub.f16 %rs38,%rs39,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs41,%rs38,%rs38;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs60,%rs35,%rs41;
}
	// end inline asm
	add.s32 	%r20, %r20, 4;
	add.s64 	%rd21, %rd21, 8;
	add.s32 	%r19, %r19, -4;
	setp.ne.s32 	%p4, %r19, 0;
	@%p4 bra 	$L__BB18_4;

$L__BB18_5:
	setp.eq.s32 	%p5, %r21, 0;
	@%p5 bra 	$L__BB18_8;

	mul.wide.s32 	%rd17, %r20, 2;
	add.s64 	%rd22, %rd1, %rd17;

$L__BB18_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs48, [%rd22];
	// begin inline asm
	{sub.f16 %rs47,%rs48,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs50,%rs47,%rs47;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs60,%rs60,%rs50;
}
	// end inline asm
	add.s64 	%rd22, %rd22, 2;
	add.s32 	%r21, %r21, -1;
	setp.ne.s32 	%p6, %r21, 0;
	@%p6 bra 	$L__BB18_7;

$L__BB18_8:
	cvta.to.global.u64 	%rd18, %rd10;
	shl.b64 	%rd19, %rd2, 1;
	add.s64 	%rd20, %rd18, %rd19;
	st.global.u16 	[%rd20], %rs60;

$L__BB18_9:
	ret;

}
	// .globl	normalization_part_2
.visible .entry normalization_part_2(
	.param .u64 normalization_part_2_param_0,
	.param .u64 normalization_part_2_param_1,
	.param .u64 normalization_part_2_param_2,
	.param .u64 normalization_part_2_param_3,
	.param .u64 normalization_part_2_param_4,
	.param .u64 normalization_part_2_param_5,
	.param .u64 normalization_part_2_param_6,
	.param .u32 normalization_part_2_param_7,
	.param .u32 normalization_part_2_param_8
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<28>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd2, [normalization_part_2_param_0];
	ld.param.u64 	%rd3, [normalization_part_2_param_1];
	ld.param.u64 	%rd4, [normalization_part_2_param_2];
	ld.param.u64 	%rd5, [normalization_part_2_param_3];
	ld.param.u64 	%rd6, [normalization_part_2_param_4];
	ld.param.u64 	%rd7, [normalization_part_2_param_5];
	ld.param.u64 	%rd8, [normalization_part_2_param_6];
	ld.param.u32 	%r4, [normalization_part_2_param_7];
	ld.param.u32 	%r3, [normalization_part_2_param_8];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB19_5;

	cvta.to.global.u64 	%rd9, %rd2;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	cvt.s64.s32 	%rd1, %r11;
	mul.wide.s32 	%rd10, %r11, 2;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.nc.u16 	%rs6, [%rd11];
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r1, 2;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u16 	%rs7, [%rd14];
	// begin inline asm
	{sub.f16 %rs5,%rs6,%rs7;
}
	// end inline asm
	cvta.to.global.u64 	%rd15, %rd4;
	add.s64 	%rd16, %rd15, %rd13;
	ld.global.nc.u16 	%rs9, [%rd16];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs9;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs11,%rs27;
}
	// end inline asm
	mov.u16 	%rs15, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs11, %rs15;
  selp.u16 %rs13, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs13, 0;
	@%p4 bra 	$L__BB19_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs16, %rs11;
  selp.u16 %rs17, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p5, %rs17, 0;
	@%p5 bra 	$L__BB19_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f11;}

	// end inline asm

$L__BB19_4:
	cvta.to.global.u64 	%rd17, %rd6;
	mul.wide.s32 	%rd18, %r2, 2;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.nc.u16 	%rs23, [%rd19];
	// begin inline asm
	{mul.f16 %rs21,%rs27,%rs23;
}
	// end inline asm
	cvta.to.global.u64 	%rd20, %rd7;
	add.s64 	%rd21, %rd20, %rd18;
	ld.global.nc.u16 	%rs26, [%rd21];
	// begin inline asm
	{add.f16 %rs24,%rs21,%rs26;
}
	// end inline asm
	cvta.to.global.u64 	%rd22, %rd8;
	shl.b64 	%rd23, %rd1, 1;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u16 	[%rd24], %rs24;
	cvta.to.global.u64 	%rd25, %rd5;
	add.s64 	%rd26, %rd25, %rd23;
	st.global.u16 	[%rd26], %rs27;

$L__BB19_5:
	ret;

}
	// .globl	addMatrix
.visible .entry addMatrix(
	.param .u64 addMatrix_param_0,
	.param .u64 addMatrix_param_1,
	.param .u32 addMatrix_param_2,
	.param .u32 addMatrix_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<30>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd13, [addMatrix_param_0];
	ld.param.u64 	%rd12, [addMatrix_param_1];
	ld.param.u32 	%r11, [addMatrix_param_2];
	ld.param.u32 	%r12, [addMatrix_param_3];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB20_9;

	cvta.to.global.u64 	%rd14, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd15, %r1, 2;
	add.s64 	%rd3, %rd14, %rd15;
	ld.global.u16 	%rs29, [%rd3];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB20_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB20_5;

	sub.s32 	%r21, %r11, %r23;
	shl.b64 	%rd16, %rd2, 1;
	add.s64 	%rd21, %rd1, %rd16;
	mul.wide.s32 	%rd5, %r12, 2;

$L__BB20_4:
	ld.global.nc.u16 	%rs12, [%rd21];
	// begin inline asm
	{add.f16 %rs10,%rs29,%rs12;
}
	// end inline asm
	add.s64 	%rd17, %rd21, %rd5;
	ld.global.nc.u16 	%rs15, [%rd17];
	// begin inline asm
	{add.f16 %rs13,%rs10,%rs15;
}
	// end inline asm
	add.s64 	%rd18, %rd17, %rd5;
	ld.global.nc.u16 	%rs18, [%rd18];
	// begin inline asm
	{add.f16 %rs16,%rs13,%rs18;
}
	// end inline asm
	add.s64 	%rd19, %rd18, %rd5;
	add.s64 	%rd21, %rd19, %rd5;
	ld.global.nc.u16 	%rs21, [%rd19];
	// begin inline asm
	{add.f16 %rs29,%rs16,%rs21;
}
	// end inline asm
	add.s32 	%r22, %r22, 4;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB20_4;

$L__BB20_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB20_8;

	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd20, %r19, 2;
	add.s64 	%rd22, %rd1, %rd20;
	mul.wide.s32 	%rd9, %r12, 2;

$L__BB20_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs24, [%rd22];
	// begin inline asm
	{add.f16 %rs29,%rs29,%rs24;
}
	// end inline asm
	add.s64 	%rd22, %rd22, %rd9;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB20_7;

$L__BB20_8:
	st.global.u16 	[%rd3], %rs29;

$L__BB20_9:
	ret;

}
	// .globl	reverse
.visible .entry reverse(
	.param .u64 reverse_param_0,
	.param .u32 reverse_param_1,
	.param .u32 reverse_param_2,
	.param .u32 reverse_param_3
)
{



	ret;

}
	// .globl	set2
.visible .entry set2(
	.param .u64 set2_param_0,
	.param .u32 set2_param_1,
	.param .u32 set2_param_2,
	.param .u32 set2_param_3,
	.param .u32 set2_param_4,
	.param .u32 set2_param_5,
	.param .align 2 .b8 set2_param_6[2]
)
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs1, [set2_param_6];
	ld.param.u64 	%rd1, [set2_param_0];
	ld.param.u32 	%r1, [set2_param_1];
	ld.param.u32 	%r2, [set2_param_2];
	ld.param.u32 	%r3, [set2_param_3];
	ld.param.u32 	%r4, [set2_param_4];
	ld.param.u32 	%r5, [set2_param_5];
	cvta.to.global.u64 	%rd2, %rd1;
	mad.lo.s32 	%r6, %r4, %r1, %r2;
	mad.lo.s32 	%r7, %r6, %r5, %r3;
	mul.wide.s32 	%rd3, %r7, 2;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u16 	[%rd4], %rs1;
	ret;

}
	// .globl	transpose_naive
.visible .entry transpose_naive(
	.param .u64 transpose_naive_param_0,
	.param .u64 transpose_naive_param_1,
	.param .u32 transpose_naive_param_2,
	.param .u32 transpose_naive_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_naive_param_0];
	ld.param.u64 	%rd2, [transpose_naive_param_1];
	ld.param.u32 	%r3, [transpose_naive_param_2];
	ld.param.u32 	%r4, [transpose_naive_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB23_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	mul.wide.u32 	%rd4, %r11, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r12, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB23_2:
	ret;

}
	// .globl	sharedMem_transpose
.visible .entry sharedMem_transpose(
	.param .u64 sharedMem_transpose_param_0,
	.param .u64 sharedMem_transpose_param_1,
	.param .u32 sharedMem_transpose_param_2,
	.param .u32 sharedMem_transpose_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ19sharedMem_transposeE8M_Shared[2048];

	ld.param.u64 	%rd1, [sharedMem_transpose_param_0];
	ld.param.u64 	%rd2, [sharedMem_transpose_param_1];
	ld.param.u32 	%r5, [sharedMem_transpose_param_2];
	ld.param.u32 	%r6, [sharedMem_transpose_param_3];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	shl.b32 	%r11, %r10, 5;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r2, %r11, %r12;
	setp.lt.s32 	%p2, %r1, %r5;
	setp.lt.s32 	%p3, %r2, %r6;
	and.pred  	%p1, %p2, %p3;
	shl.b32 	%r13, %r12, 6;
	mov.u32 	%r14, _ZZ19sharedMem_transposeE8M_Shared;
	add.s32 	%r15, %r14, %r13;
	shl.b32 	%r16, %r9, 1;
	add.s32 	%r3, %r15, %r16;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB24_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r17, %r1, %r6, %r2;
	mul.wide.s32 	%rd4, %r17, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	st.shared.u16 	[%r3], %rs1;

$L__BB24_2:
	bar.sync 	0;
	mad.lo.s32 	%r4, %r2, %r5, %r1;
	@%p4 bra 	$L__BB24_4;

	ld.shared.u16 	%rs2, [%r3];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r4, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB24_4:
	ret;

}
	// .globl	matrixTransposeSolveBankConflicts
.visible .entry matrixTransposeSolveBankConflicts(
	.param .u64 matrixTransposeSolveBankConflicts_param_0,
	.param .u64 matrixTransposeSolveBankConflicts_param_1,
	.param .u32 matrixTransposeSolveBankConflicts_param_2,
	.param .u32 matrixTransposeSolveBankConflicts_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ33matrixTransposeSolveBankConflictsE3mat[2112];

	ld.param.u64 	%rd1, [matrixTransposeSolveBankConflicts_param_0];
	ld.param.u64 	%rd2, [matrixTransposeSolveBankConflicts_param_1];
	ld.param.u32 	%r7, [matrixTransposeSolveBankConflicts_param_2];
	ld.param.u32 	%r8, [matrixTransposeSolveBankConflicts_param_3];
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 5;
	mov.u32 	%r11, %ctaid.y;
	shl.b32 	%r12, %r11, 5;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r12, %r1;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r10, %r3;
	add.s32 	%r5, %r10, %r1;
	add.s32 	%r6, %r12, %r3;
	setp.ge.s32 	%p1, %r2, %r7;
	setp.ge.s32 	%p2, %r4, %r8;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB25_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r13, %r2, %r8, %r4;
	mul.wide.s32 	%rd4, %r13, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	mov.u32 	%r14, _ZZ33matrixTransposeSolveBankConflictsE3mat;
	mad.lo.s32 	%r15, %r1, 66, %r14;
	shl.b32 	%r16, %r3, 1;
	add.s32 	%r17, %r15, %r16;
	st.shared.u16 	[%r17], %rs1;

$L__BB25_2:
	bar.sync 	0;
	setp.ge.s32 	%p4, %r5, %r7;
	setp.ge.s32 	%p5, %r6, %r8;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB25_4;

	mad.lo.s32 	%r18, %r5, %r7, %r6;
	mov.u32 	%r19, _ZZ33matrixTransposeSolveBankConflictsE3mat;
	mad.lo.s32 	%r20, %r3, 66, %r19;
	shl.b32 	%r21, %r1, 1;
	add.s32 	%r22, %r20, %r21;
	ld.shared.u16 	%rs2, [%r22];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB25_4:
	ret;

}
	// .globl	transposeV3
.visible .entry transposeV3(
	.param .u64 transposeV3_param_0,
	.param .u64 transposeV3_param_1,
	.param .u32 transposeV3_param_2,
	.param .u32 transposeV3_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ11transposeV3E3s_A[2112];

	ld.param.u64 	%rd1, [transposeV3_param_0];
	ld.param.u64 	%rd2, [transposeV3_param_1];
	ld.param.u32 	%r9, [transposeV3_param_2];
	ld.param.u32 	%r10, [transposeV3_param_3];
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %ntid.y;
	mul.lo.s32 	%r4, %r14, %r13;
	mov.u32 	%r5, %tid.y;
	add.s32 	%r6, %r4, %r5;
	setp.ge.s32 	%p1, %r3, %r9;
	setp.ge.s32 	%p2, %r6, %r10;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB26_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r15, %r6, %r10, %r3;
	mul.wide.s32 	%rd4, %r15, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	mov.u32 	%r16, _ZZ11transposeV3E3s_A;
	mad.lo.s32 	%r17, %r5, 66, %r16;
	shl.b32 	%r18, %r2, 1;
	add.s32 	%r19, %r17, %r18;
	st.shared.u16 	[%r19], %rs1;

$L__BB26_2:
	bar.sync 	0;
	add.s32 	%r7, %r4, %r2;
	setp.ge.s32 	%p4, %r7, %r9;
	add.s32 	%r8, %r1, %r5;
	setp.ge.s32 	%p5, %r8, %r10;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB26_4;

	mad.lo.s32 	%r20, %r8, %r9, %r7;
	mov.u32 	%r21, _ZZ11transposeV3E3s_A;
	mad.lo.s32 	%r22, %r2, 66, %r21;
	shl.b32 	%r23, %r5, 1;
	add.s32 	%r24, %r22, %r23;
	ld.shared.u16 	%rs2, [%r24];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r20, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB26_4:
	ret;

}
	// .globl	matrixDiv
.visible .entry matrixDiv(
	.param .u64 matrixDiv_param_0,
	.param .align 2 .b8 matrixDiv_param_1[2],
	.param .u32 matrixDiv_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs5, [matrixDiv_param_1];
	ld.param.u64 	%rd2, [matrixDiv_param_0];
	ld.param.u32 	%r2, [matrixDiv_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB27_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs6, [%rd1];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs6;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs5;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs9,%rs19;
}
	// end inline asm
	mov.u16 	%rs13, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs9, %rs13;
  selp.u16 %rs11, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs11, 0;
	@%p2 bra 	$L__BB27_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs14, %rs9;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs15, 0;
	@%p3 bra 	$L__BB27_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f11;}

	// end inline asm

$L__BB27_4:
	st.global.u16 	[%rd1], %rs19;

$L__BB27_5:
	ret;

}
	// .globl	addCopy
.visible .entry addCopy(
	.param .u64 addCopy_param_0,
	.param .u64 addCopy_param_1,
	.param .u32 addCopy_param_2,
	.param .u32 addCopy_param_3,
	.param .u32 addCopy_param_4,
	.param .u32 addCopy_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addCopy_param_0];
	ld.param.u64 	%rd2, [addCopy_param_1];
	ld.param.u32 	%r7, [addCopy_param_2];
	ld.param.u32 	%r4, [addCopy_param_3];
	ld.param.u32 	%r5, [addCopy_param_4];
	ld.param.u32 	%r6, [addCopy_param_5];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %ntid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	mov.u32 	%r14, %nctaid.y;
	mul.lo.s32 	%r15, %r14, %r12;
	mad.lo.s32 	%r3, %r15, %r1, %r2;
	mul.lo.s32 	%r16, %r5, %r7;
	setp.ge.s32 	%p1, %r3, %r16;
	@%p1 bra 	$L__BB28_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r17, %r1, %r4, %r2;
	mad.lo.s32 	%r18, %r6, %r5, %r17;
	mul.wide.s32 	%rd4, %r3, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB28_2:
	ret;

}
	// .globl	normalization_part_1
.visible .entry normalization_part_1(
	.param .u64 normalization_part_1_param_0,
	.param .u64 normalization_part_1_param_1,
	.param .u32 normalization_part_1_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [normalization_part_1_param_0];
	ld.param.u64 	%rd2, [normalization_part_1_param_1];
	ld.param.u32 	%r2, [normalization_part_1_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB29_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs2, [%rd5];
	ld.const.u16 	%rs3, [sh+10];
	// begin inline asm
	{add.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs1;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs4,r;     
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs4;

$L__BB29_2:
	ret;

}
	// .globl	DenseLayerForward
.visible .entry DenseLayerForward(
	.param .u64 DenseLayerForward_param_0,
	.param .u64 DenseLayerForward_param_1,
	.param .u64 DenseLayerForward_param_2,
	.param .u32 DenseLayerForward_param_3,
	.param .u32 DenseLayerForward_param_4,
	.param .u32 DenseLayerForward_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<34>;
	.reg .b32 	%r<34>;
	.reg .b64 	%rd<94>;


	ld.param.u64 	%rd22, [DenseLayerForward_param_0];
	ld.param.u64 	%rd24, [DenseLayerForward_param_1];
	ld.param.u64 	%rd23, [DenseLayerForward_param_2];
	ld.param.u32 	%r17, [DenseLayerForward_param_3];
	ld.param.u32 	%r16, [DenseLayerForward_param_4];
	ld.param.u32 	%r18, [DenseLayerForward_param_5];
	cvta.to.global.u64 	%rd1, %rd24;
	mov.u32 	%r19, %ctaid.x;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r1, %r20, %r19, %r21;
	mov.u32 	%r22, %ctaid.y;
	mov.u32 	%r23, %ntid.y;
	mov.u32 	%r24, %tid.y;
	mad.lo.s32 	%r2, %r23, %r22, %r24;
	setp.ge.s32 	%p1, %r1, %r18;
	setp.ge.s32 	%p2, %r2, %r17;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB30_9;

	mul.lo.s32 	%r32, %r2, %r16;
	cvta.to.global.u64 	%rd2, %rd22;
	cvt.s64.s32 	%rd3, %r1;
	ld.global.u64 	%rd25, [%rd2+8];
	mul.wide.s32 	%rd26, %r1, 8;
	add.s64 	%rd27, %rd25, %rd26;
	cvt.s64.s32 	%rd4, %r2;
	ld.u64 	%rd93, [%rd27];
	mul.wide.s32 	%rd28, %r2, 2;
	add.s64 	%rd29, %rd93, %rd28;
	ld.u16 	%rs1, [%rd29];
	setp.lt.s32 	%p4, %r16, 1;
	@%p4 bra 	$L__BB30_8;

	add.s32 	%r26, %r16, -1;
	and.b32  	%r33, %r16, 3;
	setp.lt.u32 	%p5, %r26, 3;
	mov.u32 	%r31, 0;
	@%p5 bra 	$L__BB30_5;

	sub.s32 	%r30, %r16, %r33;
	mul.wide.s32 	%rd32, %r32, 2;
	add.s64 	%rd6, %rd1, %rd32;
	mov.u64 	%rd86, 4;
	shl.b64 	%rd34, %rd3, 3;

$L__BB30_4:
	ld.global.u64 	%rd33, [%rd2];
	add.s64 	%rd35, %rd33, %rd34;
	ld.u64 	%rd36, [%rd35];
	add.s64 	%rd37, %rd36, %rd86;
	ld.u16 	%rs3, [%rd37+-4];
	add.s64 	%rd38, %rd6, %rd86;
	ld.global.u16 	%rs4, [%rd38+-4];
	// begin inline asm
	{mul.f16 %rs2,%rs3,%rs4;
}
	// end inline asm
	shl.b64 	%rd39, %rd4, 1;
	add.s64 	%rd40, %rd93, %rd39;
	ld.u16 	%rs6, [%rd40];
	// begin inline asm
	{add.f16 %rs5,%rs6,%rs2;
}
	// end inline asm
	st.u16 	[%rd40], %rs5;
	ld.global.u64 	%rd41, [%rd2+8];
	add.s64 	%rd42, %rd41, %rd34;
	ld.u64 	%rd43, [%rd42];
	ld.global.u64 	%rd44, [%rd2];
	add.s64 	%rd45, %rd44, %rd34;
	ld.u64 	%rd46, [%rd45];
	add.s64 	%rd47, %rd46, %rd86;
	ld.u16 	%rs9, [%rd47+-2];
	ld.global.u16 	%rs10, [%rd38+-2];
	// begin inline asm
	{mul.f16 %rs8,%rs9,%rs10;
}
	// end inline asm
	add.s64 	%rd48, %rd43, %rd39;
	ld.u16 	%rs12, [%rd48];
	// begin inline asm
	{add.f16 %rs11,%rs12,%rs8;
}
	// end inline asm
	st.u16 	[%rd48], %rs11;
	ld.global.u64 	%rd49, [%rd2+8];
	add.s64 	%rd50, %rd49, %rd34;
	ld.u64 	%rd51, [%rd50];
	ld.global.u64 	%rd52, [%rd2];
	add.s64 	%rd53, %rd52, %rd34;
	ld.u64 	%rd54, [%rd53];
	add.s64 	%rd55, %rd54, %rd86;
	ld.u16 	%rs15, [%rd55];
	ld.global.u16 	%rs16, [%rd38];
	// begin inline asm
	{mul.f16 %rs14,%rs15,%rs16;
}
	// end inline asm
	add.s64 	%rd56, %rd51, %rd39;
	ld.u16 	%rs18, [%rd56];
	// begin inline asm
	{add.f16 %rs17,%rs18,%rs14;
}
	// end inline asm
	st.u16 	[%rd56], %rs17;
	ld.global.u64 	%rd57, [%rd2+8];
	add.s64 	%rd58, %rd57, %rd34;
	ld.u64 	%rd59, [%rd58];
	ld.global.u64 	%rd60, [%rd2];
	add.s64 	%rd61, %rd60, %rd34;
	ld.u64 	%rd62, [%rd61];
	add.s64 	%rd63, %rd62, %rd86;
	ld.u16 	%rs21, [%rd63+2];
	ld.global.u16 	%rs22, [%rd38+2];
	// begin inline asm
	{mul.f16 %rs20,%rs21,%rs22;
}
	// end inline asm
	add.s64 	%rd64, %rd59, %rd39;
	ld.u16 	%rs24, [%rd64];
	// begin inline asm
	{add.f16 %rs23,%rs24,%rs20;
}
	// end inline asm
	st.u16 	[%rd64], %rs23;
	add.s32 	%r31, %r31, 4;
	add.s32 	%r32, %r32, 4;
	ld.global.u64 	%rd65, [%rd2+8];
	add.s64 	%rd66, %rd65, %rd34;
	ld.u64 	%rd93, [%rd66];
	add.s64 	%rd86, %rd86, 8;
	add.s32 	%r30, %r30, -4;
	setp.ne.s32 	%p6, %r30, 0;
	@%p6 bra 	$L__BB30_4;

$L__BB30_5:
	setp.eq.s32 	%p7, %r33, 0;
	@%p7 bra 	$L__BB30_8;

	mul.wide.s32 	%rd91, %r31, 2;
	mul.wide.s32 	%rd67, %r32, 2;
	add.s64 	%rd90, %rd1, %rd67;

$L__BB30_7:
	.pragma "nounroll";
	ld.global.u64 	%rd68, [%rd2];
	shl.b64 	%rd69, %rd3, 3;
	add.s64 	%rd70, %rd68, %rd69;
	ld.u64 	%rd71, [%rd70];
	add.s64 	%rd72, %rd71, %rd91;
	ld.u16 	%rs27, [%rd72];
	ld.global.u16 	%rs28, [%rd90];
	// begin inline asm
	{mul.f16 %rs26,%rs27,%rs28;
}
	// end inline asm
	shl.b64 	%rd73, %rd4, 1;
	add.s64 	%rd74, %rd93, %rd73;
	ld.u16 	%rs30, [%rd74];
	// begin inline asm
	{add.f16 %rs29,%rs30,%rs26;
}
	// end inline asm
	st.u16 	[%rd74], %rs29;
	ld.global.u64 	%rd75, [%rd2+8];
	add.s64 	%rd76, %rd75, %rd69;
	ld.u64 	%rd93, [%rd76];
	add.s64 	%rd91, %rd91, 2;
	add.s64 	%rd90, %rd90, 2;
	add.s32 	%r33, %r33, -1;
	setp.ne.s32 	%p8, %r33, 0;
	@%p8 bra 	$L__BB30_7;

$L__BB30_8:
	shl.b64 	%rd78, %rd4, 1;
	add.s64 	%rd79, %rd93, %rd78;
	st.u16 	[%rd79], %rs1;
	ld.global.u64 	%rd80, [%rd2+8];
	shl.b64 	%rd81, %rd3, 3;
	add.s64 	%rd82, %rd80, %rd81;
	ld.u64 	%rd83, [%rd82];
	add.s64 	%rd77, %rd83, %rd78;
	cvta.to.global.u64 	%rd84, %rd23;
	add.s64 	%rd85, %rd84, %rd78;
	ld.global.nc.u16 	%rs33, [%rd85];
	// begin inline asm
	{ atom.add.noftz.f16 %rs32,[%rd77],%rs33; }

	// end inline asm

$L__BB30_9:
	ret;

}
	// .globl	dot
.visible .entry dot(
	.param .u64 dot_param_0,
	.param .u64 dot_param_1,
	.param .u64 dot_param_2,
	.param .u32 dot_param_3,
	.param .u32 dot_param_4,
	.param .u32 dot_param_5,
	.param .u32 dot_param_6
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd18, [dot_param_0];
	ld.param.u64 	%rd19, [dot_param_1];
	ld.param.u64 	%rd17, [dot_param_2];
	ld.param.u32 	%r14, [dot_param_3];
	ld.param.u32 	%r12, [dot_param_4];
	ld.param.u32 	%r13, [dot_param_6];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ctaid.y;
	mov.u32 	%r19, %ntid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r2, %r19, %r18, %r20;
	setp.ge.s32 	%p1, %r1, %r14;
	setp.ge.s32 	%p2, %r2, %r13;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB31_9;

	ld.const.u16 	%rs44, [sh];
	setp.lt.s32 	%p4, %r12, 1;
	@%p4 bra 	$L__BB31_8;

	add.s32 	%r22, %r12, -1;
	and.b32  	%r31, %r12, 3;
	setp.lt.u32 	%p5, %r22, 3;
	mov.u32 	%r30, 0;
	@%p5 bra 	$L__BB31_5;

	sub.s32 	%r29, %r12, %r31;
	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd32, %rd1, %rd20;
	mul.lo.s32 	%r24, %r12, %r1;
	mul.wide.s32 	%rd21, %r24, 2;
	add.s64 	%rd22, %rd2, %rd21;
	add.s64 	%rd31, %rd22, 4;
	mul.wide.s32 	%rd5, %r13, 2;

$L__BB31_4:
	ld.global.nc.u16 	%rs11, [%rd31+-4];
	ld.global.nc.u16 	%rs12, [%rd32];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs12;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs44,%rs10;
}
	// end inline asm
	ld.global.nc.u16 	%rs17, [%rd31+-2];
	add.s64 	%rd23, %rd32, %rd5;
	ld.global.nc.u16 	%rs18, [%rd23];
	// begin inline asm
	{mul.f16 %rs16,%rs17,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs19,%rs13,%rs16;
}
	// end inline asm
	ld.global.nc.u16 	%rs23, [%rd31];
	add.s64 	%rd24, %rd23, %rd5;
	ld.global.nc.u16 	%rs24, [%rd24];
	// begin inline asm
	{mul.f16 %rs22,%rs23,%rs24;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd31+2];
	add.s64 	%rd25, %rd24, %rd5;
	add.s64 	%rd32, %rd25, %rd5;
	ld.global.nc.u16 	%rs30, [%rd25];
	// begin inline asm
	{mul.f16 %rs28,%rs29,%rs30;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs25,%rs28;
}
	// end inline asm
	add.s32 	%r30, %r30, 4;
	add.s64 	%rd31, %rd31, 8;
	add.s32 	%r29, %r29, -4;
	setp.ne.s32 	%p6, %r29, 0;
	@%p6 bra 	$L__BB31_4;

$L__BB31_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB31_8;

	mad.lo.s32 	%r25, %r30, %r13, %r2;
	mul.wide.s32 	%rd26, %r25, 2;
	add.s64 	%rd34, %rd1, %rd26;
	mul.wide.s32 	%rd11, %r13, 2;
	mad.lo.s32 	%r26, %r12, %r1, %r30;
	mul.wide.s32 	%rd27, %r26, 2;
	add.s64 	%rd33, %rd2, %rd27;

$L__BB31_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs35, [%rd33];
	ld.global.nc.u16 	%rs36, [%rd34];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs36;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs44,%rs34;
}
	// end inline asm
	add.s64 	%rd34, %rd34, %rd11;
	add.s64 	%rd33, %rd33, 2;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB31_7;

$L__BB31_8:
	mad.lo.s32 	%r27, %r1, %r13, %r2;
	cvta.to.global.u64 	%rd28, %rd17;
	mul.wide.s32 	%rd29, %r27, 2;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u16 	[%rd30], %rs44;

$L__BB31_9:
	ret;

}
	// .globl	dropout
.visible .entry dropout(
	.param .u64 dropout_param_0,
	.param .u64 dropout_param_1,
	.param .align 2 .b8 dropout_param_2[2],
	.param .u32 dropout_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<30>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs6, [dropout_param_2];
	ld.param.u64 	%rd2, [dropout_param_0];
	ld.param.u64 	%rd3, [dropout_param_1];
	ld.param.u32 	%r2, [dropout_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB32_6;

	ld.const.u16 	%rs8, [sh+8];
	// begin inline asm
	{sub.f16 %rs7,%rs8,%rs6;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs8;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs7;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs13,%rs29;
}
	// end inline asm
	mov.u16 	%rs17, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs13, %rs17;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs15, 0;
	@%p2 bra 	$L__BB32_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs13;
  selp.u16 %rs19, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs19, 0;
	@%p3 bra 	$L__BB32_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f11;}

	// end inline asm

$L__BB32_4:
	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs24, [%rd6];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs24, %rs6;
  selp.u16 %rs23, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs23, 0;
	@%p4 bra 	$L__BB32_6;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u16 	%rs27, [%rd9];
	// begin inline asm
	{mul.f16 %rs26,%rs27,%rs29;
}
	// end inline asm
	st.global.u16 	[%rd9], %rs26;

$L__BB32_6:
	ret;

}
	// .globl	sub_gpu
.visible .entry sub_gpu(
	.param .u64 sub_gpu_param_0,
	.param .u64 sub_gpu_param_1,
	.param .u64 sub_gpu_param_2,
	.param .u32 sub_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_param_0];
	ld.param.u64 	%rd2, [sub_gpu_param_1];
	ld.param.u64 	%rd3, [sub_gpu_param_2];
	ld.param.u32 	%r2, [sub_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB33_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	add.s64 	%rd8, %rd4, %rd6;
	ld.global.nc.u32 	%r7, [%rd8];
	cvta.to.global.u64 	%rd9, %rd2;
	add.s64 	%rd10, %rd9, %rd6;
	ld.global.nc.u32 	%r8, [%rd10];
	// begin inline asm
	{sub.f16x2 %r6,%r7,%r8;
}
	// end inline asm
	st.global.u32 	[%rd7], %r6;

$L__BB33_2:
	ret;

}
	// .globl	mul
.visible .entry mul(
	.param .u64 mul_param_0,
	.param .align 2 .b8 mul_param_1[2],
	.param .u32 mul_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs1, [mul_param_1];
	ld.param.u64 	%rd1, [mul_param_0];
	ld.param.u32 	%r2, [mul_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB34_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u16 	%rs3, [%rd4];
	// begin inline asm
	{mul.f16 %rs2,%rs3,%rs1;
}
	// end inline asm
	st.global.u16 	[%rd4], %rs2;

$L__BB34_2:
	ret;

}
	// .globl	clip
.visible .entry clip(
	.param .u64 clip_param_0,
	.param .align 2 .b8 clip_param_1[2],
	.param .align 2 .b8 clip_param_2[2],
	.param .u32 clip_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs5, [clip_param_2];
	ld.param.u16 	%rs4, [clip_param_1];
	ld.param.u64 	%rd2, [clip_param_0];
	ld.param.u32 	%r2, [clip_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB35_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd1];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs2, %rs5;
  selp.u16 %rs6, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs6, 0;
	@%p2 bra 	$L__BB35_3;

	st.global.u16 	[%rd1], %rs5;
	bra.uni 	$L__BB35_5;

$L__BB35_3:
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs2, %rs4;
  selp.u16 %rs9, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs9, 0;
	@%p3 bra 	$L__BB35_5;

	st.global.u16 	[%rd1], %rs4;

$L__BB35_5:
	ret;

}
	// .globl	indexMaxElement
.visible .entry indexMaxElement(
	.param .u64 indexMaxElement_param_0,
	.param .u64 indexMaxElement_param_1,
	.param .u64 indexMaxElement_param_2,
	.param .u32 indexMaxElement_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [indexMaxElement_param_0];
	ld.param.u64 	%rd3, [indexMaxElement_param_1];
	ld.param.u64 	%rd4, [indexMaxElement_param_2];
	ld.param.u32 	%r2, [indexMaxElement_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB36_3;

	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u16 	%rs1, [%rd7];
	cvta.to.global.u64 	%rd1, %rd3;
	ld.global.u16 	%rs4, [%rd1];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs1, %rs4;
  selp.u16 %rs2, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs2, 0;
	@%p2 bra 	$L__BB36_3;

	st.global.u16 	[%rd1], %rs1;
	cvta.to.global.u64 	%rd8, %rd4;
	st.global.u32 	[%rd8], %r1;

$L__BB36_3:
	ret;

}
	// .globl	derVar_part_1
.visible .entry derVar_part_1(
	.param .u64 derVar_part_1_param_0,
	.param .u64 derVar_part_1_param_1,
	.param .u32 derVar_part_1_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [derVar_part_1_param_0];
	ld.param.u64 	%rd2, [derVar_part_1_param_1];
	ld.param.u32 	%r2, [derVar_part_1_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB37_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs2, [%rd5];
	ld.const.u16 	%rs3, [sh+10];
	// begin inline asm
	{add.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	// begin inline asm
	{.reg.b16         h, r;           
 .reg.b32         f;              
  mov.b16         h, %rs1;          
  cvt.f32.f16     f, h;           
  lg2.approx.ftz.f32  f, f;       
  cvt.rn.f16.f32      r, f;       
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0xA2E2U;
  mov.b16 ulp,0x8080U;
  set.eq.f16.f16 p,r, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0xBF46U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,r, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs4, r;          
}
	// end inline asm
	ld.const.u16 	%rs7, [sh+14];
	// begin inline asm
	{mul.f16 %rs6,%rs7,%rs4;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, ULP;         
 .reg.b16         r;              
  mov.b16         r,%rs6;           
  cvt.f32.f16     f,r;            
  ex2.approx.ftz.f32      f,f;    
  mov.b32         ULP, 0x33800000U;
  fma.rn.f32      f,f,ULP,f;      
  cvt.rn.f16.f32      r,f;        
  mov.b16         %rs9,r;           
}
	// end inline asm
	ld.const.u16 	%rs12, [sh+12];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs9;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs11;

$L__BB37_2:
	ret;

}
	// .globl	derVar_part_3
.visible .entry derVar_part_3(
	.param .u64 derVar_part_3_param_0,
	.param .u64 derVar_part_3_param_1,
	.param .u32 derVar_part_3_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [derVar_part_3_param_0];
	ld.param.u64 	%rd2, [derVar_part_3_param_1];
	ld.param.u32 	%r2, [derVar_part_3_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB38_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u16 	%rs3, [%rd7];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs1;

$L__BB38_2:
	ret;

}
	// .globl	derMean_part_1
.visible .entry derMean_part_1(
	.param .u64 derMean_part_1_param_0,
	.param .u64 derMean_part_1_param_1,
	.param .u32 derMean_part_1_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<24>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [derMean_part_1_param_0];
	ld.param.u64 	%rd3, [derMean_part_1_param_1];
	ld.param.u32 	%r2, [derMean_part_1_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB39_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs6, [%rd6];
	ld.const.u16 	%rs7, [sh+10];
	// begin inline asm
	{add.f16 %rs5,%rs6,%rs7;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs5;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs8,r;     
}
	// end inline asm
	ld.const.u16 	%rs10, [sh+16];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs8;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs13,%rs23;
}
	// end inline asm
	mov.u16 	%rs17, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs13, %rs17;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs15, 0;
	@%p2 bra 	$L__BB39_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs13;
  selp.u16 %rs19, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs19, 0;
	@%p3 bra 	$L__BB39_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f11;}

	// end inline asm

$L__BB39_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs23;

$L__BB39_5:
	ret;

}
	// .globl	derMean_part_3
.visible .entry derMean_part_3(
	.param .u64 derMean_part_3_param_0,
	.param .u64 derMean_part_3_param_1,
	.param .u64 derMean_part_3_param_2,
	.param .u32 derMean_part_3_param_3,
	.param .u64 derMean_part_3_param_4,
	.param .u32 derMean_part_3_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<33>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [derMean_part_3_param_0];
	ld.param.u64 	%rd3, [derMean_part_3_param_1];
	ld.param.u64 	%rd4, [derMean_part_3_param_2];
	ld.param.u32 	%r2, [derMean_part_3_param_3];
	ld.param.u64 	%rd5, [derMean_part_3_param_4];
	ld.param.u32 	%r3, [derMean_part_3_param_5];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB40_5;

	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.s32 	%rd7, %r1, 2;
	add.s64 	%rd1, %rd6, %rd7;
	ld.global.u16 	%rs7, [%rd1];
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd7;
	ld.global.nc.u16 	%rs8, [%rd9];
	// begin inline asm
	{mul.f16 %rs6,%rs7,%rs8;
}
	// end inline asm
	ld.const.u16 	%rs10, [sh+18];
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd7;
	ld.global.nc.u16 	%rs11, [%rd11];
	// begin inline asm
	{mul.f16 %rs9,%rs10,%rs11;
}
	// end inline asm
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd7;
	ld.global.nc.u16 	%rs14, [%rd13];
	// begin inline asm
	{mul.f16 %rs12,%rs9,%rs14;
}
	// end inline asm
	// begin inline asm
	cvt.rn.f16.s32 %rs15, %r2;
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs12;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs15;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs32, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs19,%rs32;
}
	// end inline asm
	mov.u16 	%rs23, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs19, %rs23;
  selp.u16 %rs21, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs21, 0;
	@%p2 bra 	$L__BB40_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs24, %rs19;
  selp.u16 %rs25, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs25, 0;
	@%p3 bra 	$L__BB40_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs32, %f11;}

	// end inline asm

$L__BB40_4:
	// begin inline asm
	{add.f16 %rs29,%rs6,%rs32;
}
	// end inline asm
	st.global.u16 	[%rd1], %rs29;

$L__BB40_5:
	ret;

}
	// .globl	derNorm_part_1
.visible .entry derNorm_part_1(
	.param .u64 derNorm_part_1_param_0,
	.param .u64 derNorm_part_1_param_1,
	.param .u32 derNorm_part_1_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<24>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [derNorm_part_1_param_0];
	ld.param.u64 	%rd3, [derNorm_part_1_param_1];
	ld.param.u32 	%r2, [derNorm_part_1_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB41_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs6, [%rd6];
	ld.const.u16 	%rs7, [sh+10];
	// begin inline asm
	{add.f16 %rs5,%rs6,%rs7;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs5;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs8,r;     
}
	// end inline asm
	ld.const.u16 	%rs10, [sh+8];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs8;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs13,%rs23;
}
	// end inline asm
	mov.u16 	%rs17, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs13, %rs17;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs15, 0;
	@%p2 bra 	$L__BB41_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs13;
  selp.u16 %rs19, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs19, 0;
	@%p3 bra 	$L__BB41_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f11;}

	// end inline asm

$L__BB41_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs23;

$L__BB41_5:
	ret;

}
	// .globl	pow2
.visible .entry pow2(
	.param .u64 pow2_param_0,
	.param .u32 pow2_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [pow2_param_0];
	ld.param.u32 	%r2, [pow2_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB42_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u16 	%rs2, [%rd4];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	st.global.u16 	[%rd4], %rs1;

$L__BB42_2:
	ret;

}
	// .globl	fisnan
.visible .entry fisnan(
	.param .u64 fisnan_param_0,
	.param .u64 fisnan_param_1,
	.param .u32 fisnan_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [fisnan_param_0];
	ld.param.u64 	%rd3, [fisnan_param_1];
	ld.param.u32 	%r2, [fisnan_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB43_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB43_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs2, [%rd6];
	// begin inline asm
	{set.nan.f16.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	setp.eq.s16 	%p3, %rs1, 0;
	@%p3 bra 	$L__BB43_4;

	st.global.u32 	[%rd1], %r1;

$L__BB43_4:
	ret;

}
	// .globl	hisinf
.visible .entry hisinf(
	.param .u64 hisinf_param_0,
	.param .u64 hisinf_param_1,
	.param .u32 hisinf_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [hisinf_param_0];
	ld.param.u64 	%rd3, [hisinf_param_1];
	ld.param.u32 	%r2, [hisinf_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB44_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB44_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	or.b16  	%rs2, %rs1, -32768;
	setp.ne.s16 	%p3, %rs2, -1024;
	setp.ne.s16 	%p4, %rs1, -1024;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB44_4;

	st.global.u32 	[%rd1], %r1;

$L__BB44_4:
	ret;

}
	// .globl	momentum
.visible .entry momentum(
	.param .u64 momentum_param_0,
	.param .u64 momentum_param_1,
	.param .align 2 .b8 momentum_param_2[2],
	.param .u32 momentum_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs1, [momentum_param_2];
	ld.param.u64 	%rd1, [momentum_param_0];
	ld.param.u64 	%rd2, [momentum_param_1];
	ld.param.u32 	%r2, [momentum_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB45_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// begin inline asm
	{mul.f16 %rs2,%rs1,%rs4;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+8];
	// begin inline asm
	{sub.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u16 	%rs9, [%rd7];
	// begin inline asm
	{mul.f16 %rs8,%rs9,%rs5;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs11,%rs2,%rs8;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs11;

$L__BB45_2:
	ret;

}
	// .globl	momentumPow2
.visible .entry momentumPow2(
	.param .u64 momentumPow2_param_0,
	.param .u64 momentumPow2_param_1,
	.param .align 2 .b8 momentumPow2_param_2[2],
	.param .u32 momentumPow2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs1, [momentumPow2_param_2];
	ld.param.u64 	%rd1, [momentumPow2_param_0];
	ld.param.u64 	%rd2, [momentumPow2_param_1];
	ld.param.u32 	%r2, [momentumPow2_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB46_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// begin inline asm
	{mul.f16 %rs2,%rs1,%rs4;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+8];
	// begin inline asm
	{sub.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u16 	%rs10, [%rd7];
	// begin inline asm
	{mul.f16 %rs8,%rs5,%rs10;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs11,%rs8,%rs10;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs2,%rs11;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs14;

$L__BB46_2:
	ret;

}
	// .globl	subDivSqrtNorm
.visible .entry subDivSqrtNorm(
	.param .u64 subDivSqrtNorm_param_0,
	.param .u64 subDivSqrtNorm_param_1,
	.param .align 2 .b8 subDivSqrtNorm_param_2[2],
	.param .align 2 .b8 subDivSqrtNorm_param_3[2],
	.param .align 2 .b8 subDivSqrtNorm_param_4[2],
	.param .u64 subDivSqrtNorm_param_5,
	.param .u32 subDivSqrtNorm_param_6
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<74>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<13>;


	ld.param.u16 	%rs17, [subDivSqrtNorm_param_4];
	ld.param.u16 	%rs16, [subDivSqrtNorm_param_3];
	ld.param.u16 	%rs15, [subDivSqrtNorm_param_2];
	ld.param.u64 	%rd2, [subDivSqrtNorm_param_0];
	ld.param.u64 	%rd3, [subDivSqrtNorm_param_1];
	ld.param.u64 	%rd4, [subDivSqrtNorm_param_5];
	ld.param.u32 	%r2, [subDivSqrtNorm_param_6];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB47_11;

	ld.const.u16 	%rs1, [sh+10];
	// begin inline asm
	{add.f16 %rs18,%rs16,%rs1;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs15;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs18;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f15, %f14;
}
	// end inline asm
	mul.ftz.f32 	%f17, %f13, %f15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f17;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs24,%rs71;
}
	// end inline asm
	mov.u16 	%rs28, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs24, %rs28;
  selp.u16 %rs26, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs26, 0;
	@%p2 bra 	$L__BB47_4;

	mov.f32 	%f18, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f18;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs29, %rs24;
  selp.u16 %rs30, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs30, 0;
	@%p3 bra 	$L__BB47_4;

	neg.ftz.f32 	%f20, %f14;
	fma.rn.ftz.f32 	%f21, %f20, %f17, %f13;
	fma.rn.ftz.f32 	%f19, %f15, %f21, %f17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f19;}

	// end inline asm

$L__BB47_4:
	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs36, [%rd7];
	// begin inline asm
	{mul.f16 %rs34,%rs71,%rs36;
}
	// end inline asm
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.nc.u16 	%rs37, [%rd9];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs17;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f24, %f23;
}
	// end inline asm
	mul.ftz.f32 	%f26, %f22, %f24;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f26;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs40,%rs72;
}
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs40, %rs28;
  selp.u16 %rs42, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs42, 0;
	@%p4 bra 	$L__BB47_7;

	mov.f32 	%f27, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f27;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs45, %rs40;
  selp.u16 %rs46, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p5, %rs46, 0;
	@%p5 bra 	$L__BB47_7;

	neg.ftz.f32 	%f29, %f23;
	fma.rn.ftz.f32 	%f30, %f29, %f26, %f22;
	fma.rn.ftz.f32 	%f28, %f24, %f30, %f26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f28;}

	// end inline asm

$L__BB47_7:
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs72;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs50,r;     
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs52,%rs50,%rs1;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs52;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f33, %f32;
}
	// end inline asm
	mul.ftz.f32 	%f35, %f31, %f33;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f35;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs58,%rs73;
}
	// end inline asm
	mov.u16 	%rs62, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs58, %rs62;
  selp.u16 %rs60, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p6, %rs60, 0;
	@%p6 bra 	$L__BB47_10;

	mov.f32 	%f36, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs63, %f36;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs63, %rs58;
  selp.u16 %rs64, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p7, %rs64, 0;
	@%p7 bra 	$L__BB47_10;

	neg.ftz.f32 	%f38, %f32;
	fma.rn.ftz.f32 	%f39, %f38, %f35, %f31;
	fma.rn.ftz.f32 	%f37, %f33, %f39, %f35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f37;}

	// end inline asm

$L__BB47_10:
	cvta.to.global.u64 	%rd10, %rd4;
	shl.b64 	%rd11, %rd1, 1;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u16 	%rs69, [%rd12];
	// begin inline asm
	{sub.f16 %rs68,%rs69,%rs73;
}
	// end inline asm
	st.global.u16 	[%rd12], %rs68;

$L__BB47_11:
	ret;

}
	// .globl	addBackCopy
.visible .entry addBackCopy(
	.param .u64 addBackCopy_param_0,
	.param .u32 addBackCopy_param_1,
	.param .u32 addBackCopy_param_2,
	.param .u32 addBackCopy_param_3,
	.param .u32 addBackCopy_param_4,
	.param .u64 addBackCopy_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addBackCopy_param_0];
	ld.param.u32 	%r4, [addBackCopy_param_1];
	ld.param.u32 	%r7, [addBackCopy_param_2];
	ld.param.u32 	%r5, [addBackCopy_param_3];
	ld.param.u32 	%r6, [addBackCopy_param_4];
	ld.param.u64 	%rd2, [addBackCopy_param_5];
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	mov.u32 	%r14, %nctaid.y;
	mul.lo.s32 	%r15, %r14, %r11;
	mad.lo.s32 	%r3, %r15, %r1, %r2;
	mul.lo.s32 	%r16, %r5, %r7;
	setp.ge.s32 	%p1, %r3, %r16;
	@%p1 bra 	$L__BB48_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r17, %r1, %r4, %r2;
	mad.lo.s32 	%r18, %r6, %r5, %r17;
	mul.wide.s32 	%rd4, %r18, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r3, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB48_2:
	ret;

}
	// .globl	dropoutBack
.visible .entry dropoutBack(
	.param .u64 dropoutBack_param_0,
	.param .u64 dropoutBack_param_1,
	.param .align 2 .b8 dropoutBack_param_2[2],
	.param .u64 dropoutBack_param_3,
	.param .u32 dropoutBack_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<13>;


	ld.param.u16 	%rs1, [dropoutBack_param_2];
	ld.param.u64 	%rd2, [dropoutBack_param_0];
	ld.param.u64 	%rd3, [dropoutBack_param_1];
	ld.param.u64 	%rd4, [dropoutBack_param_3];
	ld.param.u32 	%r2, [dropoutBack_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB49_3;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs3, [%rd7];
	ld.const.u16 	%rs4, [sh];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.neu.f16  __$temp3, %rs3, %rs4;
  selp.u16 %rs2, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs2, 0;
	@%p2 bra 	$L__BB49_3;

	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs6, [%rd10];
	// begin inline asm
	{mul.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.u16 	[%rd12], %rs5;

$L__BB49_3:
	ret;

}
	// .globl	mask
.visible .entry mask(
	.param .u64 mask_param_0,
	.param .align 2 .b8 mask_param_1[2],
	.param .align 2 .b8 mask_param_2[2],
	.param .u64 mask_param_3,
	.param .u32 mask_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs2, [mask_param_2];
	ld.param.u16 	%rs1, [mask_param_1];
	ld.param.u64 	%rd2, [mask_param_0];
	ld.param.u64 	%rd3, [mask_param_3];
	ld.param.u32 	%r2, [mask_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB50_3;

	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs4, [%rd6];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.eq.f16  __$temp3, %rs4, %rs1;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs3, 0;
	@%p2 bra 	$L__BB50_3;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs2;

$L__BB50_3:
	ret;

}
	// .globl	fillUnderDiagonal
.visible .entry fillUnderDiagonal(
	.param .u32 fillUnderDiagonal_param_0,
	.param .align 2 .b8 fillUnderDiagonal_param_1[2],
	.param .u64 fillUnderDiagonal_param_2,
	.param .u32 fillUnderDiagonal_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<14>;


	ld.param.u16 	%rs2, [fillUnderDiagonal_param_1];
	ld.param.u32 	%r11, [fillUnderDiagonal_param_0];
	ld.param.u64 	%rd8, [fillUnderDiagonal_param_2];
	ld.param.u32 	%r12, [fillUnderDiagonal_param_3];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	setp.lt.s32 	%p2, %r1, 0;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB51_7;

	add.s32 	%r17, %r1, 1;
	and.b32  	%r24, %r17, 3;
	setp.lt.u32 	%p4, %r1, 3;
	mov.u32 	%r23, 0;
	@%p4 bra 	$L__BB51_4;

	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd9, %r19, 2;
	add.s64 	%rd10, %rd1, %rd9;
	add.s64 	%rd12, %rd10, 4;
	sub.s32 	%r21, %r1, %r24;

$L__BB51_3:
	st.global.u16 	[%rd12+-4], %rs2;
	st.global.u16 	[%rd12+-2], %rs2;
	st.global.u16 	[%rd12], %rs2;
	st.global.u16 	[%rd12+2], %rs2;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd12, %rd12, 8;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p5, %r21, -1;
	@%p5 bra 	$L__BB51_3;

$L__BB51_4:
	setp.eq.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB51_7;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd11, %r20, 2;
	add.s64 	%rd13, %rd1, %rd11;

$L__BB51_6:
	.pragma "nounroll";
	st.global.u16 	[%rd13], %rs2;
	add.s64 	%rd13, %rd13, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p7, %r24, 0;
	@%p7 bra 	$L__BB51_6;

$L__BB51_7:
	ret;

}
	// .globl	derGelu
.visible .entry derGelu(
	.param .u64 derGelu_param_0,
	.param .u64 derGelu_param_1,
	.param .u64 derGelu_param_2,
	.param .u32 derGelu_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<52>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [derGelu_param_0];
	ld.param.u64 	%rd3, [derGelu_param_1];
	ld.param.u64 	%rd4, [derGelu_param_2];
	ld.param.u32 	%r2, [derGelu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB52_5;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs1, [%rd7];
	ld.const.u16 	%rs3, [sh+2];
	// begin inline asm
	{mul.f16 %rs2,%rs3,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs5,%rs1,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs8,%rs5,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs12, [sh+4];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs8;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs2,%rs11;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs14;}

	// end inline asm
	abs.ftz.f32 	%f2, %f6;
	setp.ltu.ftz.f32 	%p2, %f2, 0f3F19999A;
	@%p2 bra 	$L__BB52_3;
	bra.uni 	$L__BB52_2;

$L__BB52_3:
	mul.ftz.f32 	%f15, %f6, %f6;
	mov.f32 	%f16, 0fBD563CAE;
	mov.f32 	%f17, 0f3C80F082;
	fma.rn.ftz.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0f3E085941;
	fma.rn.ftz.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f00000000;
	fma.rn.ftz.f32 	%f24, %f22, %f15, %f23;
	fma.rn.ftz.f32 	%f26, %f24, %f6, %f6;
	bra.uni 	$L__BB52_4;

$L__BB52_2:
	mul.ftz.f32 	%f7, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f8, %f7;
	add.ftz.f32 	%f9, %f8, 0f3F800000;
	mov.f32 	%f10, 0f3F800000;
	rcp.approx.ftz.f32 	%f11, %f9;
	mov.f32 	%f12, 0fC0000000;
	fma.rn.ftz.f32 	%f13, %f11, %f12, %f10;
	setp.ge.ftz.f32 	%p3, %f2, 0f41102CB4;
	selp.f32 	%f14, 0f3F800000, %f13, %p3;
	mov.b32 	%r6, %f14;
	mov.b32 	%r7, %f6;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f26, %r9;

$L__BB52_4:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f26;}

	// end inline asm
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs20, [%rd10];
	ld.const.u16 	%rs21, [sh+6];
	// begin inline asm
	{mul.f16 %rs19,%rs20,%rs21;
}
	// end inline asm
	ld.const.u16 	%rs23, [sh+8];
	// begin inline asm
	{add.f16 %rs22,%rs23,%rs18;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs25,%rs18,%rs18;
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs28,%rs23,%rs25;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs31,%rs1,%rs28;
}
	// end inline asm
	ld.const.u16 	%rs35, [sh+22];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs37,%rs34,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs41, [sh+20];
	// begin inline asm
	{add.f16 %rs40,%rs41,%rs37;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs43,%rs31,%rs40;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs46,%rs22,%rs43;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs49,%rs19,%rs46;
}
	// end inline asm
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.u16 	[%rd12], %rs49;

$L__BB52_5:
	ret;

}
	// .globl	matrixMultiplicationKernel
.visible .entry matrixMultiplicationKernel(
	.param .u64 matrixMultiplicationKernel_param_0,
	.param .u64 matrixMultiplicationKernel_param_1,
	.param .u64 matrixMultiplicationKernel_param_2,
	.param .u32 matrixMultiplicationKernel_param_3,
	.param .u32 matrixMultiplicationKernel_param_4,
	.param .u32 matrixMultiplicationKernel_param_5
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<46>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<36>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd18, [matrixMultiplicationKernel_param_0];
	ld.param.u64 	%rd19, [matrixMultiplicationKernel_param_1];
	ld.param.u64 	%rd17, [matrixMultiplicationKernel_param_2];
	ld.param.u32 	%r12, [matrixMultiplicationKernel_param_3];
	ld.param.u32 	%r14, [matrixMultiplicationKernel_param_4];
	ld.param.u32 	%r13, [matrixMultiplicationKernel_param_5];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %ctaid.x;
	mov.u32 	%r20, %tid.x;
	mad.lo.s32 	%r2, %r19, %r18, %r20;
	mov.u32 	%r21, %nctaid.y;
	mul.lo.s32 	%r22, %r21, %r15;
	mad.lo.s32 	%r23, %r22, %r1, %r2;
	mul.lo.s32 	%r24, %r13, %r14;
	setp.ge.s32 	%p1, %r23, %r24;
	@%p1 bra 	$L__BB53_9;

	mov.f32 	%f1, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f1;}

	// end inline asm
	setp.lt.s32 	%p2, %r12, 1;
	@%p2 bra 	$L__BB53_8;

	add.s32 	%r26, %r12, -1;
	and.b32  	%r35, %r12, 3;
	setp.lt.u32 	%p3, %r26, 3;
	mov.u32 	%r34, 0;
	@%p3 bra 	$L__BB53_5;

	sub.s32 	%r33, %r12, %r35;
	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd32, %rd1, %rd20;
	mul.lo.s32 	%r28, %r12, %r1;
	mul.wide.s32 	%rd21, %r28, 2;
	add.s64 	%rd22, %rd2, %rd21;
	add.s64 	%rd31, %rd22, 4;
	mul.wide.s32 	%rd5, %r13, 2;

$L__BB53_4:
	ld.global.nc.u16 	%rs12, [%rd31+-4];
	ld.global.nc.u16 	%rs13, [%rd32];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs13;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs45,%rs11;
}
	// end inline asm
	ld.global.nc.u16 	%rs18, [%rd31+-2];
	add.s64 	%rd23, %rd32, %rd5;
	ld.global.nc.u16 	%rs19, [%rd23];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs14,%rs17;
}
	// end inline asm
	ld.global.nc.u16 	%rs24, [%rd31];
	add.s64 	%rd24, %rd23, %rd5;
	ld.global.nc.u16 	%rs25, [%rd24];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs20,%rs23;
}
	// end inline asm
	ld.global.nc.u16 	%rs30, [%rd31+2];
	add.s64 	%rd25, %rd24, %rd5;
	add.s64 	%rd32, %rd25, %rd5;
	ld.global.nc.u16 	%rs31, [%rd25];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs45,%rs26,%rs29;
}
	// end inline asm
	add.s32 	%r34, %r34, 4;
	add.s64 	%rd31, %rd31, 8;
	add.s32 	%r33, %r33, -4;
	setp.ne.s32 	%p4, %r33, 0;
	@%p4 bra 	$L__BB53_4;

$L__BB53_5:
	setp.eq.s32 	%p5, %r35, 0;
	@%p5 bra 	$L__BB53_8;

	mad.lo.s32 	%r29, %r34, %r13, %r2;
	mul.wide.s32 	%rd26, %r29, 2;
	add.s64 	%rd34, %rd1, %rd26;
	mul.wide.s32 	%rd11, %r13, 2;
	mad.lo.s32 	%r30, %r12, %r1, %r34;
	mul.wide.s32 	%rd27, %r30, 2;
	add.s64 	%rd33, %rd2, %rd27;

$L__BB53_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs36, [%rd33];
	ld.global.nc.u16 	%rs37, [%rd34];
	// begin inline asm
	{mul.f16 %rs35,%rs36,%rs37;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs45,%rs45,%rs35;
}
	// end inline asm
	add.s64 	%rd34, %rd34, %rd11;
	add.s64 	%rd33, %rd33, 2;
	add.s32 	%r35, %r35, -1;
	setp.ne.s32 	%p6, %r35, 0;
	@%p6 bra 	$L__BB53_7;

$L__BB53_8:
	mad.lo.s32 	%r31, %r1, %r13, %r2;
	cvta.to.global.u64 	%rd28, %rd17;
	mul.wide.s32 	%rd29, %r31, 2;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u16 	[%rd30], %rs45;

$L__BB53_9:
	ret;

}
	// .globl	Softmax
.visible .entry Softmax(
	.param .u64 Softmax_param_0,
	.param .u64 Softmax_param_1,
	.param .u32 Softmax_param_2,
	.param .u32 Softmax_param_3
)
{
	.reg .pred 	%p<35>;
	.reg .b16 	%rs<180>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<52>;


	ld.param.u64 	%rd31, [Softmax_param_0];
	ld.param.u64 	%rd32, [Softmax_param_1];
	ld.param.u32 	%r30, [Softmax_param_2];
	ld.param.u32 	%r31, [Softmax_param_3];
	cvta.to.global.u64 	%rd1, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %tid.x;
	mad.lo.s32 	%r1, %r33, %r32, %r34;
	setp.ge.s32 	%p1, %r1, %r31;
	@%p1 bra 	$L__BB54_39;

	ld.const.u16 	%rs1, [sh];
	mul.lo.s32 	%r49, %r1, %r30;
	cvt.s64.s32 	%rd3, %r49;
	mul.wide.s32 	%rd33, %r49, 2;
	add.s64 	%rd34, %rd2, %rd33;
	ld.global.nc.u16 	%rs170, [%rd34];
	setp.lt.s32 	%p2, %r30, 2;
	@%p2 bra 	$L__BB54_8;

	add.s32 	%r35, %r30, -1;
	and.b32  	%r42, %r35, 3;
	add.s32 	%r36, %r30, -2;
	setp.lt.u32 	%p3, %r36, 3;
	mov.u32 	%r41, %r49;
	@%p3 bra 	$L__BB54_5;

	shl.b64 	%rd35, %rd3, 1;
	add.s64 	%rd36, %rd2, %rd35;
	add.s64 	%rd44, %rd36, 4;
	sub.s32 	%r39, %r30, %r42;
	mov.u32 	%r41, %r49;

$L__BB54_4:
	ld.global.nc.u16 	%rs42, [%rd44+-4];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs170, %rs42;
  selp.u16 %rs40, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs40, 0;
	selp.b16 	%rs44, %rs170, %rs42, %p4;
	ld.global.nc.u16 	%rs45, [%rd44+-2];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs44, %rs45;
  selp.u16 %rs43, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p5, %rs43, 0;
	selp.b16 	%rs47, %rs44, %rs45, %p5;
	ld.global.nc.u16 	%rs48, [%rd44];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs47, %rs48;
  selp.u16 %rs46, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p6, %rs46, 0;
	selp.b16 	%rs50, %rs47, %rs48, %p6;
	ld.global.nc.u16 	%rs51, [%rd44+2];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs50, %rs51;
  selp.u16 %rs49, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p7, %rs49, 0;
	selp.b16 	%rs170, %rs50, %rs51, %p7;
	add.s32 	%r41, %r41, 4;
	add.s64 	%rd44, %rd44, 8;
	add.s32 	%r39, %r39, -4;
	setp.ne.s32 	%p8, %r39, 1;
	@%p8 bra 	$L__BB54_4;

$L__BB54_5:
	setp.eq.s32 	%p9, %r42, 0;
	@%p9 bra 	$L__BB54_8;

	mul.wide.s32 	%rd37, %r41, 2;
	add.s64 	%rd45, %rd2, %rd37;

$L__BB54_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs54, [%rd45];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs170, %rs54;
  selp.u16 %rs52, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p10, %rs52, 0;
	selp.b16 	%rs170, %rs170, %rs54, %p10;
	add.s64 	%rd45, %rd45, 2;
	add.s32 	%r42, %r42, -1;
	setp.ne.s32 	%p11, %r42, 0;
	@%p11 bra 	$L__BB54_7;

$L__BB54_8:
	setp.lt.s32 	%p12, %r30, 1;
	@%p12 bra 	$L__BB54_15;

	add.s32 	%r37, %r30, -1;
	and.b32  	%r46, %r30, 3;
	setp.lt.u32 	%p13, %r37, 3;
	mov.u32 	%r45, %r49;
	mov.u16 	%rs172, %rs1;
	@%p13 bra 	$L__BB54_12;

	sub.s32 	%r44, %r30, %r46;
	shl.b64 	%rd38, %rd3, 1;
	add.s64 	%rd39, %rd38, 4;
	add.s64 	%rd47, %rd2, %rd39;
	add.s64 	%rd46, %rd1, %rd39;
	mov.u32 	%r45, %r49;
	mov.u16 	%rs172, %rs1;

$L__BB54_11:
	ld.global.nc.u16 	%rs56, [%rd47+-4];
	// begin inline asm
	{sub.f16 %rs55,%rs56,%rs170;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs55;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs58,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs60,%rs172,%rs58;
}
	// end inline asm
	st.global.u16 	[%rd46+-4], %rs58;
	ld.global.nc.u16 	%rs64, [%rd47+-2];
	// begin inline asm
	{sub.f16 %rs63,%rs64,%rs170;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs63;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs66,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs68,%rs60,%rs66;
}
	// end inline asm
	st.global.u16 	[%rd46+-2], %rs66;
	ld.global.nc.u16 	%rs72, [%rd47];
	// begin inline asm
	{sub.f16 %rs71,%rs72,%rs170;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs71;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs74,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs76,%rs68,%rs74;
}
	// end inline asm
	st.global.u16 	[%rd46], %rs74;
	ld.global.nc.u16 	%rs80, [%rd47+2];
	// begin inline asm
	{sub.f16 %rs79,%rs80,%rs170;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs79;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs82,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs172,%rs76,%rs82;
}
	// end inline asm
	st.global.u16 	[%rd46+2], %rs82;
	add.s32 	%r45, %r45, 4;
	add.s64 	%rd47, %rd47, 8;
	add.s64 	%rd46, %rd46, 8;
	add.s32 	%r44, %r44, -4;
	setp.ne.s32 	%p14, %r44, 0;
	@%p14 bra 	$L__BB54_11;

$L__BB54_12:
	setp.eq.s32 	%p15, %r46, 0;
	@%p15 bra 	$L__BB54_15;

	mul.wide.s32 	%rd40, %r45, 2;
	add.s64 	%rd49, %rd1, %rd40;
	add.s64 	%rd48, %rd2, %rd40;

$L__BB54_14:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs88, [%rd48];
	// begin inline asm
	{sub.f16 %rs87,%rs88,%rs170;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs87;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs90,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs172,%rs172,%rs90;
}
	// end inline asm
	st.global.u16 	[%rd49], %rs90;
	add.s64 	%rd49, %rd49, 2;
	add.s64 	%rd48, %rd48, 2;
	add.s32 	%r46, %r46, -1;
	setp.ne.s32 	%p16, %r46, 0;
	@%p16 bra 	$L__BB54_14;

$L__BB54_15:
	and.b16  	%rs95, %rs1, 32767;
	setp.eq.s16 	%p17, %rs95, 0;
	ld.const.u16 	%rs96, [sh+10];
	selp.b16 	%rs97, %rs1, %rs96, %p17;
	or.b16  	%rs98, %rs97, -32768;
	setp.eq.s16 	%p18, %rs98, -1024;
	ld.const.u16 	%rs15, [sh+24];
	selp.b16 	%rs174, %rs15, %rs97, %p18;
	setp.ne.s16 	%p19, %rs174, -1024;
	@%p19 bra 	$L__BB54_17;

	// begin inline asm
	{neg.f16 %rs174,%rs15;
}
	// end inline asm

$L__BB54_17:
	@%p12 bra 	$L__BB54_39;

	add.s32 	%r38, %r30, -1;
	and.b32  	%r50, %r30, 3;
	setp.lt.u32 	%p21, %r38, 3;
	@%p21 bra 	$L__BB54_33;

	sub.s32 	%r48, %r30, %r50;
	shl.b64 	%rd41, %rd3, 1;
	add.s64 	%rd42, %rd1, %rd41;
	add.s64 	%rd50, %rd42, 4;
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs174;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f23, %f22;
}
	// end inline asm
	neg.ftz.f32 	%f28, %f22;

$L__BB54_20:
	add.s64 	%rd24, %rd50, -4;
	ld.global.u16 	%rs101, [%rd50+-4];
	// begin inline asm
	{  cvt.f32.f16 %f21, %rs101;}

	// end inline asm
	mul.ftz.f32 	%f25, %f21, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs175, %f25;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs104,%rs175;
}
	// end inline asm
	mov.u16 	%rs108, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs104, %rs108;
  selp.u16 %rs106, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p22, %rs106, 0;
	@%p22 bra 	$L__BB54_23;

	mov.f32 	%f26, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs109, %f26;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs109, %rs104;
  selp.u16 %rs110, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p23, %rs110, 0;
	@%p23 bra 	$L__BB54_23;

	fma.rn.ftz.f32 	%f29, %f28, %f25, %f21;
	fma.rn.ftz.f32 	%f27, %f23, %f29, %f25;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs175, %f27;}

	// end inline asm

$L__BB54_23:
	st.global.u16 	[%rd24], %rs175;
	ld.global.u16 	%rs114, [%rd24+2];
	// begin inline asm
	{  cvt.f32.f16 %f30, %rs114;}

	// end inline asm
	mul.ftz.f32 	%f34, %f30, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs176, %f34;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs117,%rs176;
}
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs117, %rs108;
  selp.u16 %rs119, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p24, %rs119, 0;
	@%p24 bra 	$L__BB54_26;

	mov.f32 	%f35, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs122, %f35;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs122, %rs117;
  selp.u16 %rs123, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p25, %rs123, 0;
	@%p25 bra 	$L__BB54_26;

	fma.rn.ftz.f32 	%f38, %f28, %f34, %f30;
	fma.rn.ftz.f32 	%f36, %f23, %f38, %f34;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs176, %f36;}

	// end inline asm

$L__BB54_26:
	st.global.u16 	[%rd24+2], %rs176;
	ld.global.u16 	%rs127, [%rd24+4];
	// begin inline asm
	{  cvt.f32.f16 %f39, %rs127;}

	// end inline asm
	mul.ftz.f32 	%f43, %f39, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f43;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs130,%rs177;
}
	// end inline asm
	mov.u16 	%rs134, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs130, %rs134;
  selp.u16 %rs132, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p26, %rs132, 0;
	@%p26 bra 	$L__BB54_29;

	mov.f32 	%f44, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs135, %f44;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs135, %rs130;
  selp.u16 %rs136, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p27, %rs136, 0;
	@%p27 bra 	$L__BB54_29;

	fma.rn.ftz.f32 	%f47, %f28, %f43, %f39;
	fma.rn.ftz.f32 	%f45, %f23, %f47, %f43;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f45;}

	// end inline asm

$L__BB54_29:
	st.global.u16 	[%rd24+4], %rs177;
	ld.global.u16 	%rs140, [%rd24+6];
	// begin inline asm
	{  cvt.f32.f16 %f48, %rs140;}

	// end inline asm
	mul.ftz.f32 	%f52, %f48, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f52;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs143,%rs178;
}
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs143, %rs134;
  selp.u16 %rs145, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p28, %rs145, 0;
	@%p28 bra 	$L__BB54_32;

	mov.f32 	%f53, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs148, %f53;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs148, %rs143;
  selp.u16 %rs149, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p29, %rs149, 0;
	@%p29 bra 	$L__BB54_32;

	fma.rn.ftz.f32 	%f56, %f28, %f52, %f48;
	fma.rn.ftz.f32 	%f54, %f23, %f56, %f52;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f54;}

	// end inline asm

$L__BB54_32:
	st.global.u16 	[%rd24+6], %rs178;
	add.s32 	%r49, %r49, 4;
	add.s64 	%rd50, %rd50, 8;
	add.s32 	%r48, %r48, -4;
	setp.ne.s32 	%p30, %r48, 0;
	@%p30 bra 	$L__BB54_20;

$L__BB54_33:
	setp.eq.s32 	%p31, %r50, 0;
	@%p31 bra 	$L__BB54_39;

	mul.wide.s32 	%rd43, %r49, 2;
	add.s64 	%rd51, %rd1, %rd43;
	// begin inline asm
	{  cvt.f32.f16 %f58, %rs174;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f59, %f58;
}
	// end inline asm
	neg.ftz.f32 	%f64, %f58;

$L__BB54_35:
	.pragma "nounroll";
	ld.global.u16 	%rs153, [%rd51];
	// begin inline asm
	{  cvt.f32.f16 %f57, %rs153;}

	// end inline asm
	mul.ftz.f32 	%f61, %f57, %f59;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs179, %f61;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs156,%rs179;
}
	// end inline asm
	mov.u16 	%rs160, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs156, %rs160;
  selp.u16 %rs158, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p32, %rs158, 0;
	@%p32 bra 	$L__BB54_38;

	mov.f32 	%f62, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs161, %f62;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs161, %rs156;
  selp.u16 %rs162, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p33, %rs162, 0;
	@%p33 bra 	$L__BB54_38;

	fma.rn.ftz.f32 	%f65, %f64, %f61, %f57;
	fma.rn.ftz.f32 	%f63, %f59, %f65, %f61;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs179, %f63;}

	// end inline asm

$L__BB54_38:
	st.global.u16 	[%rd51], %rs179;
	add.s64 	%rd51, %rd51, 2;
	add.s32 	%r50, %r50, -1;
	setp.ne.s32 	%p34, %r50, 0;
	@%p34 bra 	$L__BB54_35;

$L__BB54_39:
	ret;

}
	// .globl	derSoftmax
.visible .entry derSoftmax(
	.param .u64 derSoftmax_param_0,
	.param .u64 derSoftmax_param_1,
	.param .u64 derSoftmax_param_2,
	.param .u32 derSoftmax_param_3,
	.param .u32 derSoftmax_param_4
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<213>;
	.reg .b32 	%r<94>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd35, [derSoftmax_param_0];
	ld.param.u64 	%rd36, [derSoftmax_param_1];
	ld.param.u64 	%rd34, [derSoftmax_param_2];
	ld.param.u32 	%r42, [derSoftmax_param_3];
	ld.param.u32 	%r41, [derSoftmax_param_4];
	cvta.to.global.u64 	%rd1, %rd36;
	cvta.to.global.u64 	%rd2, %rd35;
	mov.u32 	%r43, %ctaid.x;
	mov.u32 	%r44, %ntid.x;
	mov.u32 	%r45, %tid.x;
	mad.lo.s32 	%r1, %r44, %r43, %r45;
	mov.u32 	%r46, %ctaid.y;
	mov.u32 	%r47, %ntid.y;
	mul.lo.s32 	%r2, %r47, %r46;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r42;
	setp.ge.s32 	%p2, %r4, %r41;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB55_23;

	cvta.to.global.u64 	%rd37, %rd34;
	ld.const.u16 	%rs207, [sh];
	mul.lo.s32 	%r5, %r1, %r41;
	add.s32 	%r49, %r5, %r4;
	mul.wide.s32 	%rd38, %r49, 2;
	add.s64 	%rd3, %rd37, %rd38;
	st.global.u16 	[%rd3], %rs207;
	add.s64 	%rd39, %rd2, %rd38;
	ld.global.nc.u16 	%rs2, [%rd39];
	mov.u32 	%r89, 0;
	max.s32 	%r6, %r4, 0;
	min.s32 	%r7, %r6, %r41;
	setp.lt.s32 	%p4, %r7, 1;
	@%p4 bra 	$L__BB55_8;

	add.s32 	%r52, %r7, -1;
	and.b32  	%r82, %r7, 3;
	setp.lt.u32 	%p5, %r52, 3;
	mov.u32 	%r89, 0;
	@%p5 bra 	$L__BB55_5;

	not.b32 	%r54, %r6;
	not.b32 	%r55, %r41;
	max.s32 	%r56, %r54, %r55;
	add.s32 	%r57, %r56, %r82;
	neg.s32 	%r77, %r57;
	mul.wide.s32 	%rd40, %r5, 2;
	add.s64 	%rd41, %rd40, 4;
	add.s64 	%rd50, %rd2, %rd41;
	add.s64 	%rd49, %rd1, %rd41;

$L__BB55_4:
	ld.global.nc.u16 	%rs27, [%rd50+-4];
	// begin inline asm
	{neg.f16 %rs26,%rs27;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs28,%rs2,%rs26;
}
	// end inline asm
	ld.global.nc.u16 	%rs32, [%rd49+-4];
	// begin inline asm
	{mul.f16 %rs31,%rs32,%rs28;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs34,%rs207,%rs31;
}
	// end inline asm
	ld.global.nc.u16 	%rs38, [%rd50+-2];
	// begin inline asm
	{neg.f16 %rs37,%rs38;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs39,%rs2,%rs37;
}
	// end inline asm
	ld.global.nc.u16 	%rs43, [%rd49+-2];
	// begin inline asm
	{mul.f16 %rs42,%rs43,%rs39;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs45,%rs34,%rs42;
}
	// end inline asm
	ld.global.nc.u16 	%rs49, [%rd50];
	// begin inline asm
	{neg.f16 %rs48,%rs49;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs50,%rs2,%rs48;
}
	// end inline asm
	ld.global.nc.u16 	%rs54, [%rd49];
	// begin inline asm
	{mul.f16 %rs53,%rs54,%rs50;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs56,%rs45,%rs53;
}
	// end inline asm
	ld.global.nc.u16 	%rs60, [%rd50+2];
	// begin inline asm
	{neg.f16 %rs59,%rs60;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs61,%rs2,%rs59;
}
	// end inline asm
	ld.global.nc.u16 	%rs65, [%rd49+2];
	// begin inline asm
	{mul.f16 %rs64,%rs65,%rs61;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs207,%rs56,%rs64;
}
	// end inline asm
	add.s32 	%r89, %r89, 4;
	add.s64 	%rd50, %rd50, 8;
	add.s64 	%rd49, %rd49, 8;
	add.s32 	%r77, %r77, -4;
	setp.ne.s32 	%p6, %r77, 1;
	@%p6 bra 	$L__BB55_4;

$L__BB55_5:
	setp.eq.s32 	%p7, %r82, 0;
	@%p7 bra 	$L__BB55_8;

	add.s32 	%r58, %r89, %r5;
	mul.wide.s32 	%rd42, %r58, 2;
	add.s64 	%rd52, %rd1, %rd42;
	add.s64 	%rd51, %rd2, %rd42;

$L__BB55_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs71, [%rd51];
	// begin inline asm
	{neg.f16 %rs70,%rs71;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs72,%rs2,%rs70;
}
	// end inline asm
	ld.global.nc.u16 	%rs76, [%rd52];
	// begin inline asm
	{mul.f16 %rs75,%rs76,%rs72;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs207,%rs207,%rs75;
}
	// end inline asm
	add.s32 	%r89, %r89, 1;
	add.s64 	%rd52, %rd52, 2;
	add.s64 	%rd51, %rd51, 2;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p8, %r82, 0;
	@%p8 bra 	$L__BB55_7;

$L__BB55_8:
	add.s32 	%r59, %r4, 1;
	min.s32 	%r21, %r59, %r41;
	setp.ge.s32 	%p9, %r89, %r21;
	@%p9 bra 	$L__BB55_15;

	ld.const.u16 	%rs10, [sh+8];
	mov.u32 	%r61, -2;
	sub.s32 	%r62, %r61, %r2;
	sub.s32 	%r63, %r62, %r3;
	not.b32 	%r64, %r41;
	max.s32 	%r22, %r63, %r64;
	not.b32 	%r65, %r89;
	sub.s32 	%r66, %r65, %r22;
	and.b32  	%r85, %r66, 3;
	setp.eq.s32 	%p10, %r85, 0;
	mov.u32 	%r86, %r89;
	@%p10 bra 	$L__BB55_12;

	add.s32 	%r67, %r89, %r5;
	mul.wide.s32 	%rd43, %r67, 2;
	add.s64 	%rd53, %rd1, %rd43;
	// begin inline asm
	{sub.f16 %rs82,%rs10,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs85,%rs2,%rs82;
}
	// end inline asm
	mov.u32 	%r86, %r89;

$L__BB55_11:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs89, [%rd53];
	// begin inline asm
	{mul.f16 %rs88,%rs89,%rs85;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs207,%rs207,%rs88;
}
	// end inline asm
	add.s32 	%r86, %r86, 1;
	add.s64 	%rd53, %rd53, 2;
	add.s32 	%r85, %r85, -1;
	setp.ne.s32 	%p11, %r85, 0;
	@%p11 bra 	$L__BB55_11;

$L__BB55_12:
	sub.s32 	%r69, %r61, %r89;
	sub.s32 	%r70, %r69, %r22;
	setp.lt.u32 	%p12, %r70, 3;
	mov.u32 	%r89, %r86;
	@%p12 bra 	$L__BB55_15;

	add.s32 	%r71, %r86, %r5;
	mul.wide.s32 	%rd44, %r71, 2;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd54, %rd45, 4;
	// begin inline asm
	{sub.f16 %rs94,%rs10,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs97,%rs2,%rs94;
}
	// end inline asm
	mov.u32 	%r89, %r86;

$L__BB55_14:
	ld.global.nc.u16 	%rs101, [%rd54+-4];
	// begin inline asm
	{mul.f16 %rs100,%rs101,%rs97;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs103,%rs207,%rs100;
}
	// end inline asm
	ld.global.nc.u16 	%rs113, [%rd54+-2];
	// begin inline asm
	{mul.f16 %rs112,%rs113,%rs97;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs115,%rs103,%rs112;
}
	// end inline asm
	ld.global.nc.u16 	%rs125, [%rd54];
	// begin inline asm
	{mul.f16 %rs124,%rs125,%rs97;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs127,%rs115,%rs124;
}
	// end inline asm
	ld.global.nc.u16 	%rs137, [%rd54+2];
	// begin inline asm
	{mul.f16 %rs136,%rs137,%rs97;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs207,%rs127,%rs136;
}
	// end inline asm
	add.s64 	%rd54, %rd54, 8;
	add.s32 	%r89, %r89, 4;
	setp.lt.s32 	%p13, %r89, %r21;
	@%p13 bra 	$L__BB55_14;

$L__BB55_15:
	setp.ge.s32 	%p14, %r89, %r41;
	@%p14 bra 	$L__BB55_22;

	sub.s32 	%r72, %r41, %r89;
	and.b32  	%r91, %r72, 3;
	setp.eq.s32 	%p15, %r91, 0;
	mov.u32 	%r92, %r89;
	@%p15 bra 	$L__BB55_19;

	add.s32 	%r73, %r89, %r5;
	mul.wide.s32 	%rd46, %r73, 2;
	add.s64 	%rd56, %rd1, %rd46;
	add.s64 	%rd55, %rd2, %rd46;
	mov.u32 	%r92, %r89;

$L__BB55_18:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs144, [%rd55];
	// begin inline asm
	{neg.f16 %rs143,%rs144;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs145,%rs2,%rs143;
}
	// end inline asm
	ld.global.nc.u16 	%rs149, [%rd56];
	// begin inline asm
	{mul.f16 %rs148,%rs149,%rs145;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs207,%rs207,%rs148;
}
	// end inline asm
	add.s32 	%r92, %r92, 1;
	add.s64 	%rd56, %rd56, 2;
	add.s64 	%rd55, %rd55, 2;
	add.s32 	%r91, %r91, -1;
	setp.ne.s32 	%p16, %r91, 0;
	@%p16 bra 	$L__BB55_18;

$L__BB55_19:
	not.b32 	%r74, %r89;
	add.s32 	%r75, %r74, %r41;
	setp.lt.u32 	%p17, %r75, 3;
	@%p17 bra 	$L__BB55_22;

	add.s32 	%r76, %r92, %r5;
	mul.wide.s32 	%rd47, %r76, 2;
	add.s64 	%rd48, %rd47, 4;
	add.s64 	%rd58, %rd1, %rd48;
	add.s64 	%rd57, %rd2, %rd48;

$L__BB55_21:
	ld.global.nc.u16 	%rs155, [%rd57+-4];
	// begin inline asm
	{neg.f16 %rs154,%rs155;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs156,%rs2,%rs154;
}
	// end inline asm
	ld.global.nc.u16 	%rs160, [%rd58+-4];
	// begin inline asm
	{mul.f16 %rs159,%rs160,%rs156;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs162,%rs207,%rs159;
}
	// end inline asm
	ld.global.nc.u16 	%rs166, [%rd57+-2];
	// begin inline asm
	{neg.f16 %rs165,%rs166;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs167,%rs2,%rs165;
}
	// end inline asm
	ld.global.nc.u16 	%rs171, [%rd58+-2];
	// begin inline asm
	{mul.f16 %rs170,%rs171,%rs167;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs173,%rs162,%rs170;
}
	// end inline asm
	ld.global.nc.u16 	%rs177, [%rd57];
	// begin inline asm
	{neg.f16 %rs176,%rs177;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs178,%rs2,%rs176;
}
	// end inline asm
	ld.global.nc.u16 	%rs182, [%rd58];
	// begin inline asm
	{mul.f16 %rs181,%rs182,%rs178;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs184,%rs173,%rs181;
}
	// end inline asm
	ld.global.nc.u16 	%rs188, [%rd57+2];
	// begin inline asm
	{neg.f16 %rs187,%rs188;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs189,%rs2,%rs187;
}
	// end inline asm
	ld.global.nc.u16 	%rs193, [%rd58+2];
	// begin inline asm
	{mul.f16 %rs192,%rs193,%rs189;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs207,%rs184,%rs192;
}
	// end inline asm
	add.s64 	%rd58, %rd58, 8;
	add.s64 	%rd57, %rd57, 8;
	add.s32 	%r92, %r92, 4;
	setp.lt.s32 	%p18, %r92, %r41;
	@%p18 bra 	$L__BB55_21;

$L__BB55_22:
	st.global.u16 	[%rd3], %rs207;

$L__BB55_23:
	ret;

}

