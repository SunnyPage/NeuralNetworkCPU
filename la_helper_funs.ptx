//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32415258
// Cuda compilation tools, release 12.1, V12.1.66
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	fill
.global .align 4 .u32 SharedMemorySize = 32768;
.global .align 4 .u32 BLOCK_DIM = 32;
.const .align 2 .b8 sh[28];
// _ZZ12MatMulKernelE8blockElt has been demoted
// _ZZ12MatMulKernelE9blockxInd has been demoted
// _ZZ12MatMulKernelE9blockyInd has been demoted
// _ZZ12MatMulKernelE1b has been demoted
// _ZZ13MatMulKernelTE8blockElt has been demoted
// _ZZ13MatMulKernelTE9blockxInd has been demoted
// _ZZ13MatMulKernelTE9blockyInd has been demoted
// _ZZ13MatMulKernelTE1b has been demoted
// _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax has been demoted
// _ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx has been demoted
// _ZZ19sharedMem_transposeE8M_Shared has been demoted
// _ZZ24sharedMem_transpose_halfE8M_Shared has been demoted
// _ZZ33matrixTransposeSolveBankConflictsE3mat has been demoted
// _ZZ11transposeV3E3s_A has been demoted
// _ZZ7SoftmaxE3max has been demoted
// _ZZ7SoftmaxE3sum has been demoted
// _ZZ12Softmax_halfE3max_$_0 has been demoted
// _ZZ12Softmax_halfE3sum_$_0 has been demoted

.visible .entry fill(
	.param .u64 fill_param_0,
	.param .align 2 .b8 fill_param_1[2],
	.param .u32 fill_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs1, [fill_param_1];
	ld.param.u64 	%rd1, [fill_param_0];
	ld.param.u32 	%r2, [fill_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u16 	[%rd4], %rs1;

$L__BB0_2:
	ret;

}
	// .globl	fill_float
.visible .entry fill_float(
	.param .u64 fill_float_param_0,
	.param .f32 fill_float_param_1,
	.param .u32 fill_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_float_param_0];
	ld.param.f32 	%f1, [fill_float_param_1];
	ld.param.u32 	%r2, [fill_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f32 	[%rd4], %f1;

$L__BB1_2:
	ret;

}
	// .globl	float2HalfVector
.visible .entry float2HalfVector(
	.param .u64 float2HalfVector_param_0,
	.param .u64 float2HalfVector_param_1,
	.param .u32 float2HalfVector_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [float2HalfVector_param_0];
	ld.param.u64 	%rd3, [float2HalfVector_param_1];
	ld.param.u32 	%r2, [float2HalfVector_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_6;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f1;}

	// end inline asm
	setp.eq.s16 	%p2, %rs9, 31744;
	@%p2 bra 	$L__BB2_4;
	bra.uni 	$L__BB2_2;

$L__BB2_4:
	ld.const.u16 	%rs9, [sh+24];
	bra.uni 	$L__BB2_5;

$L__BB2_2:
	setp.ne.s16 	%p3, %rs9, -1024;
	@%p3 bra 	$L__BB2_5;

	ld.const.u16 	%rs8, [sh+24];
	// begin inline asm
	{neg.f16 %rs9,%rs8;
}
	// end inline asm

$L__BB2_5:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs9;

$L__BB2_6:
	ret;

}
	// .globl	half2FloatVector
.visible .entry half2FloatVector(
	.param .u64 half2FloatVector_param_0,
	.param .u64 half2FloatVector_param_1,
	.param .u32 half2FloatVector_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [half2FloatVector_param_0];
	ld.param.u64 	%rd2, [half2FloatVector_param_1];
	ld.param.u32 	%r2, [half2FloatVector_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB3_2:
	ret;

}
	// .globl	gelu
.visible .entry gelu(
	.param .u64 gelu_param_0,
	.param .u64 gelu_param_1,
	.param .u32 gelu_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [gelu_param_0];
	ld.param.u64 	%rd3, [gelu_param_1];
	ld.param.u32 	%r2, [gelu_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.ftz.f32 	%f7, %f1, %f1;
	mul.ftz.f32 	%f8, %f1, %f7;
	mul.ftz.f32 	%f9, %f8, 0f3D122277;
	fma.rn.ftz.f32 	%f2, %f1, 0f3F4C422A, %f9;
	abs.ftz.f32 	%f3, %f2;
	setp.ltu.ftz.f32 	%p2, %f3, 0f3F19999A;
	@%p2 bra 	$L__BB4_3;
	bra.uni 	$L__BB4_2;

$L__BB4_3:
	mul.ftz.f32 	%f18, %f2, %f2;
	mov.f32 	%f19, 0fBD563CAE;
	mov.f32 	%f20, 0f3C80F082;
	fma.rn.ftz.f32 	%f21, %f20, %f18, %f19;
	mov.f32 	%f22, 0f3E085941;
	fma.rn.ftz.f32 	%f23, %f21, %f18, %f22;
	mov.f32 	%f24, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f25, %f23, %f18, %f24;
	mov.f32 	%f26, 0f00000000;
	fma.rn.ftz.f32 	%f27, %f25, %f18, %f26;
	fma.rn.ftz.f32 	%f31, %f27, %f2, %f2;
	bra.uni 	$L__BB4_4;

$L__BB4_2:
	mul.ftz.f32 	%f10, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f11, %f10;
	add.ftz.f32 	%f12, %f11, 0f3F800000;
	mov.f32 	%f13, 0f3F800000;
	rcp.approx.ftz.f32 	%f14, %f12;
	mov.f32 	%f15, 0fC0000000;
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	setp.ge.ftz.f32 	%p3, %f3, 0f41102CB4;
	selp.f32 	%f17, 0f3F800000, %f16, %p3;
	mov.b32 	%r6, %f17;
	mov.b32 	%r7, %f2;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f31, %r9;

$L__BB4_4:
	add.ftz.f32 	%f28, %f31, 0f3F800000;
	mul.ftz.f32 	%f29, %f1, 0f3F000000;
	mul.ftz.f32 	%f30, %f29, %f28;
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f30;

$L__BB4_5:
	ret;

}
	// .globl	gelu_half
.visible .entry gelu_half(
	.param .u64 gelu_half_param_0,
	.param .u64 gelu_half_param_1,
	.param .u32 gelu_half_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<27>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [gelu_half_param_0];
	ld.param.u64 	%rd2, [gelu_half_param_1];
	ld.param.u32 	%r2, [gelu_half_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB5_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs3, [%rd5];
	ld.const.u16 	%rs2, [sh+2];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs4,%rs3,%rs3;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs7,%rs4,%rs3;
}
	// end inline asm
	ld.const.u16 	%rs11, [sh+4];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs7;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs1,%rs10;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs13;}

	// end inline asm
	sin.approx.ftz.f32 	%f3, %f1;
	cos.approx.ftz.f32 	%f4, %f1;
	div.approx.ftz.f32 	%f2, %f3, %f4;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs17, %f2;}

	// end inline asm
	ld.const.u16 	%rs19, [sh+6];
	// begin inline asm
	{mul.f16 %rs18,%rs19,%rs3;
}
	// end inline asm
	ld.const.u16 	%rs22, [sh+8];
	// begin inline asm
	{add.f16 %rs21,%rs22,%rs17;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs24,%rs18,%rs21;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs24;

$L__BB5_2:
	ret;

}
	// .globl	MatAdd
.visible .entry MatAdd(
	.param .u64 MatAdd_param_0,
	.param .u64 MatAdd_param_1,
	.param .u32 MatAdd_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [MatAdd_param_0];
	ld.param.u64 	%rd2, [MatAdd_param_1];
	ld.param.u32 	%r2, [MatAdd_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f1, [%rd7];
	ld.global.f32 	%f2, [%rd5];
	add.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

$L__BB6_2:
	ret;

}
	// .globl	MatAdd_half
.visible .entry MatAdd_half(
	.param .u64 MatAdd_half_param_0,
	.param .u64 MatAdd_half_param_1,
	.param .u32 MatAdd_half_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd2, [MatAdd_half_param_0];
	ld.param.u64 	%rd3, [MatAdd_half_param_1];
	ld.param.u32 	%r2, [MatAdd_half_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB7_6;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.u16 	%rs7, [%rd1];
	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs8, [%rd7];
	// begin inline asm
	{add.f16 %rs11,%rs7,%rs8;
}
	// end inline asm
	st.global.u16 	[%rd1], %rs11;
	setp.eq.s16 	%p2, %rs11, 31744;
	@%p2 bra 	$L__BB7_4;
	bra.uni 	$L__BB7_2;

$L__BB7_4:
	ld.const.u16 	%rs11, [sh+24];
	bra.uni 	$L__BB7_5;

$L__BB7_2:
	setp.ne.s16 	%p3, %rs11, -1024;
	@%p3 bra 	$L__BB7_5;

	ld.const.u16 	%rs10, [sh+24];
	// begin inline asm
	{neg.f16 %rs11,%rs10;
}
	// end inline asm

$L__BB7_5:
	st.global.u16 	[%rd1], %rs11;

$L__BB7_6:
	ret;

}
	// .globl	imageVector
.visible .entry imageVector(
	.param .u64 imageVector_param_0,
	.param .u64 imageVector_param_1,
	.param .u32 imageVector_param_2,
	.param .u32 imageVector_param_3,
	.param .u32 imageVector_param_4,
	.param .u32 imageVector_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [imageVector_param_0];
	ld.param.u64 	%rd14, [imageVector_param_1];
	ld.param.u32 	%r29, [imageVector_param_2];
	ld.param.u32 	%r26, [imageVector_param_3];
	ld.param.u32 	%r27, [imageVector_param_4];
	ld.param.u32 	%r28, [imageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r33, %r31, %r30, %r32;
	mul.lo.s32 	%r1, %r33, %r28;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r36, %tid.y;
	mad.lo.s32 	%r37, %r35, %r34, %r36;
	mul.lo.s32 	%r2, %r37, %r28;
	mov.u32 	%r38, %ctaid.z;
	mov.u32 	%r39, %ntid.z;
	mov.u32 	%r40, %tid.z;
	mad.lo.s32 	%r3, %r39, %r38, %r40;
	setp.ge.s32 	%p1, %r1, %r29;
	setp.ge.s32 	%p2, %r2, %r26;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r28;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB8_12;

	setp.lt.s32 	%p6, %r28, 1;
	@%p6 bra 	$L__BB8_12;

	mul.lo.s32 	%r41, %r28, %r27;
	mul.lo.s32 	%r42, %r41, %r28;
	add.s32 	%r43, %r1, %r3;
	mad.lo.s32 	%r4, %r43, %r26, %r2;
	setp.lt.s32 	%p7, %r27, 1;
	div.s32 	%r44, %r1, %r28;
	mul.lo.s32 	%r45, %r42, %r26;
	div.s32 	%r46, %r45, %r28;
	div.s32 	%r47, %r2, %r28;
	mul.lo.s32 	%r48, %r41, %r3;
	mad.lo.s32 	%r49, %r47, %r42, %r48;
	mad.lo.s32 	%r61, %r44, %r46, %r49;
	@%p7 bra 	$L__BB8_12;

	add.s32 	%r6, %r27, -1;
	and.b32  	%r7, %r27, 3;
	sub.s32 	%r8, %r27, %r7;
	add.s64 	%rd3, %rd1, 4;
	add.s64 	%rd4, %rd2, 4;
	mov.u32 	%r53, 0;

$L__BB8_4:
	add.s32 	%r52, %r4, %r53;
	mul.lo.s32 	%r59, %r52, %r27;
	setp.lt.u32 	%p8, %r6, 3;
	@%p8 bra 	$L__BB8_7;

	mul.wide.s32 	%rd15, %r61, 2;
	add.s64 	%rd20, %rd3, %rd15;
	mul.wide.s32 	%rd16, %r59, 2;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r57, %r8;

$L__BB8_6:
	ld.global.nc.u16 	%rs1, [%rd19+-4];
	st.global.u16 	[%rd20+-4], %rs1;
	ld.global.nc.u16 	%rs2, [%rd19+-2];
	st.global.u16 	[%rd20+-2], %rs2;
	ld.global.nc.u16 	%rs3, [%rd19];
	st.global.u16 	[%rd20], %rs3;
	ld.global.nc.u16 	%rs4, [%rd19+2];
	st.global.u16 	[%rd20+2], %rs4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd20, %rd20, 8;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r57, %r57, -4;
	setp.ne.s32 	%p9, %r57, 0;
	@%p9 bra 	$L__BB8_6;

$L__BB8_7:
	mov.u32 	%r20, %r61;
	setp.eq.s32 	%p10, %r7, 0;
	@%p10 bra 	$L__BB8_11;

	setp.eq.s32 	%p11, %r7, 1;
	mul.wide.s32 	%rd17, %r59, 2;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.u16 	%rs5, [%rd11];
	mul.wide.s32 	%rd18, %r20, 2;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.u16 	[%rd12], %rs5;
	add.s32 	%r61, %r20, 1;
	@%p11 bra 	$L__BB8_11;

	setp.eq.s32 	%p12, %r7, 2;
	ld.global.nc.u16 	%rs6, [%rd11+2];
	st.global.u16 	[%rd12+2], %rs6;
	add.s32 	%r61, %r20, 2;
	@%p12 bra 	$L__BB8_11;

	ld.global.nc.u16 	%rs7, [%rd11+4];
	st.global.u16 	[%rd12+4], %rs7;
	add.s32 	%r61, %r20, 3;

$L__BB8_11:
	add.s32 	%r53, %r53, 1;
	setp.lt.s32 	%p13, %r53, %r28;
	@%p13 bra 	$L__BB8_4;

$L__BB8_12:
	ret;

}
	// .globl	backImageVector
.visible .entry backImageVector(
	.param .u64 backImageVector_param_0,
	.param .u64 backImageVector_param_1,
	.param .u32 backImageVector_param_2,
	.param .u32 backImageVector_param_3,
	.param .u32 backImageVector_param_4,
	.param .u32 backImageVector_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [backImageVector_param_0];
	ld.param.u64 	%rd14, [backImageVector_param_1];
	ld.param.u32 	%r29, [backImageVector_param_2];
	ld.param.u32 	%r26, [backImageVector_param_3];
	ld.param.u32 	%r27, [backImageVector_param_4];
	ld.param.u32 	%r28, [backImageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r33, %r31, %r30, %r32;
	mul.lo.s32 	%r1, %r33, %r28;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r36, %tid.y;
	mad.lo.s32 	%r37, %r35, %r34, %r36;
	mul.lo.s32 	%r2, %r37, %r28;
	mov.u32 	%r38, %ctaid.z;
	mov.u32 	%r39, %ntid.z;
	mov.u32 	%r40, %tid.z;
	mad.lo.s32 	%r3, %r39, %r38, %r40;
	setp.ge.s32 	%p1, %r1, %r29;
	setp.ge.s32 	%p2, %r2, %r26;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r28;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB9_12;

	setp.lt.s32 	%p6, %r28, 1;
	@%p6 bra 	$L__BB9_12;

	mul.lo.s32 	%r41, %r28, %r27;
	mul.lo.s32 	%r42, %r41, %r28;
	add.s32 	%r43, %r1, %r3;
	mad.lo.s32 	%r4, %r43, %r26, %r2;
	setp.lt.s32 	%p7, %r27, 1;
	div.s32 	%r44, %r1, %r28;
	mul.lo.s32 	%r45, %r42, %r26;
	div.s32 	%r46, %r45, %r28;
	div.s32 	%r47, %r2, %r28;
	mul.lo.s32 	%r48, %r41, %r3;
	mad.lo.s32 	%r49, %r47, %r42, %r48;
	mad.lo.s32 	%r61, %r44, %r46, %r49;
	@%p7 bra 	$L__BB9_12;

	add.s32 	%r6, %r27, -1;
	and.b32  	%r7, %r27, 3;
	sub.s32 	%r8, %r27, %r7;
	add.s64 	%rd3, %rd2, 4;
	add.s64 	%rd4, %rd1, 4;
	mov.u32 	%r53, 0;

$L__BB9_4:
	add.s32 	%r52, %r4, %r53;
	mul.lo.s32 	%r59, %r52, %r27;
	setp.lt.u32 	%p8, %r6, 3;
	@%p8 bra 	$L__BB9_7;

	mul.wide.s32 	%rd15, %r61, 2;
	add.s64 	%rd20, %rd3, %rd15;
	mul.wide.s32 	%rd16, %r59, 2;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r57, %r8;

$L__BB9_6:
	ld.global.nc.u16 	%rs1, [%rd20+-4];
	st.global.u16 	[%rd19+-4], %rs1;
	ld.global.nc.u16 	%rs2, [%rd20+-2];
	st.global.u16 	[%rd19+-2], %rs2;
	ld.global.nc.u16 	%rs3, [%rd20];
	st.global.u16 	[%rd19], %rs3;
	ld.global.nc.u16 	%rs4, [%rd20+2];
	st.global.u16 	[%rd19+2], %rs4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd20, %rd20, 8;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r57, %r57, -4;
	setp.ne.s32 	%p9, %r57, 0;
	@%p9 bra 	$L__BB9_6;

$L__BB9_7:
	mov.u32 	%r20, %r61;
	setp.eq.s32 	%p10, %r7, 0;
	@%p10 bra 	$L__BB9_11;

	setp.eq.s32 	%p11, %r7, 1;
	mul.wide.s32 	%rd17, %r20, 2;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.u16 	%rs5, [%rd11];
	mul.wide.s32 	%rd18, %r59, 2;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.u16 	[%rd12], %rs5;
	add.s32 	%r61, %r20, 1;
	@%p11 bra 	$L__BB9_11;

	setp.eq.s32 	%p12, %r7, 2;
	ld.global.nc.u16 	%rs6, [%rd11+2];
	st.global.u16 	[%rd12+2], %rs6;
	add.s32 	%r61, %r20, 2;
	@%p12 bra 	$L__BB9_11;

	ld.global.nc.u16 	%rs7, [%rd11+4];
	st.global.u16 	[%rd12+4], %rs7;
	add.s32 	%r61, %r20, 3;

$L__BB9_11:
	add.s32 	%r53, %r53, 1;
	setp.lt.s32 	%p13, %r53, %r28;
	@%p13 bra 	$L__BB9_4;

$L__BB9_12:
	ret;

}
	// .globl	add3
.visible .entry add3(
	.param .u64 add3_param_0,
	.param .u64 add3_param_1,
	.param .u32 add3_param_2,
	.param .u32 add3_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [add3_param_0];
	ld.param.u64 	%rd2, [add3_param_1];
	ld.param.u32 	%r4, [add3_param_2];
	ld.param.u32 	%r3, [add3_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB10_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd4, %r11, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	add.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

$L__BB10_2:
	ret;

}
	// .globl	add3_half
.visible .entry add3_half(
	.param .u64 add3_half_param_0,
	.param .u64 add3_half_param_1,
	.param .u32 add3_half_param_2,
	.param .u32 add3_half_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [add3_half_param_0];
	ld.param.u64 	%rd2, [add3_half_param_1];
	ld.param.u32 	%r4, [add3_half_param_2];
	ld.param.u32 	%r3, [add3_half_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB11_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd4, %r11, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r2, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u16 	%rs3, [%rd8];
	// begin inline asm
	{add.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs1;

$L__BB11_2:
	ret;

}
	// .globl	dot_VectorAndMatrix
.visible .entry dot_VectorAndMatrix(
	.param .u64 dot_VectorAndMatrix_param_0,
	.param .u64 dot_VectorAndMatrix_param_1,
	.param .u64 dot_VectorAndMatrix_param_2,
	.param .u32 dot_VectorAndMatrix_param_3,
	.param .u32 dot_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd14, [dot_VectorAndMatrix_param_0];
	ld.param.u64 	%rd15, [dot_VectorAndMatrix_param_1];
	ld.param.u64 	%rd16, [dot_VectorAndMatrix_param_2];
	ld.param.u32 	%r12, [dot_VectorAndMatrix_param_3];
	ld.param.u32 	%r11, [dot_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB12_9;

	ld.const.u16 	%rs44, [sh];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB12_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r24, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r23, 0;
	@%p3 bra 	$L__BB12_5;

	sub.s32 	%r22, %r11, %r24;
	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd17, %r19, 2;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd25, %rd18, 4;
	mov.u64 	%rd24, %rd2;

$L__BB12_4:
	ld.global.nc.u16 	%rs11, [%rd24];
	ld.global.nc.u16 	%rs12, [%rd25+-4];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs12;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs44,%rs10;
}
	// end inline asm
	ld.global.nc.u16 	%rs17, [%rd24+2];
	ld.global.nc.u16 	%rs18, [%rd25+-2];
	// begin inline asm
	{mul.f16 %rs16,%rs17,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs19,%rs13,%rs16;
}
	// end inline asm
	ld.global.nc.u16 	%rs23, [%rd24+4];
	ld.global.nc.u16 	%rs24, [%rd25];
	// begin inline asm
	{mul.f16 %rs22,%rs23,%rs24;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd24+6];
	ld.global.nc.u16 	%rs30, [%rd25+2];
	// begin inline asm
	{mul.f16 %rs28,%rs29,%rs30;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs25,%rs28;
}
	// end inline asm
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd25, %rd25, 8;
	add.s64 	%rd24, %rd24, 8;
	add.s32 	%r22, %r22, -4;
	setp.ne.s32 	%p4, %r22, 0;
	@%p4 bra 	$L__BB12_4;

$L__BB12_5:
	setp.eq.s32 	%p5, %r24, 0;
	@%p5 bra 	$L__BB12_8;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd19, %r20, 2;
	add.s64 	%rd27, %rd1, %rd19;
	mul.wide.s32 	%rd20, %r23, 2;
	add.s64 	%rd26, %rd2, %rd20;

$L__BB12_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs35, [%rd26];
	ld.global.nc.u16 	%rs36, [%rd27];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs36;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs44,%rs34;
}
	// end inline asm
	add.s64 	%rd27, %rd27, 2;
	add.s64 	%rd26, %rd26, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB12_7;

$L__BB12_8:
	cvta.to.global.u64 	%rd21, %rd14;
	mul.wide.s32 	%rd22, %r1, 2;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u16 	[%rd23], %rs44;

$L__BB12_9:
	ret;

}
	// .globl	MatMulKernel
.visible .entry MatMulKernel(
	.param .u64 MatMulKernel_param_0,
	.param .u64 MatMulKernel_param_1,
	.param .u64 MatMulKernel_param_2,
	.param .u32 MatMulKernel_param_3,
	.param .u32 MatMulKernel_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<48>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<27>;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE8blockElt;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE9blockxInd;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE9blockyInd;
	// demoted variable
	.shared .align 2 .b8 _ZZ12MatMulKernelE1b[128];

	ld.param.u64 	%rd11, [MatMulKernel_param_0];
	ld.param.u64 	%rd12, [MatMulKernel_param_1];
	ld.param.u64 	%rd13, [MatMulKernel_param_2];
	ld.param.u32 	%r26, [MatMulKernel_param_3];
	ld.param.u32 	%r27, [MatMulKernel_param_4];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB13_4;

	mov.u32 	%r29, %ctaid.x;
	shl.b32 	%r2, %r29, 6;
	add.s32 	%r30, %r2, 64;
	mov.u32 	%r54, 64;
	setp.le.u32 	%p2, %r30, %r27;
	@%p2 bra 	$L__BB13_3;

	shr.s32 	%r31, %r27, 31;
	shr.u32 	%r32, %r31, 26;
	add.s32 	%r33, %r27, %r32;
	and.b32  	%r34, %r33, -64;
	sub.s32 	%r54, %r27, %r34;

$L__BB13_3:
	st.shared.u32 	[_ZZ12MatMulKernelE8blockElt], %r54;
	st.shared.u32 	[_ZZ12MatMulKernelE9blockxInd], %r2;
	mov.u32 	%r35, %ctaid.y;
	shl.b32 	%r36, %r35, 10;
	st.shared.u32 	[_ZZ12MatMulKernelE9blockyInd], %r36;

$L__BB13_4:
	bar.sync 	0;
	ld.shared.u32 	%r37, [_ZZ12MatMulKernelE8blockElt];
	setp.ge.u32 	%p3, %r1, %r37;
	@%p3 bra 	$L__BB13_6;

	ld.shared.u32 	%r38, [_ZZ12MatMulKernelE9blockxInd];
	add.s32 	%r39, %r38, %r1;
	cvta.to.global.u64 	%rd14, %rd12;
	mul.wide.u32 	%rd15, %r39, 2;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u16 	%rs9, [%rd16];
	shl.b32 	%r40, %r1, 1;
	mov.u32 	%r41, _ZZ12MatMulKernelE1b;
	add.s32 	%r42, %r41, %r40;
	st.shared.u16 	[%r42], %rs9;

$L__BB13_6:
	bar.sync 	0;
	ld.const.u16 	%rs47, [sh];
	ld.shared.u32 	%r43, [_ZZ12MatMulKernelE9blockyInd];
	add.s32 	%r5, %r43, %r1;
	setp.ge.s32 	%p4, %r5, %r26;
	@%p4 bra 	$L__BB13_15;

	ld.shared.u32 	%r6, [_ZZ12MatMulKernelE8blockElt];
	setp.lt.s32 	%p5, %r6, 1;
	@%p5 bra 	$L__BB13_14;

	ld.shared.u32 	%r7, [_ZZ12MatMulKernelE9blockxInd];
	mov.u32 	%r59, 0;
	and.b32  	%r61, %r6, 3;
	add.s32 	%r45, %r6, -1;
	setp.lt.u32 	%p6, %r45, 3;
	@%p6 bra 	$L__BB13_11;

	sub.s32 	%r58, %r6, %r61;
	add.s32 	%r48, %r7, 1;
	mad.lo.s32 	%r55, %r26, %r48, %r5;
	mad.lo.s32 	%r49, %r7, %r26, %r5;
	mul.wide.s32 	%rd17, %r49, 2;
	add.s64 	%rd25, %rd1, %rd17;
	shl.b32 	%r11, %r26, 2;
	mul.wide.s32 	%rd3, %r11, 2;
	mul.wide.s32 	%rd4, %r26, 2;
	mov.u32 	%r56, _ZZ12MatMulKernelE1b;

$L__BB13_10:
	ld.shared.u16 	%rs12, [%r56];
	ld.global.u16 	%rs13, [%rd25];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs13;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs47,%rs11;
}
	// end inline asm
	ld.shared.u16 	%rs18, [%r56+2];
	mul.wide.s32 	%rd18, %r55, 2;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.u16 	%rs19, [%rd19];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs14,%rs17;
}
	// end inline asm
	ld.shared.u16 	%rs24, [%r56+4];
	add.s64 	%rd20, %rd19, %rd4;
	ld.global.u16 	%rs25, [%rd20];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs20,%rs23;
}
	// end inline asm
	ld.shared.u16 	%rs30, [%r56+6];
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.u16 	%rs31, [%rd21];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs26,%rs29;
}
	// end inline asm
	add.s32 	%r59, %r59, 4;
	add.s32 	%r56, %r56, 8;
	add.s32 	%r55, %r55, %r11;
	add.s64 	%rd25, %rd25, %rd3;
	add.s32 	%r58, %r58, -4;
	setp.ne.s32 	%p7, %r58, 0;
	@%p7 bra 	$L__BB13_10;

$L__BB13_11:
	setp.eq.s32 	%p8, %r61, 0;
	@%p8 bra 	$L__BB13_14;

	shl.b32 	%r50, %r59, 1;
	mov.u32 	%r51, _ZZ12MatMulKernelE1b;
	add.s32 	%r60, %r51, %r50;
	add.s32 	%r52, %r59, %r7;
	mad.lo.s32 	%r53, %r26, %r52, %r5;
	mul.wide.s32 	%rd22, %r53, 2;
	add.s64 	%rd26, %rd1, %rd22;
	mul.wide.s32 	%rd8, %r26, 2;

$L__BB13_13:
	.pragma "nounroll";
	ld.shared.u16 	%rs36, [%r60];
	ld.global.u16 	%rs37, [%rd26];
	// begin inline asm
	{mul.f16 %rs35,%rs36,%rs37;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs47,%rs35;
}
	// end inline asm
	add.s32 	%r60, %r60, 2;
	add.s64 	%rd26, %rd26, %rd8;
	add.s32 	%r61, %r61, -1;
	setp.ne.s32 	%p9, %r61, 0;
	@%p9 bra 	$L__BB13_13;

$L__BB13_14:
	mul.wide.s32 	%rd24, %r5, 2;
	add.s64 	%rd23, %rd11, %rd24;
	// begin inline asm
	{ atom.add.noftz.f16 %rs41,[%rd23],%rs47; }

	// end inline asm

$L__BB13_15:
	ret;

}
	// .globl	MatMulKernelT
.visible .entry MatMulKernelT(
	.param .u64 MatMulKernelT_param_0,
	.param .u64 MatMulKernelT_param_1,
	.param .u64 MatMulKernelT_param_2,
	.param .u32 MatMulKernelT_param_3,
	.param .u32 MatMulKernelT_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<48>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<21>;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE8blockElt;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE9blockxInd;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE9blockyInd;
	// demoted variable
	.shared .align 2 .b8 _ZZ13MatMulKernelTE1b[128];

	ld.param.u64 	%rd8, [MatMulKernelT_param_0];
	ld.param.u64 	%rd9, [MatMulKernelT_param_1];
	ld.param.u64 	%rd10, [MatMulKernelT_param_2];
	ld.param.u32 	%r22, [MatMulKernelT_param_3];
	ld.param.u32 	%r23, [MatMulKernelT_param_4];
	cvta.to.global.u64 	%rd1, %rd10;
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB14_4;

	mov.u32 	%r25, %ctaid.y;
	shl.b32 	%r2, %r25, 6;
	add.s32 	%r26, %r2, 64;
	mov.u32 	%r49, 64;
	setp.le.u32 	%p2, %r26, %r22;
	@%p2 bra 	$L__BB14_3;

	shr.s32 	%r27, %r22, 31;
	shr.u32 	%r28, %r27, 26;
	add.s32 	%r29, %r22, %r28;
	and.b32  	%r30, %r29, -64;
	sub.s32 	%r49, %r22, %r30;

$L__BB14_3:
	st.shared.u32 	[_ZZ13MatMulKernelTE8blockElt], %r49;
	mov.u32 	%r31, %ctaid.x;
	shl.b32 	%r32, %r31, 10;
	st.shared.u32 	[_ZZ13MatMulKernelTE9blockxInd], %r32;
	st.shared.u32 	[_ZZ13MatMulKernelTE9blockyInd], %r2;

$L__BB14_4:
	bar.sync 	0;
	ld.shared.u32 	%r33, [_ZZ13MatMulKernelTE8blockElt];
	setp.ge.u32 	%p3, %r1, %r33;
	@%p3 bra 	$L__BB14_6;

	ld.shared.u32 	%r34, [_ZZ13MatMulKernelTE9blockyInd];
	add.s32 	%r35, %r34, %r1;
	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r35, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u16 	%rs9, [%rd13];
	shl.b32 	%r36, %r1, 1;
	mov.u32 	%r37, _ZZ13MatMulKernelTE1b;
	add.s32 	%r38, %r37, %r36;
	st.shared.u16 	[%r38], %rs9;

$L__BB14_6:
	bar.sync 	0;
	ld.const.u16 	%rs47, [sh];
	ld.shared.u32 	%r39, [_ZZ13MatMulKernelTE9blockxInd];
	add.s32 	%r5, %r39, %r1;
	setp.ge.s32 	%p4, %r5, %r23;
	@%p4 bra 	$L__BB14_15;

	ld.shared.u32 	%r6, [_ZZ13MatMulKernelTE8blockElt];
	setp.lt.s32 	%p5, %r6, 1;
	@%p5 bra 	$L__BB14_14;

	ld.shared.u32 	%r7, [_ZZ13MatMulKernelTE9blockyInd];
	mov.u32 	%r53, 0;
	and.b32  	%r55, %r6, 3;
	add.s32 	%r41, %r6, -1;
	setp.lt.u32 	%p6, %r41, 3;
	@%p6 bra 	$L__BB14_11;

	sub.s32 	%r52, %r6, %r55;
	mad.lo.s32 	%r44, %r22, %r5, %r7;
	mul.wide.s32 	%rd14, %r44, 2;
	add.s64 	%rd15, %rd1, %rd14;
	add.s64 	%rd19, %rd15, 4;
	mov.u32 	%r50, _ZZ13MatMulKernelTE1b;

$L__BB14_10:
	ld.shared.u16 	%rs12, [%r50];
	ld.global.u16 	%rs13, [%rd19+-4];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs13;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs47,%rs11;
}
	// end inline asm
	ld.shared.u16 	%rs18, [%r50+2];
	ld.global.u16 	%rs19, [%rd19+-2];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs14,%rs17;
}
	// end inline asm
	ld.shared.u16 	%rs24, [%r50+4];
	ld.global.u16 	%rs25, [%rd19];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs20,%rs23;
}
	// end inline asm
	ld.shared.u16 	%rs30, [%r50+6];
	ld.global.u16 	%rs31, [%rd19+2];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs26,%rs29;
}
	// end inline asm
	add.s32 	%r53, %r53, 4;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r50, %r50, 8;
	add.s32 	%r52, %r52, -4;
	setp.ne.s32 	%p7, %r52, 0;
	@%p7 bra 	$L__BB14_10;

$L__BB14_11:
	setp.eq.s32 	%p8, %r55, 0;
	@%p8 bra 	$L__BB14_14;

	shl.b32 	%r45, %r53, 1;
	mov.u32 	%r46, _ZZ13MatMulKernelTE1b;
	add.s32 	%r54, %r46, %r45;
	add.s32 	%r47, %r53, %r7;
	mad.lo.s32 	%r48, %r22, %r5, %r47;
	mul.wide.s32 	%rd16, %r48, 2;
	add.s64 	%rd20, %rd1, %rd16;

$L__BB14_13:
	.pragma "nounroll";
	ld.shared.u16 	%rs36, [%r54];
	ld.global.u16 	%rs37, [%rd20];
	// begin inline asm
	{mul.f16 %rs35,%rs36,%rs37;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs47,%rs35;
}
	// end inline asm
	add.s32 	%r54, %r54, 2;
	add.s64 	%rd20, %rd20, 2;
	add.s32 	%r55, %r55, -1;
	setp.ne.s32 	%p9, %r55, 0;
	@%p9 bra 	$L__BB14_13;

$L__BB14_14:
	mul.wide.s32 	%rd18, %r5, 2;
	add.s64 	%rd17, %rd8, %rd18;
	// begin inline asm
	{ atom.add.noftz.f16 %rs41,[%rd17],%rs47; }

	// end inline asm

$L__BB14_15:
	ret;

}
	// .globl	dotT_VectorAndMatrix
.visible .entry dotT_VectorAndMatrix(
	.param .u64 dotT_VectorAndMatrix_param_0,
	.param .u64 dotT_VectorAndMatrix_param_1,
	.param .u64 dotT_VectorAndMatrix_param_2,
	.param .u32 dotT_VectorAndMatrix_param_3,
	.param .u32 dotT_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd17, [dotT_VectorAndMatrix_param_0];
	ld.param.u64 	%rd18, [dotT_VectorAndMatrix_param_1];
	ld.param.u64 	%rd16, [dotT_VectorAndMatrix_param_2];
	ld.param.u32 	%r11, [dotT_VectorAndMatrix_param_3];
	ld.param.u32 	%r12, [dotT_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd17;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB15_9;

	ld.const.u16 	%rs1, [sh];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs1;}

	// end inline asm
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB15_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB15_5;

	sub.s32 	%r21, %r11, %r23;
	mul.wide.s32 	%rd19, %r1, 4;
	add.s64 	%rd29, %rd1, %rd19;
	mul.wide.s32 	%rd4, %r12, 4;
	mov.u64 	%rd28, %rd2;

$L__BB15_4:
	ld.global.nc.f32 	%f11, [%rd29];
	ld.global.nc.f32 	%f12, [%rd28];
	fma.rn.ftz.f32 	%f13, %f12, %f11, %f28;
	add.s64 	%rd20, %rd29, %rd4;
	ld.global.nc.f32 	%f14, [%rd20];
	ld.global.nc.f32 	%f15, [%rd28+4];
	fma.rn.ftz.f32 	%f16, %f15, %f14, %f13;
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.nc.f32 	%f17, [%rd21];
	ld.global.nc.f32 	%f18, [%rd28+8];
	fma.rn.ftz.f32 	%f19, %f18, %f17, %f16;
	add.s64 	%rd22, %rd21, %rd4;
	add.s64 	%rd29, %rd22, %rd4;
	ld.global.nc.f32 	%f20, [%rd22];
	ld.global.nc.f32 	%f21, [%rd28+12];
	fma.rn.ftz.f32 	%f28, %f21, %f20, %f19;
	add.s32 	%r22, %r22, 4;
	add.s64 	%rd28, %rd28, 16;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB15_4;

$L__BB15_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB15_8;

	mul.wide.s32 	%rd23, %r22, 4;
	add.s64 	%rd31, %rd2, %rd23;
	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd24, %r19, 4;
	add.s64 	%rd30, %rd1, %rd24;
	mul.wide.s32 	%rd11, %r12, 4;

$L__BB15_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f22, [%rd30];
	ld.global.nc.f32 	%f23, [%rd31];
	fma.rn.ftz.f32 	%f28, %f23, %f22, %f28;
	add.s64 	%rd31, %rd31, 4;
	add.s64 	%rd30, %rd30, %rd11;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB15_7;

$L__BB15_8:
	cvta.to.global.u64 	%rd25, %rd16;
	mul.wide.s32 	%rd26, %r1, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f28;

$L__BB15_9:
	ret;

}
	// .globl	derivativeWeight
.visible .entry derivativeWeight(
	.param .u64 derivativeWeight_param_0,
	.param .u64 derivativeWeight_param_1,
	.param .u64 derivativeWeight_param_2,
	.param .u32 derivativeWeight_param_3,
	.param .u32 derivativeWeight_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [derivativeWeight_param_0];
	ld.param.u64 	%rd2, [derivativeWeight_param_1];
	ld.param.u64 	%rd3, [derivativeWeight_param_2];
	ld.param.u32 	%r4, [derivativeWeight_param_3];
	ld.param.u32 	%r3, [derivativeWeight_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB16_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd5, %r11, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r2, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.f32 	%f1, [%rd12];
	ld.global.nc.f32 	%f2, [%rd9];
	ld.global.f32 	%f3, [%rd6];
	fma.rn.ftz.f32 	%f4, %f2, %f1, %f3;
	st.global.f32 	[%rd6], %f4;

$L__BB16_2:
	ret;

}
	// .globl	derivativeWeight_half
.visible .entry derivativeWeight_half(
	.param .u64 derivativeWeight_half_param_0,
	.param .u64 derivativeWeight_half_param_1,
	.param .u64 derivativeWeight_half_param_2,
	.param .u32 derivativeWeight_half_param_3,
	.param .u32 derivativeWeight_half_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [derivativeWeight_half_param_0];
	ld.param.u64 	%rd2, [derivativeWeight_half_param_1];
	ld.param.u64 	%rd3, [derivativeWeight_half_param_2];
	ld.param.u32 	%r4, [derivativeWeight_half_param_3];
	ld.param.u32 	%r3, [derivativeWeight_half_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB17_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs2, [%rd7];
	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.s32 	%rd9, %r2, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs3, [%rd10];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	mul.wide.s32 	%rd11, %r11, 2;
	add.s64 	%rd12, %rd4, %rd11;
	ld.global.u16 	%rs5, [%rd12];
	// begin inline asm
	{add.f16 %rs4,%rs5,%rs1;
}
	// end inline asm
	st.global.u16 	[%rd12], %rs4;

$L__BB17_2:
	ret;

}
	// .globl	addMatrix
.visible .entry addMatrix(
	.param .u64 addMatrix_param_0,
	.param .u64 addMatrix_param_1,
	.param .u32 addMatrix_param_2,
	.param .u32 addMatrix_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd13, [addMatrix_param_0];
	ld.param.u64 	%rd12, [addMatrix_param_1];
	ld.param.u32 	%r11, [addMatrix_param_2];
	ld.param.u32 	%r12, [addMatrix_param_3];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB18_9;

	cvta.to.global.u64 	%rd14, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd15, %r1, 4;
	add.s64 	%rd3, %rd14, %rd15;
	ld.global.f32 	%f22, [%rd3];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB18_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB18_5;

	sub.s32 	%r21, %r11, %r23;
	shl.b64 	%rd16, %rd2, 2;
	add.s64 	%rd21, %rd1, %rd16;
	mul.wide.s32 	%rd5, %r12, 4;

$L__BB18_4:
	ld.global.nc.f32 	%f10, [%rd21];
	add.ftz.f32 	%f11, %f22, %f10;
	add.s64 	%rd17, %rd21, %rd5;
	ld.global.nc.f32 	%f12, [%rd17];
	add.ftz.f32 	%f13, %f11, %f12;
	add.s64 	%rd18, %rd17, %rd5;
	ld.global.nc.f32 	%f14, [%rd18];
	add.ftz.f32 	%f15, %f13, %f14;
	add.s64 	%rd19, %rd18, %rd5;
	add.s64 	%rd21, %rd19, %rd5;
	ld.global.nc.f32 	%f16, [%rd19];
	add.ftz.f32 	%f22, %f15, %f16;
	add.s32 	%r22, %r22, 4;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB18_4;

$L__BB18_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB18_8;

	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd20, %r19, 4;
	add.s64 	%rd22, %rd1, %rd20;
	mul.wide.s32 	%rd9, %r12, 4;

$L__BB18_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f17, [%rd22];
	add.ftz.f32 	%f22, %f22, %f17;
	add.s64 	%rd22, %rd22, %rd9;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB18_7;

$L__BB18_8:
	st.global.f32 	[%rd3], %f22;

$L__BB18_9:
	ret;

}
	// .globl	addMatrix_half
.visible .entry addMatrix_half(
	.param .u64 addMatrix_half_param_0,
	.param .u64 addMatrix_half_param_1,
	.param .u32 addMatrix_half_param_2,
	.param .u32 addMatrix_half_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<30>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd13, [addMatrix_half_param_0];
	ld.param.u64 	%rd12, [addMatrix_half_param_1];
	ld.param.u32 	%r11, [addMatrix_half_param_2];
	ld.param.u32 	%r12, [addMatrix_half_param_3];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB19_9;

	cvta.to.global.u64 	%rd14, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd15, %r1, 2;
	add.s64 	%rd3, %rd14, %rd15;
	ld.global.u16 	%rs29, [%rd3];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB19_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB19_5;

	sub.s32 	%r21, %r11, %r23;
	shl.b64 	%rd16, %rd2, 1;
	add.s64 	%rd21, %rd1, %rd16;
	mul.wide.s32 	%rd5, %r12, 2;

$L__BB19_4:
	ld.global.nc.u16 	%rs12, [%rd21];
	// begin inline asm
	{add.f16 %rs10,%rs29,%rs12;
}
	// end inline asm
	add.s64 	%rd17, %rd21, %rd5;
	ld.global.nc.u16 	%rs15, [%rd17];
	// begin inline asm
	{add.f16 %rs13,%rs10,%rs15;
}
	// end inline asm
	add.s64 	%rd18, %rd17, %rd5;
	ld.global.nc.u16 	%rs18, [%rd18];
	// begin inline asm
	{add.f16 %rs16,%rs13,%rs18;
}
	// end inline asm
	add.s64 	%rd19, %rd18, %rd5;
	add.s64 	%rd21, %rd19, %rd5;
	ld.global.nc.u16 	%rs21, [%rd19];
	// begin inline asm
	{add.f16 %rs29,%rs16,%rs21;
}
	// end inline asm
	add.s32 	%r22, %r22, 4;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB19_4;

$L__BB19_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB19_8;

	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd20, %r19, 2;
	add.s64 	%rd22, %rd1, %rd20;
	mul.wide.s32 	%rd9, %r12, 2;

$L__BB19_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs24, [%rd22];
	// begin inline asm
	{add.f16 %rs29,%rs29,%rs24;
}
	// end inline asm
	add.s64 	%rd22, %rd22, %rd9;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB19_7;

$L__BB19_8:
	st.global.u16 	[%rd3], %rs29;

$L__BB19_9:
	ret;

}
	// .globl	reduceMaxIdxOptimizedShared
.visible .entry reduceMaxIdxOptimizedShared(
	.param .u64 reduceMaxIdxOptimizedShared_param_0,
	.param .u32 reduceMaxIdxOptimizedShared_param_1,
	.param .u64 reduceMaxIdxOptimizedShared_param_2,
	.param .u64 reduceMaxIdxOptimizedShared_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .f32 _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax;
	// demoted variable
	.shared .align 4 .u32 _ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx;

	ld.param.u64 	%rd2, [reduceMaxIdxOptimizedShared_param_0];
	ld.param.u32 	%r12, [reduceMaxIdxOptimizedShared_param_1];
	ld.param.u64 	%rd3, [reduceMaxIdxOptimizedShared_param_2];
	ld.param.u64 	%rd4, [reduceMaxIdxOptimizedShared_param_3];
	mov.u32 	%r18, %tid.x;
	setp.ne.s32 	%p1, %r18, 0;
	@%p1 bra 	$L__BB20_2;

	mov.u32 	%r13, 0;
	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax], %r13;
	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx], %r13;

$L__BB20_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r18, %r12;
	mov.u32 	%r20, 0;
	mov.f32 	%f12, 0f00000000;
	@%p2 bra 	$L__BB20_5;

	mov.u32 	%r2, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB20_4:
	mul.wide.s32 	%rd5, %r18, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f7, [%rd6];
	setp.lt.ftz.f32 	%p3, %f12, %f7;
	selp.f32 	%f12, %f7, %f12, %p3;
	selp.b32 	%r20, %r18, %r20, %p3;
	add.s32 	%r18, %r18, %r2;
	setp.lt.s32 	%p4, %r18, %r12;
	@%p4 bra 	$L__BB20_4;

$L__BB20_5:
	ld.shared.f32 	%f4, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	setp.ge.ftz.f32 	%p5, %f4, %f12;
	@%p5 bra 	$L__BB20_9;

	mov.b32 	%r21, %f4;
	mov.b32 	%r9, %f12;

$L__BB20_7:
	mov.b32 	%f8, %r21;
	setp.le.ftz.f32 	%p6, %f12, %f8;
	@%p6 bra 	$L__BB20_9;

	mov.u32 	%r16, _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax;
	atom.shared.cas.b32 	%r11, [%r16], %r21, %r9;
	setp.ne.s32 	%p7, %r21, %r11;
	mov.u32 	%r21, %r11;
	@%p7 bra 	$L__BB20_7;

$L__BB20_9:
	bar.sync 	0;
	ld.shared.f32 	%f9, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	setp.neu.ftz.f32 	%p8, %f9, %f12;
	@%p8 bra 	$L__BB20_11;

	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx], %r20;

$L__BB20_11:
	bar.sync 	0;
	@%p1 bra 	$L__BB20_13;

	ld.shared.f32 	%f10, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	cvta.to.global.u64 	%rd7, %rd3;
	st.global.f32 	[%rd7], %f10;
	ld.shared.u32 	%r17, [_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx];
	cvta.to.global.u64 	%rd8, %rd4;
	st.global.u32 	[%rd8], %r17;

$L__BB20_13:
	ret;

}
	// .globl	reverse
.visible .entry reverse(
	.param .u64 reverse_param_0,
	.param .u32 reverse_param_1,
	.param .u32 reverse_param_2,
	.param .u32 reverse_param_3
)
{



	ret;

}
	// .globl	sharedMem_transpose
.visible .entry sharedMem_transpose(
	.param .u64 sharedMem_transpose_param_0,
	.param .u64 sharedMem_transpose_param_1,
	.param .u32 sharedMem_transpose_param_2,
	.param .u32 sharedMem_transpose_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ19sharedMem_transposeE8M_Shared[4096];

	ld.param.u64 	%rd1, [sharedMem_transpose_param_0];
	ld.param.u64 	%rd2, [sharedMem_transpose_param_1];
	ld.param.u32 	%r5, [sharedMem_transpose_param_2];
	ld.param.u32 	%r6, [sharedMem_transpose_param_3];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	shl.b32 	%r11, %r10, 5;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r2, %r11, %r12;
	setp.lt.s32 	%p2, %r1, %r5;
	setp.lt.s32 	%p3, %r2, %r6;
	and.pred  	%p1, %p2, %p3;
	shl.b32 	%r13, %r12, 7;
	mov.u32 	%r14, _ZZ19sharedMem_transposeE8M_Shared;
	add.s32 	%r15, %r14, %r13;
	shl.b32 	%r16, %r9, 2;
	add.s32 	%r3, %r15, %r16;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB22_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r17, %r1, %r6, %r2;
	mul.wide.s32 	%rd4, %r17, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	st.shared.f32 	[%r3], %f1;

$L__BB22_2:
	bar.sync 	0;
	mad.lo.s32 	%r4, %r2, %r5, %r1;
	@%p4 bra 	$L__BB22_4;

	ld.shared.f32 	%f2, [%r3];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f2;

$L__BB22_4:
	ret;

}
	// .globl	sharedMem_transpose_half
.visible .entry sharedMem_transpose_half(
	.param .u64 sharedMem_transpose_half_param_0,
	.param .u64 sharedMem_transpose_half_param_1,
	.param .u32 sharedMem_transpose_half_param_2,
	.param .u32 sharedMem_transpose_half_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ24sharedMem_transpose_halfE8M_Shared[2048];

	ld.param.u64 	%rd1, [sharedMem_transpose_half_param_0];
	ld.param.u64 	%rd2, [sharedMem_transpose_half_param_1];
	ld.param.u32 	%r5, [sharedMem_transpose_half_param_2];
	ld.param.u32 	%r6, [sharedMem_transpose_half_param_3];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	shl.b32 	%r11, %r10, 5;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r2, %r11, %r12;
	setp.lt.s32 	%p2, %r1, %r5;
	setp.lt.s32 	%p3, %r2, %r6;
	and.pred  	%p1, %p2, %p3;
	shl.b32 	%r13, %r12, 6;
	mov.u32 	%r14, _ZZ24sharedMem_transpose_halfE8M_Shared;
	add.s32 	%r15, %r14, %r13;
	shl.b32 	%r16, %r9, 1;
	add.s32 	%r3, %r15, %r16;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB23_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r17, %r1, %r6, %r2;
	mul.wide.s32 	%rd4, %r17, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	st.shared.u16 	[%r3], %rs1;

$L__BB23_2:
	bar.sync 	0;
	mad.lo.s32 	%r4, %r2, %r5, %r1;
	@%p4 bra 	$L__BB23_4;

	ld.shared.u16 	%rs2, [%r3];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r4, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB23_4:
	ret;

}
	// .globl	matrixTransposeSolveBankConflicts
.visible .entry matrixTransposeSolveBankConflicts(
	.param .u64 matrixTransposeSolveBankConflicts_param_0,
	.param .u64 matrixTransposeSolveBankConflicts_param_1,
	.param .u32 matrixTransposeSolveBankConflicts_param_2,
	.param .u32 matrixTransposeSolveBankConflicts_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ33matrixTransposeSolveBankConflictsE3mat[2112];

	ld.param.u64 	%rd1, [matrixTransposeSolveBankConflicts_param_0];
	ld.param.u64 	%rd2, [matrixTransposeSolveBankConflicts_param_1];
	ld.param.u32 	%r7, [matrixTransposeSolveBankConflicts_param_2];
	ld.param.u32 	%r8, [matrixTransposeSolveBankConflicts_param_3];
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 5;
	mov.u32 	%r11, %ctaid.y;
	shl.b32 	%r12, %r11, 5;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r12, %r1;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r10, %r3;
	add.s32 	%r5, %r10, %r1;
	add.s32 	%r6, %r12, %r3;
	setp.ge.s32 	%p1, %r2, %r7;
	setp.ge.s32 	%p2, %r4, %r8;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB24_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r13, %r2, %r8, %r4;
	mul.wide.s32 	%rd4, %r13, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	mov.u32 	%r14, _ZZ33matrixTransposeSolveBankConflictsE3mat;
	mad.lo.s32 	%r15, %r1, 66, %r14;
	shl.b32 	%r16, %r3, 1;
	add.s32 	%r17, %r15, %r16;
	st.shared.u16 	[%r17], %rs1;

$L__BB24_2:
	bar.sync 	0;
	setp.ge.s32 	%p4, %r5, %r7;
	setp.ge.s32 	%p5, %r6, %r8;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB24_4;

	mad.lo.s32 	%r18, %r5, %r7, %r6;
	mov.u32 	%r19, _ZZ33matrixTransposeSolveBankConflictsE3mat;
	mad.lo.s32 	%r20, %r3, 66, %r19;
	shl.b32 	%r21, %r1, 1;
	add.s32 	%r22, %r20, %r21;
	ld.shared.u16 	%rs2, [%r22];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB24_4:
	ret;

}
	// .globl	transposeV3
.visible .entry transposeV3(
	.param .u64 transposeV3_param_0,
	.param .u64 transposeV3_param_1,
	.param .u32 transposeV3_param_2,
	.param .u32 transposeV3_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ11transposeV3E3s_A[2112];

	ld.param.u64 	%rd1, [transposeV3_param_0];
	ld.param.u64 	%rd2, [transposeV3_param_1];
	ld.param.u32 	%r9, [transposeV3_param_2];
	ld.param.u32 	%r10, [transposeV3_param_3];
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %ntid.y;
	mul.lo.s32 	%r4, %r14, %r13;
	mov.u32 	%r5, %tid.y;
	add.s32 	%r6, %r4, %r5;
	setp.ge.s32 	%p1, %r3, %r9;
	setp.ge.s32 	%p2, %r6, %r10;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB25_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r15, %r6, %r10, %r3;
	mul.wide.s32 	%rd4, %r15, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	mov.u32 	%r16, _ZZ11transposeV3E3s_A;
	mad.lo.s32 	%r17, %r5, 66, %r16;
	shl.b32 	%r18, %r2, 1;
	add.s32 	%r19, %r17, %r18;
	st.shared.u16 	[%r19], %rs1;

$L__BB25_2:
	bar.sync 	0;
	add.s32 	%r7, %r4, %r2;
	setp.ge.s32 	%p4, %r7, %r9;
	add.s32 	%r8, %r1, %r5;
	setp.ge.s32 	%p5, %r8, %r10;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB25_4;

	mad.lo.s32 	%r20, %r8, %r9, %r7;
	mov.u32 	%r21, _ZZ11transposeV3E3s_A;
	mad.lo.s32 	%r22, %r2, 66, %r21;
	shl.b32 	%r23, %r5, 1;
	add.s32 	%r24, %r22, %r23;
	ld.shared.u16 	%rs2, [%r24];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r20, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB25_4:
	ret;

}
	// .globl	matrixDiv
.visible .entry matrixDiv(
	.param .u64 matrixDiv_param_0,
	.param .align 2 .b8 matrixDiv_param_1[2],
	.param .u32 matrixDiv_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs5, [matrixDiv_param_1];
	ld.param.u64 	%rd2, [matrixDiv_param_0];
	ld.param.u32 	%r2, [matrixDiv_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB26_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs6, [%rd1];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs6;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs5;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs9,%rs19;
}
	// end inline asm
	mov.u16 	%rs13, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs9, %rs13;
  selp.u16 %rs11, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs11, 0;
	@%p2 bra 	$L__BB26_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs14, %rs9;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs15, 0;
	@%p3 bra 	$L__BB26_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f11;}

	// end inline asm

$L__BB26_4:
	st.global.u16 	[%rd1], %rs19;

$L__BB26_5:
	ret;

}
	// .globl	matrixDiv_float
.visible .entry matrixDiv_float(
	.param .u64 matrixDiv_float_param_0,
	.param .f32 matrixDiv_float_param_1,
	.param .u32 matrixDiv_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [matrixDiv_float_param_0];
	ld.param.f32 	%f1, [matrixDiv_float_param_1];
	ld.param.u32 	%r2, [matrixDiv_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB27_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	div.approx.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB27_2:
	ret;

}
	// .globl	addCopy
.visible .entry addCopy(
	.param .u64 addCopy_param_0,
	.param .u64 addCopy_param_1,
	.param .u32 addCopy_param_2,
	.param .u32 addCopy_param_3,
	.param .u32 addCopy_param_4,
	.param .u32 addCopy_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addCopy_param_0];
	ld.param.u64 	%rd2, [addCopy_param_1];
	ld.param.u32 	%r7, [addCopy_param_2];
	ld.param.u32 	%r4, [addCopy_param_3];
	ld.param.u32 	%r5, [addCopy_param_4];
	ld.param.u32 	%r6, [addCopy_param_5];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %ntid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	mov.u32 	%r14, %nctaid.y;
	mul.lo.s32 	%r15, %r14, %r12;
	mad.lo.s32 	%r3, %r15, %r1, %r2;
	mul.lo.s32 	%r16, %r5, %r7;
	setp.ge.s32 	%p1, %r3, %r16;
	@%p1 bra 	$L__BB28_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r17, %r1, %r4, %r2;
	mad.lo.s32 	%r18, %r6, %r5, %r17;
	mul.wide.s32 	%rd4, %r3, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB28_2:
	ret;

}
	// .globl	addCopy_half
.visible .entry addCopy_half(
	.param .u64 addCopy_half_param_0,
	.param .u64 addCopy_half_param_1,
	.param .u32 addCopy_half_param_2,
	.param .u32 addCopy_half_param_3,
	.param .u32 addCopy_half_param_4,
	.param .u32 addCopy_half_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addCopy_half_param_0];
	ld.param.u64 	%rd2, [addCopy_half_param_1];
	ld.param.u32 	%r7, [addCopy_half_param_2];
	ld.param.u32 	%r4, [addCopy_half_param_3];
	ld.param.u32 	%r5, [addCopy_half_param_4];
	ld.param.u32 	%r6, [addCopy_half_param_5];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %ntid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	mov.u32 	%r14, %nctaid.y;
	mul.lo.s32 	%r15, %r14, %r12;
	mad.lo.s32 	%r3, %r15, %r1, %r2;
	mul.lo.s32 	%r16, %r5, %r7;
	setp.ge.s32 	%p1, %r3, %r16;
	@%p1 bra 	$L__BB29_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r17, %r1, %r4, %r2;
	mad.lo.s32 	%r18, %r6, %r5, %r17;
	mul.wide.s32 	%rd4, %r3, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB29_2:
	ret;

}
	// .globl	NormalizationLayerForward2D
.visible .entry NormalizationLayerForward2D(
	.param .u64 NormalizationLayerForward2D_param_0,
	.param .u64 NormalizationLayerForward2D_param_1,
	.param .u64 NormalizationLayerForward2D_param_2,
	.param .u32 NormalizationLayerForward2D_param_3,
	.param .u32 NormalizationLayerForward2D_param_4,
	.param .u32 NormalizationLayerForward2D_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<99>;


	ld.param.u64 	%rd45, [NormalizationLayerForward2D_param_0];
	ld.param.u64 	%rd46, [NormalizationLayerForward2D_param_1];
	ld.param.u64 	%rd47, [NormalizationLayerForward2D_param_2];
	ld.param.u32 	%r35, [NormalizationLayerForward2D_param_3];
	ld.param.u32 	%r34, [NormalizationLayerForward2D_param_4];
	ld.param.u32 	%r36, [NormalizationLayerForward2D_param_5];
	cvta.to.global.u64 	%rd1, %rd47;
	cvta.to.global.u64 	%rd2, %rd46;
	cvta.to.global.u64 	%rd3, %rd45;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r38, %r37, %r39;
	mov.u32 	%r40, %ctaid.y;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r42, %tid.y;
	mad.lo.s32 	%r2, %r41, %r40, %r42;
	setp.ge.s32 	%p1, %r1, %r36;
	setp.ge.s32 	%p2, %r2, %r35;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB30_22;

	ld.global.u64 	%rd48, [%rd3];
	cvta.to.global.u64 	%rd49, %rd48;
	cvt.s64.s32 	%rd4, %r1;
	mul.wide.s32 	%rd50, %r1, 8;
	add.s64 	%rd51, %rd49, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	cvta.to.global.u64 	%rd5, %rd52;
	ld.global.u64 	%rd53, [%rd3+8];
	cvta.to.global.u64 	%rd54, %rd53;
	add.s64 	%rd55, %rd54, %rd50;
	ld.global.u64 	%rd56, [%rd55];
	cvta.to.global.u64 	%rd57, %rd56;
	cvt.s64.s32 	%rd6, %r2;
	mul.wide.s32 	%rd58, %r2, 4;
	add.s64 	%rd7, %rd57, %rd58;
	ld.global.f32 	%f80, [%rd7];
	mul.lo.s32 	%r60, %r2, %r34;
	setp.lt.s32 	%p4, %r34, 1;
	@%p4 bra 	$L__BB30_8;

	add.s32 	%r43, %r34, -1;
	and.b32  	%r51, %r34, 3;
	setp.lt.u32 	%p5, %r43, 3;
	mov.u32 	%r50, %r60;
	@%p5 bra 	$L__BB30_5;

	sub.s32 	%r49, %r34, %r51;
	mul.wide.s32 	%rd59, %r60, 4;
	add.s64 	%rd60, %rd5, %rd59;
	add.s64 	%rd87, %rd60, 8;
	mov.u32 	%r50, %r60;

$L__BB30_4:
	ld.global.f32 	%f21, [%rd87+-8];
	add.ftz.f32 	%f22, %f80, %f21;
	ld.global.f32 	%f23, [%rd87+-4];
	add.ftz.f32 	%f24, %f22, %f23;
	ld.global.f32 	%f25, [%rd87];
	add.ftz.f32 	%f26, %f24, %f25;
	ld.global.f32 	%f27, [%rd87+4];
	add.ftz.f32 	%f80, %f26, %f27;
	add.s32 	%r50, %r50, 4;
	add.s64 	%rd87, %rd87, 16;
	add.s32 	%r49, %r49, -4;
	setp.ne.s32 	%p6, %r49, 0;
	@%p6 bra 	$L__BB30_4;

$L__BB30_5:
	setp.eq.s32 	%p7, %r51, 0;
	@%p7 bra 	$L__BB30_8;

	mul.wide.s32 	%rd61, %r50, 4;
	add.s64 	%rd88, %rd5, %rd61;

$L__BB30_7:
	.pragma "nounroll";
	ld.global.f32 	%f28, [%rd88];
	add.ftz.f32 	%f80, %f80, %f28;
	add.s64 	%rd88, %rd88, 4;
	add.s32 	%r51, %r51, -1;
	setp.ne.s32 	%p8, %r51, 0;
	@%p8 bra 	$L__BB30_7;

$L__BB30_8:
	cvt.rn.f32.s32 	%f9, %r34;
	div.approx.ftz.f32 	%f29, %f80, %f9;
	st.global.f32 	[%rd7], %f29;
	ld.global.u64 	%rd62, [%rd3+16];
	cvta.to.global.u64 	%rd63, %rd62;
	shl.b64 	%rd64, %rd4, 3;
	add.s64 	%rd65, %rd63, %rd64;
	ld.global.u64 	%rd66, [%rd65];
	cvta.to.global.u64 	%rd67, %rd66;
	shl.b64 	%rd68, %rd6, 2;
	add.s64 	%rd15, %rd67, %rd68;
	ld.global.f32 	%f85, [%rd15];
	ld.global.u64 	%rd69, [%rd3+8];
	cvta.to.global.u64 	%rd70, %rd69;
	add.s64 	%rd71, %rd70, %rd64;
	ld.global.u64 	%rd72, [%rd71];
	cvta.to.global.u64 	%rd73, %rd72;
	add.s64 	%rd74, %rd73, %rd68;
	ld.global.f32 	%f11, [%rd74];
	@%p4 bra 	$L__BB30_15;

	add.s32 	%r44, %r34, -1;
	and.b32  	%r55, %r34, 3;
	setp.lt.u32 	%p10, %r44, 3;
	mov.u32 	%r54, %r60;
	@%p10 bra 	$L__BB30_12;

	sub.s32 	%r53, %r34, %r55;
	mul.wide.s32 	%rd75, %r60, 4;
	add.s64 	%rd76, %rd5, %rd75;
	add.s64 	%rd89, %rd76, 8;
	mov.u32 	%r54, %r60;

$L__BB30_11:
	ld.global.f32 	%f31, [%rd89+-8];
	sub.ftz.f32 	%f32, %f31, %f11;
	fma.rn.ftz.f32 	%f33, %f32, %f32, %f85;
	ld.global.f32 	%f34, [%rd89+-4];
	sub.ftz.f32 	%f35, %f34, %f11;
	fma.rn.ftz.f32 	%f36, %f35, %f35, %f33;
	ld.global.f32 	%f37, [%rd89];
	sub.ftz.f32 	%f38, %f37, %f11;
	fma.rn.ftz.f32 	%f39, %f38, %f38, %f36;
	ld.global.f32 	%f40, [%rd89+4];
	sub.ftz.f32 	%f41, %f40, %f11;
	fma.rn.ftz.f32 	%f85, %f41, %f41, %f39;
	add.s32 	%r54, %r54, 4;
	add.s64 	%rd89, %rd89, 16;
	add.s32 	%r53, %r53, -4;
	setp.ne.s32 	%p11, %r53, 0;
	@%p11 bra 	$L__BB30_11;

$L__BB30_12:
	setp.eq.s32 	%p12, %r55, 0;
	@%p12 bra 	$L__BB30_15;

	mul.wide.s32 	%rd77, %r54, 4;
	add.s64 	%rd90, %rd5, %rd77;

$L__BB30_14:
	.pragma "nounroll";
	ld.global.f32 	%f42, [%rd90];
	sub.ftz.f32 	%f43, %f42, %f11;
	fma.rn.ftz.f32 	%f85, %f43, %f43, %f85;
	add.s64 	%rd90, %rd90, 4;
	add.s32 	%r55, %r55, -1;
	setp.ne.s32 	%p13, %r55, 0;
	@%p13 bra 	$L__BB30_14;

$L__BB30_15:
	div.approx.ftz.f32 	%f44, %f85, %f9;
	st.global.f32 	[%rd15], %f44;
	add.ftz.f32 	%f45, %f44, 0f33D6BF95;
	sqrt.approx.ftz.f32 	%f19, %f45;
	ld.global.u64 	%rd78, [%rd3+24];
	cvta.to.global.u64 	%rd79, %rd78;
	add.s64 	%rd81, %rd79, %rd64;
	ld.global.u64 	%rd82, [%rd81];
	cvta.to.global.u64 	%rd22, %rd82;
	@%p4 bra 	$L__BB30_22;

	add.s32 	%r46, %r34, -1;
	and.b32  	%r61, %r34, 3;
	setp.lt.u32 	%p15, %r46, 3;
	mov.u32 	%r59, 0;
	@%p15 bra 	$L__BB30_19;

	sub.s32 	%r58, %r34, %r61;
	mul.wide.s32 	%rd83, %r60, 4;
	add.s64 	%rd84, %rd83, 8;
	add.s64 	%rd94, %rd5, %rd84;
	add.s64 	%rd93, %rd22, %rd84;
	mov.u64 	%rd91, %rd2;
	mov.u64 	%rd92, %rd1;

$L__BB30_18:
	ld.global.f32 	%f46, [%rd94+-8];
	sub.ftz.f32 	%f47, %f46, %f11;
	div.approx.ftz.f32 	%f48, %f47, %f19;
	ld.global.nc.f32 	%f49, [%rd91];
	ld.global.nc.f32 	%f50, [%rd92];
	fma.rn.ftz.f32 	%f51, %f48, %f49, %f50;
	st.global.f32 	[%rd93+-8], %f51;
	ld.global.f32 	%f52, [%rd94+-4];
	sub.ftz.f32 	%f53, %f52, %f11;
	div.approx.ftz.f32 	%f54, %f53, %f19;
	ld.global.nc.f32 	%f55, [%rd91+4];
	ld.global.nc.f32 	%f56, [%rd92+4];
	fma.rn.ftz.f32 	%f57, %f54, %f55, %f56;
	st.global.f32 	[%rd93+-4], %f57;
	ld.global.f32 	%f58, [%rd94];
	sub.ftz.f32 	%f59, %f58, %f11;
	div.approx.ftz.f32 	%f60, %f59, %f19;
	ld.global.nc.f32 	%f61, [%rd91+8];
	ld.global.nc.f32 	%f62, [%rd92+8];
	fma.rn.ftz.f32 	%f63, %f60, %f61, %f62;
	st.global.f32 	[%rd93], %f63;
	ld.global.f32 	%f64, [%rd94+4];
	sub.ftz.f32 	%f65, %f64, %f11;
	div.approx.ftz.f32 	%f66, %f65, %f19;
	ld.global.nc.f32 	%f67, [%rd91+12];
	ld.global.nc.f32 	%f68, [%rd92+12];
	fma.rn.ftz.f32 	%f69, %f66, %f67, %f68;
	st.global.f32 	[%rd93+4], %f69;
	add.s32 	%r59, %r59, 4;
	add.s32 	%r60, %r60, 4;
	add.s64 	%rd94, %rd94, 16;
	add.s64 	%rd93, %rd93, 16;
	add.s64 	%rd92, %rd92, 16;
	add.s64 	%rd91, %rd91, 16;
	add.s32 	%r58, %r58, -4;
	setp.ne.s32 	%p16, %r58, 0;
	@%p16 bra 	$L__BB30_18;

$L__BB30_19:
	setp.eq.s32 	%p17, %r61, 0;
	@%p17 bra 	$L__BB30_22;

	mul.wide.s32 	%rd85, %r59, 4;
	add.s64 	%rd98, %rd1, %rd85;
	add.s64 	%rd97, %rd2, %rd85;
	mul.wide.s32 	%rd86, %r60, 4;
	add.s64 	%rd96, %rd22, %rd86;
	add.s64 	%rd95, %rd5, %rd86;

$L__BB30_21:
	.pragma "nounroll";
	ld.global.f32 	%f70, [%rd95];
	sub.ftz.f32 	%f71, %f70, %f11;
	div.approx.ftz.f32 	%f72, %f71, %f19;
	ld.global.nc.f32 	%f73, [%rd97];
	ld.global.nc.f32 	%f74, [%rd98];
	fma.rn.ftz.f32 	%f75, %f72, %f73, %f74;
	st.global.f32 	[%rd96], %f75;
	add.s64 	%rd98, %rd98, 4;
	add.s64 	%rd97, %rd97, 4;
	add.s64 	%rd96, %rd96, 4;
	add.s64 	%rd95, %rd95, 4;
	add.s32 	%r61, %r61, -1;
	setp.ne.s32 	%p18, %r61, 0;
	@%p18 bra 	$L__BB30_21;

$L__BB30_22:
	ret;

}
	// .globl	NormalizationLayerBackward2D
.visible .entry NormalizationLayerBackward2D(
	.param .u64 NormalizationLayerBackward2D_param_0,
	.param .u64 NormalizationLayerBackward2D_param_1,
	.param .u64 NormalizationLayerBackward2D_param_2,
	.param .u64 NormalizationLayerBackward2D_param_3,
	.param .u64 NormalizationLayerBackward2D_param_4,
	.param .u32 NormalizationLayerBackward2D_param_5,
	.param .u32 NormalizationLayerBackward2D_param_6,
	.param .u32 NormalizationLayerBackward2D_param_7,
	.param .u32 NormalizationLayerBackward2D_param_8,
	.param .u32 NormalizationLayerBackward2D_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<189>;


	ld.param.u64 	%rd98, [NormalizationLayerBackward2D_param_0];
	ld.param.u64 	%rd100, [NormalizationLayerBackward2D_param_1];
	ld.param.u64 	%rd99, [NormalizationLayerBackward2D_param_2];
	ld.param.u64 	%rd101, [NormalizationLayerBackward2D_param_3];
	ld.param.u64 	%rd102, [NormalizationLayerBackward2D_param_4];
	ld.param.u32 	%r53, [NormalizationLayerBackward2D_param_7];
	ld.param.u32 	%r52, [NormalizationLayerBackward2D_param_8];
	ld.param.u32 	%r54, [NormalizationLayerBackward2D_param_9];
	cvta.to.global.u64 	%rd1, %rd101;
	cvta.to.global.u64 	%rd2, %rd100;
	cvta.to.global.u64 	%rd3, %rd102;
	mov.u32 	%r55, %ctaid.x;
	mov.u32 	%r56, %ntid.x;
	mov.u32 	%r57, %tid.x;
	mad.lo.s32 	%r1, %r56, %r55, %r57;
	mov.u32 	%r58, %ctaid.y;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r60, %tid.y;
	mad.lo.s32 	%r2, %r59, %r58, %r60;
	setp.ge.s32 	%p1, %r1, %r54;
	setp.ge.s32 	%p2, %r2, %r53;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB31_28;

	cvta.to.global.u64 	%rd103, %rd98;
	ld.global.u64 	%rd104, [%rd103];
	cvta.to.global.u64 	%rd105, %rd104;
	mul.wide.s32 	%rd106, %r1, 8;
	add.s64 	%rd107, %rd105, %rd106;
	ld.global.u64 	%rd108, [%rd107];
	cvta.to.global.u64 	%rd4, %rd108;
	ld.global.u64 	%rd109, [%rd103+8];
	cvta.to.global.u64 	%rd110, %rd109;
	add.s64 	%rd111, %rd110, %rd106;
	ld.global.u64 	%rd112, [%rd111];
	cvta.to.global.u64 	%rd113, %rd112;
	mul.wide.s32 	%rd114, %r2, 4;
	add.s64 	%rd115, %rd113, %rd114;
	ld.global.u64 	%rd116, [%rd103+16];
	cvta.to.global.u64 	%rd117, %rd116;
	add.s64 	%rd118, %rd117, %rd106;
	ld.global.u64 	%rd119, [%rd118];
	cvta.to.global.u64 	%rd5, %rd119;
	ld.global.u64 	%rd120, [%rd103+24];
	cvta.to.global.u64 	%rd121, %rd120;
	add.s64 	%rd122, %rd121, %rd106;
	ld.global.u64 	%rd123, [%rd122];
	cvta.to.global.u64 	%rd124, %rd123;
	add.s64 	%rd125, %rd124, %rd114;
	ld.global.f32 	%f1, [%rd125];
	ld.global.u64 	%rd126, [%rd103+32];
	cvta.to.global.u64 	%rd127, %rd126;
	add.s64 	%rd128, %rd127, %rd106;
	ld.global.u64 	%rd129, [%rd128];
	cvta.to.global.u64 	%rd6, %rd129;
	ld.global.u64 	%rd130, [%rd103+40];
	cvta.to.global.u64 	%rd131, %rd130;
	add.s64 	%rd132, %rd131, %rd106;
	ld.global.u64 	%rd133, [%rd132];
	cvta.to.global.u64 	%rd7, %rd133;
	ld.global.f32 	%f2, [%rd115];
	mul.lo.s32 	%r93, %r2, %r52;
	setp.lt.s32 	%p4, %r52, 1;
	mov.f32 	%f211, 0f00000000;
	mov.f32 	%f202, %f211;
	@%p4 bra 	$L__BB31_8;

	add.s32 	%r62, %r52, -1;
	and.b32  	%r76, %r52, 3;
	setp.lt.u32 	%p5, %r62, 3;
	mov.f32 	%f202, 0f00000000;
	mov.u32 	%r74, 0;
	mov.u32 	%r75, %r93;
	@%p5 bra 	$L__BB31_5;

	sub.s32 	%r73, %r52, %r76;
	mul.wide.s32 	%rd134, %r93, 4;
	add.s64 	%rd135, %rd134, 8;
	add.s64 	%rd159, %rd4, %rd135;
	add.s64 	%rd158, %rd5, %rd135;
	mov.u64 	%rd157, %rd2;
	mov.u32 	%r75, %r93;

$L__BB31_4:
	ld.global.nc.f32 	%f31, [%rd157];
	ld.global.f32 	%f32, [%rd159+-8];
	mul.ftz.f32 	%f33, %f32, %f31;
	ld.global.f32 	%f34, [%rd158+-8];
	sub.ftz.f32 	%f35, %f34, %f1;
	fma.rn.ftz.f32 	%f36, %f33, %f35, %f202;
	ld.global.nc.f32 	%f37, [%rd157+4];
	ld.global.f32 	%f38, [%rd159+-4];
	mul.ftz.f32 	%f39, %f38, %f37;
	ld.global.f32 	%f40, [%rd158+-4];
	sub.ftz.f32 	%f41, %f40, %f1;
	fma.rn.ftz.f32 	%f42, %f39, %f41, %f36;
	ld.global.nc.f32 	%f43, [%rd157+8];
	ld.global.f32 	%f44, [%rd159];
	mul.ftz.f32 	%f45, %f44, %f43;
	ld.global.f32 	%f46, [%rd158];
	sub.ftz.f32 	%f47, %f46, %f1;
	fma.rn.ftz.f32 	%f48, %f45, %f47, %f42;
	ld.global.nc.f32 	%f49, [%rd157+12];
	ld.global.f32 	%f50, [%rd159+4];
	mul.ftz.f32 	%f51, %f50, %f49;
	ld.global.f32 	%f52, [%rd158+4];
	sub.ftz.f32 	%f53, %f52, %f1;
	fma.rn.ftz.f32 	%f202, %f51, %f53, %f48;
	add.s32 	%r74, %r74, 4;
	add.s32 	%r75, %r75, 4;
	add.s64 	%rd159, %rd159, 16;
	add.s64 	%rd158, %rd158, 16;
	add.s64 	%rd157, %rd157, 16;
	add.s32 	%r73, %r73, -4;
	setp.ne.s32 	%p6, %r73, 0;
	@%p6 bra 	$L__BB31_4;

$L__BB31_5:
	setp.eq.s32 	%p7, %r76, 0;
	@%p7 bra 	$L__BB31_8;

	mul.wide.s32 	%rd136, %r74, 4;
	add.s64 	%rd162, %rd2, %rd136;
	mul.wide.s32 	%rd137, %r75, 4;
	add.s64 	%rd161, %rd5, %rd137;
	add.s64 	%rd160, %rd4, %rd137;

$L__BB31_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f54, [%rd162];
	ld.global.f32 	%f55, [%rd160];
	mul.ftz.f32 	%f56, %f55, %f54;
	ld.global.f32 	%f57, [%rd161];
	sub.ftz.f32 	%f58, %f57, %f1;
	fma.rn.ftz.f32 	%f202, %f56, %f58, %f202;
	add.s64 	%rd162, %rd162, 4;
	add.s64 	%rd161, %rd161, 4;
	add.s64 	%rd160, %rd160, 4;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p8, %r76, 0;
	@%p8 bra 	$L__BB31_7;

$L__BB31_8:
	mov.f32 	%f212, %f211;
	@%p4 bra 	$L__BB31_15;

	add.s32 	%r65, %r52, -1;
	and.b32  	%r82, %r52, 3;
	setp.lt.u32 	%p10, %r65, 3;
	mov.f32 	%f212, 0f00000000;
	mov.u32 	%r80, 0;
	mov.u32 	%r81, %r93;
	mov.f32 	%f211, %f212;
	@%p10 bra 	$L__BB31_12;

	sub.s32 	%r79, %r52, %r82;
	mul.wide.s32 	%rd138, %r93, 4;
	add.s64 	%rd139, %rd138, 8;
	add.s64 	%rd165, %rd4, %rd139;
	add.s64 	%rd164, %rd5, %rd139;
	mov.u64 	%rd163, %rd2;
	mov.u32 	%r81, %r93;

$L__BB31_11:
	ld.global.nc.f32 	%f66, [%rd163];
	ld.global.f32 	%f67, [%rd165+-8];
	fma.rn.ftz.f32 	%f68, %f67, %f66, %f212;
	ld.global.f32 	%f69, [%rd164+-8];
	sub.ftz.f32 	%f70, %f69, %f1;
	add.ftz.f32 	%f71, %f211, %f70;
	ld.global.nc.f32 	%f72, [%rd163+4];
	ld.global.f32 	%f73, [%rd165+-4];
	fma.rn.ftz.f32 	%f74, %f73, %f72, %f68;
	ld.global.f32 	%f75, [%rd164+-4];
	sub.ftz.f32 	%f76, %f75, %f1;
	add.ftz.f32 	%f77, %f71, %f76;
	ld.global.nc.f32 	%f78, [%rd163+8];
	ld.global.f32 	%f79, [%rd165];
	fma.rn.ftz.f32 	%f80, %f79, %f78, %f74;
	ld.global.f32 	%f81, [%rd164];
	sub.ftz.f32 	%f82, %f81, %f1;
	add.ftz.f32 	%f83, %f77, %f82;
	ld.global.nc.f32 	%f84, [%rd163+12];
	ld.global.f32 	%f85, [%rd165+4];
	fma.rn.ftz.f32 	%f212, %f85, %f84, %f80;
	ld.global.f32 	%f86, [%rd164+4];
	sub.ftz.f32 	%f87, %f86, %f1;
	add.ftz.f32 	%f211, %f83, %f87;
	add.s32 	%r80, %r80, 4;
	add.s32 	%r81, %r81, 4;
	add.s64 	%rd165, %rd165, 16;
	add.s64 	%rd164, %rd164, 16;
	add.s64 	%rd163, %rd163, 16;
	add.s32 	%r79, %r79, -4;
	setp.ne.s32 	%p11, %r79, 0;
	@%p11 bra 	$L__BB31_11;

$L__BB31_12:
	setp.eq.s32 	%p12, %r82, 0;
	@%p12 bra 	$L__BB31_15;

	mul.wide.s32 	%rd140, %r80, 4;
	add.s64 	%rd168, %rd2, %rd140;
	mul.wide.s32 	%rd141, %r81, 4;
	add.s64 	%rd167, %rd5, %rd141;
	add.s64 	%rd166, %rd4, %rd141;

$L__BB31_14:
	.pragma "nounroll";
	ld.global.nc.f32 	%f88, [%rd168];
	ld.global.f32 	%f89, [%rd166];
	fma.rn.ftz.f32 	%f212, %f89, %f88, %f212;
	ld.global.f32 	%f90, [%rd167];
	sub.ftz.f32 	%f91, %f90, %f1;
	add.ftz.f32 	%f211, %f211, %f91;
	add.s64 	%rd168, %rd168, 4;
	add.s64 	%rd167, %rd167, 4;
	add.s64 	%rd166, %rd166, 4;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p13, %r82, 0;
	@%p13 bra 	$L__BB31_14;

$L__BB31_15:
	add.ftz.f32 	%f92, %f2, 0f33D6BF95;
	lg2.approx.ftz.f32 	%f93, %f92;
	mul.ftz.f32 	%f94, %f93, 0fBFC00000;
	ex2.approx.ftz.f32 	%f95, %f94;
	mul.ftz.f32 	%f96, %f95, 0fBF000000;
	mul.ftz.f32 	%f97, %f96, %f202;
	sqrt.approx.ftz.f32 	%f98, %f92;
	mul.ftz.f32 	%f99, %f97, 0fC0000000;
	mul.ftz.f32 	%f100, %f99, %f211;
	cvt.rn.f32.s32 	%f101, %r52;
	div.approx.ftz.f32 	%f102, %f100, %f101;
	mov.f32 	%f103, 0fBF800000;
	div.approx.ftz.f32 	%f104, %f103, %f98;
	fma.rn.ftz.f32 	%f105, %f104, %f212, %f102;
	div.approx.ftz.f32 	%f24, %f105, %f101;
	mov.f32 	%f106, 0f40000000;
	div.approx.ftz.f32 	%f107, %f106, %f101;
	mul.ftz.f32 	%f25, %f97, %f107;
	rsqrt.approx.ftz.f32 	%f26, %f92;
	@%p4 bra 	$L__BB31_28;

	add.s32 	%r28, %r52, -1;
	and.b32  	%r94, %r52, 3;
	setp.lt.u32 	%p15, %r28, 3;
	mov.u32 	%r86, 0;
	mov.u32 	%r87, %r93;
	@%p15 bra 	$L__BB31_19;

	sub.s32 	%r85, %r52, %r94;
	mul.wide.s32 	%rd142, %r93, 4;
	add.s64 	%rd143, %rd142, 8;
	add.s64 	%rd172, %rd4, %rd143;
	add.s64 	%rd171, %rd5, %rd143;
	add.s64 	%rd169, %rd6, %rd143;
	mov.u64 	%rd170, %rd2;
	mov.u32 	%r87, %r93;

$L__BB31_18:
	ld.global.nc.f32 	%f108, [%rd170];
	ld.global.f32 	%f109, [%rd172+-8];
	mul.ftz.f32 	%f110, %f109, %f108;
	ld.global.f32 	%f111, [%rd171+-8];
	sub.ftz.f32 	%f112, %f111, %f1;
	mul.ftz.f32 	%f113, %f25, %f112;
	fma.rn.ftz.f32 	%f114, %f26, %f110, %f113;
	add.ftz.f32 	%f115, %f24, %f114;
	st.global.f32 	[%rd169+-8], %f115;
	ld.global.nc.f32 	%f116, [%rd170+4];
	ld.global.f32 	%f117, [%rd172+-4];
	mul.ftz.f32 	%f118, %f117, %f116;
	ld.global.f32 	%f119, [%rd171+-4];
	sub.ftz.f32 	%f120, %f119, %f1;
	mul.ftz.f32 	%f121, %f25, %f120;
	fma.rn.ftz.f32 	%f122, %f26, %f118, %f121;
	add.ftz.f32 	%f123, %f24, %f122;
	st.global.f32 	[%rd169+-4], %f123;
	ld.global.nc.f32 	%f124, [%rd170+8];
	ld.global.f32 	%f125, [%rd172];
	mul.ftz.f32 	%f126, %f125, %f124;
	ld.global.f32 	%f127, [%rd171];
	sub.ftz.f32 	%f128, %f127, %f1;
	mul.ftz.f32 	%f129, %f25, %f128;
	fma.rn.ftz.f32 	%f130, %f26, %f126, %f129;
	add.ftz.f32 	%f131, %f24, %f130;
	st.global.f32 	[%rd169], %f131;
	ld.global.nc.f32 	%f132, [%rd170+12];
	ld.global.f32 	%f133, [%rd172+4];
	mul.ftz.f32 	%f134, %f133, %f132;
	ld.global.f32 	%f135, [%rd171+4];
	sub.ftz.f32 	%f136, %f135, %f1;
	mul.ftz.f32 	%f137, %f25, %f136;
	fma.rn.ftz.f32 	%f138, %f26, %f134, %f137;
	add.ftz.f32 	%f139, %f24, %f138;
	st.global.f32 	[%rd169+4], %f139;
	add.s32 	%r86, %r86, 4;
	add.s32 	%r87, %r87, 4;
	add.s64 	%rd172, %rd172, 16;
	add.s64 	%rd171, %rd171, 16;
	add.s64 	%rd170, %rd170, 16;
	add.s64 	%rd169, %rd169, 16;
	add.s32 	%r85, %r85, -4;
	setp.ne.s32 	%p16, %r85, 0;
	@%p16 bra 	$L__BB31_18;

$L__BB31_19:
	setp.eq.s32 	%p17, %r94, 0;
	@%p17 bra 	$L__BB31_22;

	mul.wide.s32 	%rd144, %r86, 4;
	add.s64 	%rd176, %rd2, %rd144;
	mul.wide.s32 	%rd145, %r87, 4;
	add.s64 	%rd175, %rd6, %rd145;
	add.s64 	%rd174, %rd5, %rd145;
	add.s64 	%rd173, %rd4, %rd145;
	mov.u32 	%r88, %r94;

$L__BB31_21:
	.pragma "nounroll";
	ld.global.nc.f32 	%f140, [%rd176];
	ld.global.f32 	%f141, [%rd173];
	mul.ftz.f32 	%f142, %f141, %f140;
	ld.global.f32 	%f143, [%rd174];
	sub.ftz.f32 	%f144, %f143, %f1;
	mul.ftz.f32 	%f145, %f25, %f144;
	fma.rn.ftz.f32 	%f146, %f26, %f142, %f145;
	add.ftz.f32 	%f147, %f24, %f146;
	st.global.f32 	[%rd175], %f147;
	add.s64 	%rd176, %rd176, 4;
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, 4;
	add.s64 	%rd173, %rd173, 4;
	add.s32 	%r88, %r88, -1;
	setp.ne.s32 	%p18, %r88, 0;
	@%p18 bra 	$L__BB31_21;

$L__BB31_22:
	mov.u32 	%r92, 0;
	@%p15 bra 	$L__BB31_25;

	sub.s32 	%r91, %r52, %r94;
	mul.wide.s32 	%rd146, %r93, 4;
	add.s64 	%rd147, %rd146, 8;
	add.s64 	%rd182, %rd4, %rd147;
	add.s64 	%rd181, %rd7, %rd147;
	cvta.to.global.u64 	%rd178, %rd99;
	mov.u64 	%rd177, %rd1;
	mov.u64 	%rd179, %rd3;
	mov.u64 	%rd180, %rd2;

$L__BB31_24:
	ld.global.f32 	%f148, [%rd182+-8];
	atom.global.add.f32 	%f149, [%rd179], %f148;
	ld.global.nc.f32 	%f150, [%rd178];
	ld.global.f32 	%f151, [%rd181+-8];
	sub.ftz.f32 	%f152, %f151, %f150;
	ld.global.nc.f32 	%f153, [%rd180];
	div.approx.ftz.f32 	%f154, %f152, %f153;
	ld.global.f32 	%f155, [%rd182+-8];
	mul.ftz.f32 	%f156, %f155, %f154;
	atom.global.add.f32 	%f157, [%rd177], %f156;
	ld.global.f32 	%f158, [%rd182+-4];
	add.s64 	%rd148, %rd179, 4;
	atom.global.add.f32 	%f159, [%rd148], %f158;
	ld.global.nc.f32 	%f160, [%rd178+4];
	ld.global.f32 	%f161, [%rd181+-4];
	sub.ftz.f32 	%f162, %f161, %f160;
	ld.global.nc.f32 	%f163, [%rd180+4];
	div.approx.ftz.f32 	%f164, %f162, %f163;
	ld.global.f32 	%f165, [%rd182+-4];
	mul.ftz.f32 	%f166, %f165, %f164;
	add.s64 	%rd149, %rd177, 4;
	atom.global.add.f32 	%f167, [%rd149], %f166;
	ld.global.f32 	%f168, [%rd182];
	add.s64 	%rd150, %rd179, 8;
	atom.global.add.f32 	%f169, [%rd150], %f168;
	ld.global.nc.f32 	%f170, [%rd178+8];
	ld.global.f32 	%f171, [%rd181];
	sub.ftz.f32 	%f172, %f171, %f170;
	ld.global.nc.f32 	%f173, [%rd180+8];
	div.approx.ftz.f32 	%f174, %f172, %f173;
	ld.global.f32 	%f175, [%rd182];
	mul.ftz.f32 	%f176, %f175, %f174;
	add.s64 	%rd151, %rd177, 8;
	atom.global.add.f32 	%f177, [%rd151], %f176;
	ld.global.f32 	%f178, [%rd182+4];
	add.s64 	%rd152, %rd179, 12;
	atom.global.add.f32 	%f179, [%rd152], %f178;
	ld.global.nc.f32 	%f180, [%rd178+12];
	ld.global.f32 	%f181, [%rd181+4];
	sub.ftz.f32 	%f182, %f181, %f180;
	ld.global.nc.f32 	%f183, [%rd180+12];
	div.approx.ftz.f32 	%f184, %f182, %f183;
	ld.global.f32 	%f185, [%rd182+4];
	mul.ftz.f32 	%f186, %f185, %f184;
	add.s64 	%rd153, %rd177, 12;
	atom.global.add.f32 	%f187, [%rd153], %f186;
	add.s32 	%r92, %r92, 4;
	add.s32 	%r93, %r93, 4;
	add.s64 	%rd182, %rd182, 16;
	add.s64 	%rd181, %rd181, 16;
	add.s64 	%rd180, %rd180, 16;
	add.s64 	%rd179, %rd179, 16;
	add.s64 	%rd178, %rd178, 16;
	add.s64 	%rd177, %rd177, 16;
	add.s32 	%r91, %r91, -4;
	setp.ne.s32 	%p20, %r91, 0;
	@%p20 bra 	$L__BB31_24;

$L__BB31_25:
	@%p17 bra 	$L__BB31_28;

	mul.wide.s32 	%rd154, %r92, 4;
	add.s64 	%rd188, %rd2, %rd154;
	cvta.to.global.u64 	%rd155, %rd99;
	add.s64 	%rd187, %rd155, %rd154;
	add.s64 	%rd186, %rd1, %rd154;
	add.s64 	%rd185, %rd3, %rd154;
	mul.wide.s32 	%rd156, %r93, 4;
	add.s64 	%rd184, %rd7, %rd156;
	add.s64 	%rd183, %rd4, %rd156;

$L__BB31_27:
	.pragma "nounroll";
	ld.global.f32 	%f188, [%rd183];
	atom.global.add.f32 	%f189, [%rd185], %f188;
	ld.global.nc.f32 	%f190, [%rd187];
	ld.global.f32 	%f191, [%rd184];
	sub.ftz.f32 	%f192, %f191, %f190;
	ld.global.nc.f32 	%f193, [%rd188];
	div.approx.ftz.f32 	%f194, %f192, %f193;
	ld.global.f32 	%f195, [%rd183];
	mul.ftz.f32 	%f196, %f195, %f194;
	atom.global.add.f32 	%f197, [%rd186], %f196;
	add.s64 	%rd188, %rd188, 4;
	add.s64 	%rd187, %rd187, 4;
	add.s64 	%rd186, %rd186, 4;
	add.s64 	%rd185, %rd185, 4;
	add.s64 	%rd184, %rd184, 4;
	add.s64 	%rd183, %rd183, 4;
	add.s32 	%r94, %r94, -1;
	setp.ne.s32 	%p22, %r94, 0;
	@%p22 bra 	$L__BB31_27;

$L__BB31_28:
	ret;

}
	// .globl	dropout
.visible .entry dropout(
	.param .u64 dropout_param_0,
	.param .u64 dropout_param_1,
	.param .align 2 .b8 dropout_param_2[2],
	.param .u32 dropout_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<30>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs6, [dropout_param_2];
	ld.param.u64 	%rd2, [dropout_param_0];
	ld.param.u64 	%rd3, [dropout_param_1];
	ld.param.u32 	%r2, [dropout_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB32_6;

	ld.const.u16 	%rs8, [sh+8];
	// begin inline asm
	{sub.f16 %rs7,%rs8,%rs6;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs8;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs7;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs13,%rs29;
}
	// end inline asm
	mov.u16 	%rs17, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs13, %rs17;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs15, 0;
	@%p2 bra 	$L__BB32_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs13;
  selp.u16 %rs19, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs19, 0;
	@%p3 bra 	$L__BB32_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f11;}

	// end inline asm

$L__BB32_4:
	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs24, [%rd6];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs24, %rs6;
  selp.u16 %rs23, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs23, 0;
	@%p4 bra 	$L__BB32_6;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u16 	%rs27, [%rd9];
	// begin inline asm
	{mul.f16 %rs26,%rs27,%rs29;
}
	// end inline asm
	st.global.u16 	[%rd9], %rs26;

$L__BB32_6:
	ret;

}
	// .globl	sub_gpu_half2
.visible .entry sub_gpu_half2(
	.param .u64 sub_gpu_half2_param_0,
	.param .u64 sub_gpu_half2_param_1,
	.param .u64 sub_gpu_half2_param_2,
	.param .u32 sub_gpu_half2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_half2_param_0];
	ld.param.u64 	%rd2, [sub_gpu_half2_param_1];
	ld.param.u64 	%rd3, [sub_gpu_half2_param_2];
	ld.param.u32 	%r2, [sub_gpu_half2_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB33_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	add.s64 	%rd8, %rd4, %rd6;
	ld.global.nc.u32 	%r7, [%rd8];
	cvta.to.global.u64 	%rd9, %rd2;
	add.s64 	%rd10, %rd9, %rd6;
	ld.global.nc.u32 	%r8, [%rd10];
	// begin inline asm
	{sub.f16x2 %r6,%r7,%r8;
}
	// end inline asm
	st.global.u32 	[%rd7], %r6;

$L__BB33_2:
	ret;

}
	// .globl	sub_gpu
.visible .entry sub_gpu(
	.param .u64 sub_gpu_param_0,
	.param .u64 sub_gpu_param_1,
	.param .u64 sub_gpu_param_2,
	.param .u32 sub_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_param_0];
	ld.param.u64 	%rd2, [sub_gpu_param_1];
	ld.param.u64 	%rd3, [sub_gpu_param_2];
	ld.param.u32 	%r2, [sub_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB34_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.nc.f32 	%f1, [%rd8];
	ld.global.nc.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

$L__BB34_2:
	ret;

}
	// .globl	sub_gpu_half
.visible .entry sub_gpu_half(
	.param .u64 sub_gpu_half_param_0,
	.param .u64 sub_gpu_half_param_1,
	.param .u64 sub_gpu_half_param_2,
	.param .u32 sub_gpu_half_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_half_param_0];
	ld.param.u64 	%rd2, [sub_gpu_half_param_1];
	ld.param.u64 	%rd3, [sub_gpu_half_param_2];
	ld.param.u32 	%r2, [sub_gpu_half_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB35_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.nc.u16 	%rs3, [%rd8];
	// begin inline asm
	{sub.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.u16 	[%rd10], %rs1;

$L__BB35_2:
	ret;

}
	// .globl	sub_half_float_gpu
.visible .entry sub_half_float_gpu(
	.param .u64 sub_half_float_gpu_param_0,
	.param .u64 sub_half_float_gpu_param_1,
	.param .u64 sub_half_float_gpu_param_2,
	.param .u32 sub_half_float_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [sub_half_float_gpu_param_0];
	ld.param.u64 	%rd2, [sub_half_float_gpu_param_1];
	ld.param.u64 	%rd3, [sub_half_float_gpu_param_2];
	ld.param.u32 	%r2, [sub_half_float_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB36_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.f32 	%f2, [%rd9];
	sub.ftz.f32 	%f3, %f1, %f2;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd8;
	st.global.f32 	[%rd11], %f3;

$L__BB36_2:
	ret;

}
	// .globl	sub_float_half_gpu
.visible .entry sub_float_half_gpu(
	.param .u64 sub_float_half_gpu_param_0,
	.param .u64 sub_float_half_gpu_param_1,
	.param .u64 sub_float_half_gpu_param_2,
	.param .u32 sub_float_half_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [sub_float_half_gpu_param_0];
	ld.param.u64 	%rd2, [sub_float_half_gpu_param_1];
	ld.param.u64 	%rd3, [sub_float_half_gpu_param_2];
	ld.param.u32 	%r2, [sub_float_half_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB37_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.u16 	%rs1, [%rd9];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	sub.ftz.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f32 	[%rd11], %f3;

$L__BB37_2:
	ret;

}
	// .globl	mul
.visible .entry mul(
	.param .u64 mul_param_0,
	.param .f32 mul_param_1,
	.param .u32 mul_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [mul_param_0];
	ld.param.f32 	%f1, [mul_param_1];
	ld.param.u32 	%r2, [mul_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB38_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	mul.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB38_2:
	ret;

}
	// .globl	mul_float
.visible .entry mul_float(
	.param .u64 mul_float_param_0,
	.param .f32 mul_float_param_1,
	.param .u32 mul_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [mul_float_param_0];
	ld.param.f32 	%f1, [mul_float_param_1];
	ld.param.u32 	%r2, [mul_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB39_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	mul.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB39_2:
	ret;

}
	// .globl	clip
.visible .entry clip(
	.param .u64 clip_param_0,
	.param .f32 clip_param_1,
	.param .f32 clip_param_2,
	.param .u32 clip_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [clip_param_0];
	ld.param.f32 	%f2, [clip_param_1];
	ld.param.f32 	%f3, [clip_param_2];
	ld.param.u32 	%r2, [clip_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB40_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd1];
	setp.gt.ftz.f32 	%p2, %f1, %f3;
	@%p2 bra 	$L__BB40_4;
	bra.uni 	$L__BB40_2;

$L__BB40_4:
	st.global.f32 	[%rd1], %f3;
	bra.uni 	$L__BB40_5;

$L__BB40_2:
	setp.geu.ftz.f32 	%p3, %f1, %f2;
	@%p3 bra 	$L__BB40_5;

	st.global.f32 	[%rd1], %f2;

$L__BB40_5:
	ret;

}
	// .globl	clip_half
.visible .entry clip_half(
	.param .u64 clip_half_param_0,
	.param .align 2 .b8 clip_half_param_1[2],
	.param .align 2 .b8 clip_half_param_2[2],
	.param .u32 clip_half_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs5, [clip_half_param_2];
	ld.param.u16 	%rs4, [clip_half_param_1];
	ld.param.u64 	%rd2, [clip_half_param_0];
	ld.param.u32 	%r2, [clip_half_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB41_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd1];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs2, %rs5;
  selp.u16 %rs6, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs6, 0;
	@%p2 bra 	$L__BB41_3;

	st.global.u16 	[%rd1], %rs5;
	bra.uni 	$L__BB41_5;

$L__BB41_3:
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs2, %rs4;
  selp.u16 %rs9, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs9, 0;
	@%p3 bra 	$L__BB41_5;

	st.global.u16 	[%rd1], %rs4;

$L__BB41_5:
	ret;

}
	// .globl	pow2
.visible .entry pow2(
	.param .u64 pow2_param_0,
	.param .u32 pow2_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [pow2_param_0];
	ld.param.u32 	%r2, [pow2_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB42_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	mul.ftz.f32 	%f2, %f1, %f1;
	st.global.f32 	[%rd4], %f2;

$L__BB42_2:
	ret;

}
	// .globl	pow2_half
.visible .entry pow2_half(
	.param .u64 pow2_half_param_0,
	.param .u32 pow2_half_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [pow2_half_param_0];
	ld.param.u32 	%r2, [pow2_half_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB43_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u16 	%rs2, [%rd4];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	st.global.u16 	[%rd4], %rs1;

$L__BB43_2:
	ret;

}
	// .globl	subAbs_half
.visible .entry subAbs_half(
	.param .u64 subAbs_half_param_0,
	.param .u64 subAbs_half_param_1,
	.param .u64 subAbs_half_param_2,
	.param .u32 subAbs_half_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subAbs_half_param_0];
	ld.param.u64 	%rd2, [subAbs_half_param_1];
	ld.param.u64 	%rd3, [subAbs_half_param_2];
	ld.param.u32 	%r2, [subAbs_half_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB44_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.u16 	%rs3, [%rd8];
	// begin inline asm
	{sub.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	// begin inline asm
	{abs.f16 %rs4,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.u16 	[%rd10], %rs4;

$L__BB44_2:
	ret;

}
	// .globl	subAbs
.visible .entry subAbs(
	.param .u64 subAbs_param_0,
	.param .u64 subAbs_param_1,
	.param .u64 subAbs_param_2,
	.param .u32 subAbs_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subAbs_param_0];
	ld.param.u64 	%rd2, [subAbs_param_1];
	ld.param.u64 	%rd3, [subAbs_param_2];
	ld.param.u32 	%r2, [subAbs_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB45_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	abs.ftz.f32 	%f4, %f3;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f4;

$L__BB45_2:
	ret;

}
	// .globl	sum
.visible .entry sum(
	.param .u64 sum_param_0,
	.param .u64 sum_param_1,
	.param .u32 sum_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [sum_param_0];
	ld.param.u64 	%rd2, [sum_param_1];
	ld.param.u32 	%r2, [sum_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB46_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	atom.global.add.f32 	%f2, [%rd6], %f1;

$L__BB46_2:
	ret;

}
	// .globl	sum_half
.visible .entry sum_half(
	.param .u64 sum_half_param_0,
	.param .u64 sum_half_param_1,
	.param .u32 sum_half_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [sum_half_param_0];
	ld.param.u64 	%rd2, [sum_half_param_1];
	ld.param.u32 	%r2, [sum_half_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB47_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs2, [%rd6];
	// begin inline asm
	{ atom.add.noftz.f16 %rs1,[%rd2],%rs2; }

	// end inline asm

$L__BB47_2:
	ret;

}
	// .globl	derAbs
.visible .entry derAbs(
	.param .u64 derAbs_param_0,
	.param .u64 derAbs_param_1,
	.param .u64 derAbs_param_2,
	.param .u32 derAbs_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [derAbs_param_0];
	ld.param.u64 	%rd2, [derAbs_param_1];
	ld.param.u64 	%rd3, [derAbs_param_2];
	ld.param.u32 	%r2, [derAbs_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB48_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	abs.ftz.f32 	%f4, %f3;
	div.approx.ftz.f32 	%f5, %f3, %f4;
	add.ftz.f32 	%f6, %f5, 0f33D6BF95;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f6;

$L__BB48_2:
	ret;

}
	// .globl	fisnan
.visible .entry fisnan(
	.param .u64 fisnan_param_0,
	.param .u64 fisnan_param_1,
	.param .u32 fisnan_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [fisnan_param_0];
	ld.param.u64 	%rd3, [fisnan_param_1];
	ld.param.u32 	%r2, [fisnan_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB49_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB49_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs2, [%rd6];
	// begin inline asm
	{set.nan.f16.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	setp.eq.s16 	%p3, %rs1, 0;
	@%p3 bra 	$L__BB49_4;

	st.global.u32 	[%rd1], %r1;

$L__BB49_4:
	ret;

}
	// .globl	fisnan_float
.visible .entry fisnan_float(
	.param .u64 fisnan_float_param_0,
	.param .u64 fisnan_float_param_1,
	.param .u32 fisnan_float_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [fisnan_float_param_0];
	ld.param.u64 	%rd3, [fisnan_float_param_1];
	ld.param.u32 	%r2, [fisnan_float_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB50_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB50_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	cvt.ftz.f64.f32 	%fd1, %f1;
	abs.f64 	%fd2, %fd1;
	setp.le.f64 	%p3, %fd2, 0d7FF0000000000000;
	@%p3 bra 	$L__BB50_4;

	st.global.u32 	[%rd1], %r1;

$L__BB50_4:
	ret;

}
	// .globl	hisinf
.visible .entry hisinf(
	.param .u64 hisinf_param_0,
	.param .u64 hisinf_param_1,
	.param .u32 hisinf_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [hisinf_param_0];
	ld.param.u64 	%rd3, [hisinf_param_1];
	ld.param.u32 	%r2, [hisinf_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB51_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB51_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	or.b16  	%rs2, %rs1, -32768;
	setp.ne.s16 	%p3, %rs2, -1024;
	setp.ne.s16 	%p4, %rs1, -1024;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB51_4;

	st.global.u32 	[%rd1], %r1;

$L__BB51_4:
	ret;

}
	// .globl	hisinf_float
.visible .entry hisinf_float(
	.param .u64 hisinf_float_param_0,
	.param .u64 hisinf_float_param_1,
	.param .u32 hisinf_float_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<10>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [hisinf_float_param_0];
	ld.param.u64 	%rd3, [hisinf_float_param_1];
	ld.param.u32 	%r2, [hisinf_float_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB52_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB52_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	cvt.ftz.f64.f32 	%fd1, %f1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd1;
	}
	and.b32  	%r9, %r8, 2147483647;
	setp.ne.s32 	%p3, %r9, 2146435072;
	setp.ne.s32 	%p4, %r7, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB52_4;

	st.global.u32 	[%rd1], %r1;

$L__BB52_4:
	ret;

}
	// .globl	momentum
.visible .entry momentum(
	.param .u64 momentum_param_0,
	.param .u64 momentum_param_1,
	.param .align 2 .b8 momentum_param_2[2],
	.param .u32 momentum_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs1, [momentum_param_2];
	ld.param.u64 	%rd1, [momentum_param_0];
	ld.param.u64 	%rd2, [momentum_param_1];
	ld.param.u32 	%r2, [momentum_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB53_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// begin inline asm
	{mul.f16 %rs2,%rs1,%rs4;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+8];
	// begin inline asm
	{sub.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u16 	%rs9, [%rd7];
	// begin inline asm
	{mul.f16 %rs8,%rs9,%rs5;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs11,%rs2,%rs8;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs11;

$L__BB53_2:
	ret;

}
	// .globl	momentum_float
.visible .entry momentum_float(
	.param .u64 momentum_float_param_0,
	.param .u64 momentum_float_param_1,
	.param .f32 momentum_float_param_2,
	.param .u32 momentum_float_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [momentum_float_param_0];
	ld.param.u64 	%rd2, [momentum_float_param_1];
	ld.param.f32 	%f1, [momentum_float_param_2];
	ld.param.u32 	%r2, [momentum_float_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB54_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	mov.f32 	%f3, 0f3F800000;
	sub.ftz.f32 	%f4, %f3, %f1;
	ld.global.nc.f32 	%f5, [%rd7];
	mul.ftz.f32 	%f6, %f4, %f5;
	fma.rn.ftz.f32 	%f7, %f2, %f1, %f6;
	st.global.f32 	[%rd5], %f7;

$L__BB54_2:
	ret;

}
	// .globl	momentumPow2
.visible .entry momentumPow2(
	.param .u64 momentumPow2_param_0,
	.param .u64 momentumPow2_param_1,
	.param .align 2 .b8 momentumPow2_param_2[2],
	.param .u32 momentumPow2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs1, [momentumPow2_param_2];
	ld.param.u64 	%rd1, [momentumPow2_param_0];
	ld.param.u64 	%rd2, [momentumPow2_param_1];
	ld.param.u32 	%r2, [momentumPow2_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB55_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// begin inline asm
	{mul.f16 %rs2,%rs1,%rs4;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+8];
	// begin inline asm
	{sub.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u16 	%rs10, [%rd7];
	// begin inline asm
	{mul.f16 %rs8,%rs5,%rs10;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs11,%rs8,%rs10;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs2,%rs11;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs14;

$L__BB55_2:
	ret;

}
	// .globl	momentumPow2_float
.visible .entry momentumPow2_float(
	.param .u64 momentumPow2_float_param_0,
	.param .u64 momentumPow2_float_param_1,
	.param .f32 momentumPow2_float_param_2,
	.param .u32 momentumPow2_float_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [momentumPow2_float_param_0];
	ld.param.u64 	%rd2, [momentumPow2_float_param_1];
	ld.param.f32 	%f1, [momentumPow2_float_param_2];
	ld.param.u32 	%r2, [momentumPow2_float_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB56_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f3, [%rd7];
	mov.f32 	%f4, 0f3F800000;
	sub.ftz.f32 	%f5, %f4, %f1;
	mul.ftz.f32 	%f6, %f5, %f3;
	mul.ftz.f32 	%f7, %f3, %f6;
	fma.rn.ftz.f32 	%f8, %f2, %f1, %f7;
	st.global.f32 	[%rd5], %f8;

$L__BB56_2:
	ret;

}
	// .globl	subDivSqrtNorm_half
.visible .entry subDivSqrtNorm_half(
	.param .u64 subDivSqrtNorm_half_param_0,
	.param .u64 subDivSqrtNorm_half_param_1,
	.param .align 2 .b8 subDivSqrtNorm_half_param_2[2],
	.param .align 2 .b8 subDivSqrtNorm_half_param_3[2],
	.param .align 2 .b8 subDivSqrtNorm_half_param_4[2],
	.param .u64 subDivSqrtNorm_half_param_5,
	.param .u32 subDivSqrtNorm_half_param_6
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<75>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u16 	%rs17, [subDivSqrtNorm_half_param_4];
	ld.param.u16 	%rs16, [subDivSqrtNorm_half_param_3];
	ld.param.u16 	%rs15, [subDivSqrtNorm_half_param_2];
	ld.param.u64 	%rd2, [subDivSqrtNorm_half_param_0];
	ld.param.u64 	%rd3, [subDivSqrtNorm_half_param_1];
	ld.param.u64 	%rd4, [subDivSqrtNorm_half_param_5];
	ld.param.u32 	%r2, [subDivSqrtNorm_half_param_6];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB57_11;

	ld.const.u16 	%rs1, [sh+10];
	// begin inline asm
	{add.f16 %rs18,%rs16,%rs1;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs15;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs18;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f15, %f14;
}
	// end inline asm
	mul.ftz.f32 	%f17, %f13, %f15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f17;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs24,%rs72;
}
	// end inline asm
	mov.u16 	%rs28, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs24, %rs28;
  selp.u16 %rs26, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs26, 0;
	@%p2 bra 	$L__BB57_4;

	mov.f32 	%f18, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f18;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs29, %rs24;
  selp.u16 %rs30, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs30, 0;
	@%p3 bra 	$L__BB57_4;

	neg.ftz.f32 	%f20, %f14;
	fma.rn.ftz.f32 	%f21, %f20, %f17, %f13;
	fma.rn.ftz.f32 	%f19, %f15, %f21, %f17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f19;}

	// end inline asm

$L__BB57_4:
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs36, [%rd7];
	// begin inline asm
	{mul.f16 %rs34,%rs72,%rs36;
}
	// end inline asm
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd5;
	ld.global.nc.u16 	%rs37, [%rd9];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs17;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f24, %f23;
}
	// end inline asm
	mul.ftz.f32 	%f26, %f22, %f24;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f26;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs40,%rs73;
}
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs40, %rs28;
  selp.u16 %rs42, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs42, 0;
	@%p4 bra 	$L__BB57_7;

	mov.f32 	%f27, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f27;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs45, %rs40;
  selp.u16 %rs46, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p5, %rs46, 0;
	@%p5 bra 	$L__BB57_7;

	neg.ftz.f32 	%f29, %f23;
	fma.rn.ftz.f32 	%f30, %f29, %f26, %f22;
	fma.rn.ftz.f32 	%f28, %f24, %f30, %f26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f28;}

	// end inline asm

$L__BB57_7:
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs73;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs50,r;     
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs52,%rs50,%rs1;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs52;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f33, %f32;
}
	// end inline asm
	mul.ftz.f32 	%f35, %f31, %f33;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f35;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs58,%rs74;
}
	// end inline asm
	mov.u16 	%rs62, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs58, %rs62;
  selp.u16 %rs60, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p6, %rs60, 0;
	@%p6 bra 	$L__BB57_10;

	mov.f32 	%f36, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs63, %f36;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs63, %rs58;
  selp.u16 %rs64, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p7, %rs64, 0;
	@%p7 bra 	$L__BB57_10;

	neg.ftz.f32 	%f38, %f32;
	fma.rn.ftz.f32 	%f39, %f38, %f35, %f31;
	fma.rn.ftz.f32 	%f37, %f33, %f39, %f35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f37;}

	// end inline asm

$L__BB57_10:
	// begin inline asm
	{neg.f16 %rs68,%rs74;
}
	// end inline asm
	// begin inline asm
	{ atom.add.noftz.f16 %rs70,[%rd1],%rs68; }

	// end inline asm

$L__BB57_11:
	ret;

}
	// .globl	subDivSqrtNorm
.visible .entry subDivSqrtNorm(
	.param .u64 subDivSqrtNorm_param_0,
	.param .u64 subDivSqrtNorm_param_1,
	.param .f32 subDivSqrtNorm_param_2,
	.param .f32 subDivSqrtNorm_param_3,
	.param .f32 subDivSqrtNorm_param_4,
	.param .u64 subDivSqrtNorm_param_5,
	.param .u32 subDivSqrtNorm_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subDivSqrtNorm_param_0];
	ld.param.u64 	%rd2, [subDivSqrtNorm_param_1];
	ld.param.f32 	%f1, [subDivSqrtNorm_param_2];
	ld.param.f32 	%f2, [subDivSqrtNorm_param_3];
	ld.param.f32 	%f3, [subDivSqrtNorm_param_4];
	ld.param.u64 	%rd3, [subDivSqrtNorm_param_5];
	ld.param.u32 	%r2, [subDivSqrtNorm_param_6];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB58_2;

	cvta.to.global.u64 	%rd4, %rd1;
	add.ftz.f32 	%f4, %f2, 0f33D6BF95;
	div.approx.ftz.f32 	%f5, %f1, %f4;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	add.s64 	%rd8, %rd4, %rd6;
	ld.global.nc.f32 	%f6, [%rd8];
	mul.ftz.f32 	%f7, %f5, %f6;
	cvta.to.global.u64 	%rd9, %rd2;
	add.s64 	%rd10, %rd9, %rd6;
	ld.global.nc.f32 	%f8, [%rd10];
	div.approx.ftz.f32 	%f9, %f8, %f3;
	sqrt.approx.ftz.f32 	%f10, %f9;
	add.ftz.f32 	%f11, %f10, 0f358637BD;
	div.approx.ftz.f32 	%f12, %f7, %f11;
	neg.ftz.f32 	%f13, %f12;
	atom.global.add.f32 	%f14, [%rd7], %f13;

$L__BB58_2:
	ret;

}
	// .globl	addBackCopy
.visible .entry addBackCopy(
	.param .u64 addBackCopy_param_0,
	.param .u32 addBackCopy_param_1,
	.param .u32 addBackCopy_param_2,
	.param .u32 addBackCopy_param_3,
	.param .u32 addBackCopy_param_4,
	.param .u64 addBackCopy_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addBackCopy_param_0];
	ld.param.u32 	%r4, [addBackCopy_param_1];
	ld.param.u32 	%r7, [addBackCopy_param_2];
	ld.param.u32 	%r5, [addBackCopy_param_3];
	ld.param.u32 	%r6, [addBackCopy_param_4];
	ld.param.u64 	%rd2, [addBackCopy_param_5];
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	mov.u32 	%r14, %nctaid.y;
	mul.lo.s32 	%r15, %r14, %r11;
	mad.lo.s32 	%r3, %r15, %r1, %r2;
	mul.lo.s32 	%r16, %r5, %r7;
	setp.ge.s32 	%p1, %r3, %r16;
	@%p1 bra 	$L__BB59_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r17, %r1, %r4, %r2;
	mad.lo.s32 	%r18, %r6, %r5, %r17;
	mul.wide.s32 	%rd4, %r18, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB59_2:
	ret;

}
	// .globl	addBackCopy_half
.visible .entry addBackCopy_half(
	.param .u64 addBackCopy_half_param_0,
	.param .u32 addBackCopy_half_param_1,
	.param .u32 addBackCopy_half_param_2,
	.param .u32 addBackCopy_half_param_3,
	.param .u32 addBackCopy_half_param_4,
	.param .u64 addBackCopy_half_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addBackCopy_half_param_0];
	ld.param.u32 	%r4, [addBackCopy_half_param_1];
	ld.param.u32 	%r7, [addBackCopy_half_param_2];
	ld.param.u32 	%r5, [addBackCopy_half_param_3];
	ld.param.u32 	%r6, [addBackCopy_half_param_4];
	ld.param.u64 	%rd2, [addBackCopy_half_param_5];
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r8, %r9, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	mov.u32 	%r14, %nctaid.y;
	mul.lo.s32 	%r15, %r14, %r11;
	mad.lo.s32 	%r3, %r15, %r1, %r2;
	mul.lo.s32 	%r16, %r5, %r7;
	setp.ge.s32 	%p1, %r3, %r16;
	@%p1 bra 	$L__BB60_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r17, %r1, %r4, %r2;
	mad.lo.s32 	%r18, %r6, %r5, %r17;
	mul.wide.s32 	%rd4, %r18, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r3, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB60_2:
	ret;

}
	// .globl	dropoutBack
.visible .entry dropoutBack(
	.param .u64 dropoutBack_param_0,
	.param .u64 dropoutBack_param_1,
	.param .align 2 .b8 dropoutBack_param_2[2],
	.param .u64 dropoutBack_param_3,
	.param .u32 dropoutBack_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<13>;


	ld.param.u16 	%rs1, [dropoutBack_param_2];
	ld.param.u64 	%rd2, [dropoutBack_param_0];
	ld.param.u64 	%rd3, [dropoutBack_param_1];
	ld.param.u64 	%rd4, [dropoutBack_param_3];
	ld.param.u32 	%r2, [dropoutBack_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB61_3;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs3, [%rd7];
	ld.const.u16 	%rs4, [sh];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.neu.f16  __$temp3, %rs3, %rs4;
  selp.u16 %rs2, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs2, 0;
	@%p2 bra 	$L__BB61_3;

	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs6, [%rd10];
	// begin inline asm
	{mul.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.u16 	[%rd12], %rs5;

$L__BB61_3:
	ret;

}
	// .globl	mask
.visible .entry mask(
	.param .u64 mask_param_0,
	.param .f32 mask_param_1,
	.param .f32 mask_param_2,
	.param .u64 mask_param_3,
	.param .u32 mask_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [mask_param_0];
	ld.param.f32 	%f1, [mask_param_1];
	ld.param.f32 	%f2, [mask_param_2];
	ld.param.u64 	%rd3, [mask_param_3];
	ld.param.u32 	%r2, [mask_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB62_3;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f3, [%rd6];
	setp.neu.ftz.f32 	%p2, %f3, %f1;
	@%p2 bra 	$L__BB62_3;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f2;

$L__BB62_3:
	ret;

}
	// .globl	mask_half
.visible .entry mask_half(
	.param .u64 mask_half_param_0,
	.param .align 2 .b8 mask_half_param_1[2],
	.param .align 2 .b8 mask_half_param_2[2],
	.param .u64 mask_half_param_3,
	.param .u32 mask_half_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs2, [mask_half_param_2];
	ld.param.u16 	%rs1, [mask_half_param_1];
	ld.param.u64 	%rd2, [mask_half_param_0];
	ld.param.u64 	%rd3, [mask_half_param_3];
	ld.param.u32 	%r2, [mask_half_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB63_3;

	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs4, [%rd6];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.eq.f16  __$temp3, %rs4, %rs1;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs3, 0;
	@%p2 bra 	$L__BB63_3;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs2;

$L__BB63_3:
	ret;

}
	// .globl	fillUnderDiagonal
.visible .entry fillUnderDiagonal(
	.param .u32 fillUnderDiagonal_param_0,
	.param .align 2 .b8 fillUnderDiagonal_param_1[2],
	.param .u64 fillUnderDiagonal_param_2,
	.param .u32 fillUnderDiagonal_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<14>;


	ld.param.u16 	%rs2, [fillUnderDiagonal_param_1];
	ld.param.u32 	%r11, [fillUnderDiagonal_param_0];
	ld.param.u64 	%rd8, [fillUnderDiagonal_param_2];
	ld.param.u32 	%r12, [fillUnderDiagonal_param_3];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	setp.lt.s32 	%p2, %r1, 0;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB64_7;

	add.s32 	%r17, %r1, 1;
	and.b32  	%r24, %r17, 3;
	setp.lt.u32 	%p4, %r1, 3;
	mov.u32 	%r23, 0;
	@%p4 bra 	$L__BB64_4;

	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd9, %r19, 2;
	add.s64 	%rd10, %rd1, %rd9;
	add.s64 	%rd12, %rd10, 4;
	sub.s32 	%r21, %r1, %r24;

$L__BB64_3:
	st.global.u16 	[%rd12+-4], %rs2;
	st.global.u16 	[%rd12+-2], %rs2;
	st.global.u16 	[%rd12], %rs2;
	st.global.u16 	[%rd12+2], %rs2;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd12, %rd12, 8;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p5, %r21, -1;
	@%p5 bra 	$L__BB64_3;

$L__BB64_4:
	setp.eq.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB64_7;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd11, %r20, 2;
	add.s64 	%rd13, %rd1, %rd11;

$L__BB64_6:
	.pragma "nounroll";
	st.global.u16 	[%rd13], %rs2;
	add.s64 	%rd13, %rd13, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p7, %r24, 0;
	@%p7 bra 	$L__BB64_6;

$L__BB64_7:
	ret;

}
	// .globl	derGelu
.visible .entry derGelu(
	.param .u64 derGelu_param_0,
	.param .u64 derGelu_param_1,
	.param .u64 derGelu_param_2,
	.param .u32 derGelu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [derGelu_param_0];
	ld.param.u64 	%rd2, [derGelu_param_1];
	ld.param.u64 	%rd3, [derGelu_param_2];
	ld.param.u32 	%r2, [derGelu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB65_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.ftz.f32 	%f2, %f1, 0f3D122277;
	mul.ftz.f32 	%f3, %f1, %f2;
	mul.ftz.f32 	%f4, %f1, %f3;
	fma.rn.ftz.f32 	%f5, %f1, 0f3F4C422A, %f4;
	sin.approx.ftz.f32 	%f6, %f5;
	cos.approx.ftz.f32 	%f7, %f5;
	div.approx.ftz.f32 	%f8, %f6, %f7;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.nc.f32 	%f9, [%rd8];
	mul.ftz.f32 	%f10, %f9, 0f3F000000;
	add.ftz.f32 	%f11, %f8, 0f3F800000;
	mov.f32 	%f12, 0f3F800000;
	mul.ftz.f32 	%f13, %f8, %f8;
	sub.ftz.f32 	%f14, %f12, %f13;
	mul.ftz.f32 	%f15, %f1, %f14;
	mul.ftz.f32 	%f16, %f1, 0f3DDB33B3;
	fma.rn.ftz.f32 	%f17, %f1, %f16, 0f3F4C426B;
	fma.rn.ftz.f32 	%f18, %f17, %f15, %f11;
	mul.ftz.f32 	%f19, %f10, %f18;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f19;

$L__BB65_2:
	ret;

}
	// .globl	derGelu_half
.visible .entry derGelu_half(
	.param .u64 derGelu_half_param_0,
	.param .u64 derGelu_half_param_1,
	.param .u64 derGelu_half_param_2,
	.param .u32 derGelu_half_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<52>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [derGelu_half_param_0];
	ld.param.u64 	%rd3, [derGelu_half_param_1];
	ld.param.u64 	%rd4, [derGelu_half_param_2];
	ld.param.u32 	%r2, [derGelu_half_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB66_5;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs1, [%rd7];
	ld.const.u16 	%rs3, [sh+2];
	// begin inline asm
	{mul.f16 %rs2,%rs3,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+4];
	// begin inline asm
	{mul.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs8,%rs5,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs11,%rs8,%rs1;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs2,%rs11;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs14;}

	// end inline asm
	abs.ftz.f32 	%f2, %f6;
	setp.ltu.ftz.f32 	%p2, %f2, 0f3F19999A;
	@%p2 bra 	$L__BB66_3;
	bra.uni 	$L__BB66_2;

$L__BB66_3:
	mul.ftz.f32 	%f15, %f6, %f6;
	mov.f32 	%f16, 0fBD563CAE;
	mov.f32 	%f17, 0f3C80F082;
	fma.rn.ftz.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0f3E085941;
	fma.rn.ftz.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f00000000;
	fma.rn.ftz.f32 	%f24, %f22, %f15, %f23;
	fma.rn.ftz.f32 	%f26, %f24, %f6, %f6;
	bra.uni 	$L__BB66_4;

$L__BB66_2:
	mul.ftz.f32 	%f7, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f8, %f7;
	add.ftz.f32 	%f9, %f8, 0f3F800000;
	mov.f32 	%f10, 0f3F800000;
	rcp.approx.ftz.f32 	%f11, %f9;
	mov.f32 	%f12, 0fC0000000;
	fma.rn.ftz.f32 	%f13, %f11, %f12, %f10;
	setp.ge.ftz.f32 	%p3, %f2, 0f41102CB4;
	selp.f32 	%f14, 0f3F800000, %f13, %p3;
	mov.b32 	%r6, %f14;
	mov.b32 	%r7, %f6;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f26, %r9;

$L__BB66_4:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f26;}

	// end inline asm
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs20, [%rd10];
	ld.const.u16 	%rs21, [sh+6];
	// begin inline asm
	{mul.f16 %rs19,%rs20,%rs21;
}
	// end inline asm
	ld.const.u16 	%rs23, [sh+8];
	// begin inline asm
	{add.f16 %rs22,%rs23,%rs18;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs25,%rs18,%rs18;
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs28,%rs23,%rs25;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs31,%rs1,%rs28;
}
	// end inline asm
	ld.const.u16 	%rs35, [sh+22];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs37,%rs34,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs41, [sh+20];
	// begin inline asm
	{add.f16 %rs40,%rs41,%rs37;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs43,%rs31,%rs40;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs46,%rs22,%rs43;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs49,%rs19,%rs46;
}
	// end inline asm
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.u16 	[%rd12], %rs49;

$L__BB66_5:
	ret;

}
	// .globl	Softmax
.visible .entry Softmax(
	.param .u64 Softmax_param_0,
	.param .u64 Softmax_param_1,
	.param .u32 Softmax_param_2,
	.param .u32 Softmax_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<10>;
	// demoted variable
	.shared .align 4 .f32 _ZZ7SoftmaxE3max;
	// demoted variable
	.shared .align 4 .f32 _ZZ7SoftmaxE3sum;

	ld.param.u64 	%rd3, [Softmax_param_0];
	ld.param.u64 	%rd4, [Softmax_param_1];
	ld.param.u32 	%r3, [Softmax_param_2];
	ld.param.u32 	%r2, [Softmax_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r1, %r2;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB67_6;

	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r7, 0;
	st.shared.u32 	[_ZZ7SoftmaxE3max], %r7;
	st.shared.u32 	[_ZZ7SoftmaxE3sum], %r7;
	bar.sync 	0;
	mad.lo.s32 	%r8, %r1, %r2, %r1;
	cvt.s64.s32 	%rd1, %r8;
	mul.wide.s32 	%rd6, %r8, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.shared.f32 	%f4, [_ZZ7SoftmaxE3max];
	ld.global.nc.f32 	%f1, [%rd7];
	setp.geu.ftz.f32 	%p4, %f4, %f1;
	@%p4 bra 	$L__BB67_3;

	st.shared.f32 	[_ZZ7SoftmaxE3max], %f1;

$L__BB67_3:
	bar.sync 	0;
	ld.shared.f32 	%f5, [_ZZ7SoftmaxE3max];
	sub.ftz.f32 	%f6, %f1, %f5;
	mul.ftz.f32 	%f7, %f6, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f8, %f7;
	ld.shared.f32 	%f9, [_ZZ7SoftmaxE3sum];
	add.ftz.f32 	%f10, %f8, %f9;
	st.shared.f32 	[_ZZ7SoftmaxE3sum], %f10;
	cvta.to.global.u64 	%rd8, %rd4;
	shl.b64 	%rd9, %rd1, 2;
	add.s64 	%rd2, %rd8, %rd9;
	st.global.f32 	[%rd2], %f8;
	bar.sync 	0;
	ld.shared.f32 	%f2, [_ZZ7SoftmaxE3sum];
	setp.neu.ftz.f32 	%p5, %f2, 0f00000000;
	ld.global.f32 	%f3, [%rd2];
	@%p5 bra 	$L__BB67_5;
	bra.uni 	$L__BB67_4;

$L__BB67_5:
	div.approx.ftz.f32 	%f13, %f3, %f2;
	st.global.f32 	[%rd2], %f13;
	bra.uni 	$L__BB67_6;

$L__BB67_4:
	mov.f32 	%f11, 0f33D6BF95;
	div.approx.ftz.f32 	%f12, %f3, %f11;
	st.global.f32 	[%rd2], %f12;

$L__BB67_6:
	ret;

}
	// .globl	Softmax_half
.visible .entry Softmax_half(
	.param .u64 Softmax_half_param_0,
	.param .u64 Softmax_half_param_1,
	.param .u32 Softmax_half_param_2,
	.param .u32 Softmax_half_param_3
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<71>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<10>;
	// demoted variable
	.shared .align 2 .u16 _ZZ12Softmax_halfE3max_$_0;
	// demoted variable
	.shared .align 2 .u16 _ZZ12Softmax_halfE3sum_$_0;

	ld.param.u64 	%rd3, [Softmax_half_param_0];
	ld.param.u64 	%rd4, [Softmax_half_param_1];
	ld.param.u32 	%r3, [Softmax_half_param_2];
	ld.param.u32 	%r2, [Softmax_half_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r1, %r2;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB68_20;

	cvta.to.global.u64 	%rd5, %rd3;
	ld.const.u16 	%rs1, [sh];
	st.shared.u16 	[_ZZ12Softmax_halfE3max_$_0], %rs1;
	st.shared.u16 	[_ZZ12Softmax_halfE3sum_$_0], %rs1;
	bar.sync 	0;
	mad.lo.s32 	%r7, %r1, %r2, %r1;
	cvt.s64.s32 	%rd1, %r7;
	mul.wide.s32 	%rd6, %r7, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs2, [%rd7];
	ld.shared.u16 	%rs23, [_ZZ12Softmax_halfE3max_$_0];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs23, %rs2;
  selp.u16 %rs22, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs22, 0;
	@%p4 bra 	$L__BB68_3;

	st.shared.u16 	[_ZZ12Softmax_halfE3max_$_0], %rs2;

$L__BB68_3:
	bar.sync 	0;
	ld.shared.u16 	%rs27, [_ZZ12Softmax_halfE3max_$_0];
	// begin inline asm
	{sub.f16 %rs25,%rs2,%rs27;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs25;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs67,r;           
}
	// end inline asm
	setp.eq.s16 	%p5, %rs67, 31744;
	@%p5 bra 	$L__BB68_6;
	bra.uni 	$L__BB68_4;

$L__BB68_6:
	ld.const.u16 	%rs67, [sh+24];
	bra.uni 	$L__BB68_7;

$L__BB68_4:
	setp.ne.s16 	%p6, %rs67, -1024;
	@%p6 bra 	$L__BB68_7;

	ld.const.u16 	%rs31, [sh+24];
	// begin inline asm
	{neg.f16 %rs67,%rs31;
}
	// end inline asm

$L__BB68_7:
	ld.shared.u16 	%rs33, [_ZZ12Softmax_halfE3sum_$_0];
	// begin inline asm
	{add.f16 %rs68,%rs33,%rs67;
}
	// end inline asm
	st.shared.u16 	[_ZZ12Softmax_halfE3sum_$_0], %rs68;
	cvta.to.global.u64 	%rd8, %rd4;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd2, %rd8, %rd9;
	st.global.u16 	[%rd2], %rs67;
	setp.eq.s16 	%p7, %rs68, 31744;
	@%p7 bra 	$L__BB68_10;
	bra.uni 	$L__BB68_8;

$L__BB68_10:
	ld.const.u16 	%rs68, [sh+24];
	bra.uni 	$L__BB68_11;

$L__BB68_8:
	setp.ne.s16 	%p8, %rs68, -1024;
	@%p8 bra 	$L__BB68_11;

	ld.const.u16 	%rs36, [sh+24];
	// begin inline asm
	{neg.f16 %rs68,%rs36;
}
	// end inline asm

$L__BB68_11:
	st.shared.u16 	[_ZZ12Softmax_halfE3sum_$_0], %rs68;
	bar.sync 	0;
	ld.shared.u16 	%rs13, [_ZZ12Softmax_halfE3sum_$_0];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.neu.f16  __$temp3, %rs13, %rs1;
  selp.u16 %rs37, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p9, %rs37, 0;
	@%p9 bra 	$L__BB68_16;

	ld.global.u16 	%rs40, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs40;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs13;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f11, %f10;
}
	// end inline asm
	mul.ftz.f32 	%f13, %f9, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f13;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs43,%rs69;
}
	// end inline asm
	mov.u16 	%rs47, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs43, %rs47;
  selp.u16 %rs45, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p10, %rs45, 0;
	@%p10 bra 	$L__BB68_15;

	mov.f32 	%f14, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f14;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs48, %rs43;
  selp.u16 %rs49, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p11, %rs49, 0;
	@%p11 bra 	$L__BB68_15;

	neg.ftz.f32 	%f16, %f10;
	fma.rn.ftz.f32 	%f17, %f16, %f13, %f9;
	fma.rn.ftz.f32 	%f15, %f11, %f17, %f13;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f15;}

	// end inline asm

$L__BB68_15:
	st.global.u16 	[%rd2], %rs69;
	bra.uni 	$L__BB68_20;

$L__BB68_16:
	mov.f32 	%f18, 0f33D6BF95;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f18;}

	// end inline asm
	ld.global.u16 	%rs54, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs54;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs53;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f21, %f20;
}
	// end inline asm
	mul.ftz.f32 	%f23, %f19, %f21;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f23;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs57,%rs70;
}
	// end inline asm
	mov.u16 	%rs61, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs57, %rs61;
  selp.u16 %rs59, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p12, %rs59, 0;
	@%p12 bra 	$L__BB68_19;

	mov.f32 	%f24, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs62, %f24;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs62, %rs57;
  selp.u16 %rs63, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p13, %rs63, 0;
	@%p13 bra 	$L__BB68_19;

	neg.ftz.f32 	%f26, %f20;
	fma.rn.ftz.f32 	%f27, %f26, %f23, %f19;
	fma.rn.ftz.f32 	%f25, %f21, %f27, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f25;}

	// end inline asm

$L__BB68_19:
	st.global.u16 	[%rd2], %rs70;

$L__BB68_20:
	ret;

}
	// .globl	derSoftmax
.visible .entry derSoftmax(
	.param .u64 derSoftmax_param_0,
	.param .u64 derSoftmax_param_1,
	.param .u64 derSoftmax_param_2,
	.param .u32 derSoftmax_param_3,
	.param .u32 derSoftmax_param_4
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<101>;
	.reg .b32 	%r<94>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd35, [derSoftmax_param_0];
	ld.param.u64 	%rd36, [derSoftmax_param_1];
	ld.param.u64 	%rd34, [derSoftmax_param_2];
	ld.param.u32 	%r42, [derSoftmax_param_3];
	ld.param.u32 	%r41, [derSoftmax_param_4];
	cvta.to.global.u64 	%rd1, %rd36;
	cvta.to.global.u64 	%rd2, %rd35;
	mov.u32 	%r43, %ctaid.x;
	mov.u32 	%r44, %ntid.x;
	mov.u32 	%r45, %tid.x;
	mad.lo.s32 	%r1, %r44, %r43, %r45;
	mov.u32 	%r46, %ctaid.y;
	mov.u32 	%r47, %ntid.y;
	mul.lo.s32 	%r2, %r47, %r46;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r42;
	setp.ge.s32 	%p2, %r4, %r41;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB69_23;

	cvta.to.global.u64 	%rd37, %rd34;
	mul.lo.s32 	%r5, %r1, %r41;
	add.s32 	%r49, %r5, %r4;
	mul.wide.s32 	%rd38, %r49, 4;
	add.s64 	%rd3, %rd37, %rd38;
	mov.f32 	%f95, 0f00000000;
	mov.u32 	%r89, 0;
	st.global.u32 	[%rd3], %r89;
	add.s64 	%rd39, %rd2, %rd38;
	ld.global.nc.f32 	%f1, [%rd39];
	max.s32 	%r6, %r4, 0;
	min.s32 	%r7, %r6, %r41;
	setp.lt.s32 	%p4, %r7, 1;
	@%p4 bra 	$L__BB69_8;

	add.s32 	%r52, %r7, -1;
	and.b32  	%r82, %r7, 3;
	setp.lt.u32 	%p5, %r52, 3;
	mov.f32 	%f95, 0f00000000;
	mov.u32 	%r89, 0;
	@%p5 bra 	$L__BB69_5;

	not.b32 	%r54, %r6;
	not.b32 	%r55, %r41;
	max.s32 	%r56, %r54, %r55;
	add.s32 	%r57, %r56, %r82;
	neg.s32 	%r77, %r57;
	mul.wide.s32 	%rd40, %r5, 4;
	add.s64 	%rd41, %rd40, 8;
	add.s64 	%rd50, %rd2, %rd41;
	add.s64 	%rd49, %rd1, %rd41;

$L__BB69_4:
	ld.global.nc.f32 	%f28, [%rd50+-8];
	mul.ftz.f32 	%f29, %f28, %f1;
	ld.global.nc.f32 	%f30, [%rd49+-8];
	mul.ftz.f32 	%f31, %f29, %f30;
	sub.ftz.f32 	%f32, %f95, %f31;
	ld.global.nc.f32 	%f33, [%rd50+-4];
	mul.ftz.f32 	%f34, %f33, %f1;
	ld.global.nc.f32 	%f35, [%rd49+-4];
	mul.ftz.f32 	%f36, %f34, %f35;
	sub.ftz.f32 	%f37, %f32, %f36;
	ld.global.nc.f32 	%f38, [%rd50];
	mul.ftz.f32 	%f39, %f38, %f1;
	ld.global.nc.f32 	%f40, [%rd49];
	mul.ftz.f32 	%f41, %f39, %f40;
	sub.ftz.f32 	%f42, %f37, %f41;
	ld.global.nc.f32 	%f43, [%rd50+4];
	mul.ftz.f32 	%f44, %f43, %f1;
	ld.global.nc.f32 	%f45, [%rd49+4];
	mul.ftz.f32 	%f46, %f44, %f45;
	sub.ftz.f32 	%f95, %f42, %f46;
	add.s32 	%r89, %r89, 4;
	add.s64 	%rd50, %rd50, 16;
	add.s64 	%rd49, %rd49, 16;
	add.s32 	%r77, %r77, -4;
	setp.ne.s32 	%p6, %r77, 1;
	@%p6 bra 	$L__BB69_4;

$L__BB69_5:
	setp.eq.s32 	%p7, %r82, 0;
	@%p7 bra 	$L__BB69_8;

	add.s32 	%r58, %r89, %r5;
	mul.wide.s32 	%rd42, %r58, 4;
	add.s64 	%rd52, %rd1, %rd42;
	add.s64 	%rd51, %rd2, %rd42;

$L__BB69_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f47, [%rd51];
	mul.ftz.f32 	%f48, %f47, %f1;
	ld.global.nc.f32 	%f49, [%rd52];
	mul.ftz.f32 	%f50, %f48, %f49;
	sub.ftz.f32 	%f95, %f95, %f50;
	add.s32 	%r89, %r89, 1;
	add.s64 	%rd52, %rd52, 4;
	add.s64 	%rd51, %rd51, 4;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p8, %r82, 0;
	@%p8 bra 	$L__BB69_7;

$L__BB69_8:
	add.s32 	%r59, %r4, 1;
	min.s32 	%r21, %r59, %r41;
	setp.ge.s32 	%p9, %r89, %r21;
	@%p9 bra 	$L__BB69_15;

	mov.f32 	%f52, 0f3F800000;
	sub.ftz.f32 	%f53, %f52, %f1;
	mul.ftz.f32 	%f9, %f1, %f53;
	mov.u32 	%r61, -2;
	sub.s32 	%r62, %r61, %r2;
	sub.s32 	%r63, %r62, %r3;
	not.b32 	%r64, %r41;
	max.s32 	%r22, %r63, %r64;
	not.b32 	%r65, %r89;
	sub.s32 	%r66, %r65, %r22;
	and.b32  	%r85, %r66, 3;
	setp.eq.s32 	%p10, %r85, 0;
	mov.u32 	%r86, %r89;
	@%p10 bra 	$L__BB69_12;

	add.s32 	%r67, %r89, %r5;
	mul.wide.s32 	%rd43, %r67, 4;
	add.s64 	%rd53, %rd1, %rd43;
	mov.u32 	%r86, %r89;

$L__BB69_11:
	.pragma "nounroll";
	ld.global.nc.f32 	%f54, [%rd53];
	fma.rn.ftz.f32 	%f95, %f54, %f9, %f95;
	add.s32 	%r86, %r86, 1;
	add.s64 	%rd53, %rd53, 4;
	add.s32 	%r85, %r85, -1;
	setp.ne.s32 	%p11, %r85, 0;
	@%p11 bra 	$L__BB69_11;

$L__BB69_12:
	sub.s32 	%r69, %r61, %r89;
	sub.s32 	%r70, %r69, %r22;
	setp.lt.u32 	%p12, %r70, 3;
	mov.u32 	%r89, %r86;
	@%p12 bra 	$L__BB69_15;

	add.s32 	%r71, %r86, %r5;
	mul.wide.s32 	%rd44, %r71, 4;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd54, %rd45, 8;
	mov.u32 	%r89, %r86;

$L__BB69_14:
	ld.global.nc.f32 	%f55, [%rd54+-8];
	fma.rn.ftz.f32 	%f56, %f55, %f9, %f95;
	ld.global.nc.f32 	%f57, [%rd54+-4];
	fma.rn.ftz.f32 	%f58, %f57, %f9, %f56;
	ld.global.nc.f32 	%f59, [%rd54];
	fma.rn.ftz.f32 	%f60, %f59, %f9, %f58;
	ld.global.nc.f32 	%f61, [%rd54+4];
	fma.rn.ftz.f32 	%f95, %f61, %f9, %f60;
	add.s64 	%rd54, %rd54, 16;
	add.s32 	%r89, %r89, 4;
	setp.lt.s32 	%p13, %r89, %r21;
	@%p13 bra 	$L__BB69_14;

$L__BB69_15:
	setp.ge.s32 	%p14, %r89, %r41;
	@%p14 bra 	$L__BB69_22;

	sub.s32 	%r72, %r41, %r89;
	and.b32  	%r91, %r72, 3;
	setp.eq.s32 	%p15, %r91, 0;
	mov.u32 	%r92, %r89;
	@%p15 bra 	$L__BB69_19;

	add.s32 	%r73, %r89, %r5;
	mul.wide.s32 	%rd46, %r73, 4;
	add.s64 	%rd56, %rd1, %rd46;
	add.s64 	%rd55, %rd2, %rd46;
	mov.u32 	%r92, %r89;

$L__BB69_18:
	.pragma "nounroll";
	ld.global.nc.f32 	%f63, [%rd55];
	mul.ftz.f32 	%f64, %f63, %f1;
	ld.global.nc.f32 	%f65, [%rd56];
	mul.ftz.f32 	%f66, %f64, %f65;
	sub.ftz.f32 	%f95, %f95, %f66;
	add.s32 	%r92, %r92, 1;
	add.s64 	%rd56, %rd56, 4;
	add.s64 	%rd55, %rd55, 4;
	add.s32 	%r91, %r91, -1;
	setp.ne.s32 	%p16, %r91, 0;
	@%p16 bra 	$L__BB69_18;

$L__BB69_19:
	not.b32 	%r74, %r89;
	add.s32 	%r75, %r74, %r41;
	setp.lt.u32 	%p17, %r75, 3;
	@%p17 bra 	$L__BB69_22;

	add.s32 	%r76, %r92, %r5;
	mul.wide.s32 	%rd47, %r76, 4;
	add.s64 	%rd48, %rd47, 8;
	add.s64 	%rd58, %rd1, %rd48;
	add.s64 	%rd57, %rd2, %rd48;

$L__BB69_21:
	ld.global.nc.f32 	%f67, [%rd57+-8];
	mul.ftz.f32 	%f68, %f67, %f1;
	ld.global.nc.f32 	%f69, [%rd58+-8];
	mul.ftz.f32 	%f70, %f68, %f69;
	sub.ftz.f32 	%f71, %f95, %f70;
	ld.global.nc.f32 	%f72, [%rd57+-4];
	mul.ftz.f32 	%f73, %f72, %f1;
	ld.global.nc.f32 	%f74, [%rd58+-4];
	mul.ftz.f32 	%f75, %f73, %f74;
	sub.ftz.f32 	%f76, %f71, %f75;
	ld.global.nc.f32 	%f77, [%rd57];
	mul.ftz.f32 	%f78, %f77, %f1;
	ld.global.nc.f32 	%f79, [%rd58];
	mul.ftz.f32 	%f80, %f78, %f79;
	sub.ftz.f32 	%f81, %f76, %f80;
	ld.global.nc.f32 	%f82, [%rd57+4];
	mul.ftz.f32 	%f83, %f82, %f1;
	ld.global.nc.f32 	%f84, [%rd58+4];
	mul.ftz.f32 	%f85, %f83, %f84;
	sub.ftz.f32 	%f95, %f81, %f85;
	add.s64 	%rd58, %rd58, 16;
	add.s64 	%rd57, %rd57, 16;
	add.s32 	%r92, %r92, 4;
	setp.lt.s32 	%p18, %r92, %r41;
	@%p18 bra 	$L__BB69_21;

$L__BB69_22:
	st.global.f32 	[%rd3], %f95;

$L__BB69_23:
	ret;

}
	// .globl	derSoftmax_half
.visible .entry derSoftmax_half(
	.param .u64 derSoftmax_half_param_0,
	.param .u64 derSoftmax_half_param_1,
	.param .u64 derSoftmax_half_param_2,
	.param .u32 derSoftmax_half_param_3,
	.param .u32 derSoftmax_half_param_4
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<132>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd18, [derSoftmax_half_param_0];
	ld.param.u64 	%rd20, [derSoftmax_half_param_1];
	ld.param.u64 	%rd19, [derSoftmax_half_param_2];
	ld.param.u32 	%r22, [derSoftmax_half_param_3];
	ld.param.u32 	%r21, [derSoftmax_half_param_4];
	cvta.to.global.u64 	%rd1, %rd20;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r1, %r24, %r23, %r25;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %ntid.y;
	mul.lo.s32 	%r2, %r27, %r26;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r22;
	setp.ge.s32 	%p2, %r4, %r21;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB70_28;

	cvta.to.global.u64 	%rd21, %rd19;
	ld.const.u16 	%rs130, [sh];
	mul.lo.s32 	%r5, %r1, %r21;
	add.s32 	%r28, %r5, %r4;
	mul.wide.s32 	%rd22, %r28, 2;
	add.s64 	%rd3, %rd21, %rd22;
	st.global.u16 	[%rd3], %rs130;
	add.s64 	%rd23, %rd2, %rd22;
	ld.global.nc.u16 	%rs2, [%rd23];
	setp.lt.s32 	%p4, %r21, 1;
	@%p4 bra 	$L__BB70_23;

	ld.const.u16 	%rs3, [sh+8];
	and.b32  	%r42, %r21, 3;
	add.s32 	%r30, %r21, -1;
	setp.lt.u32 	%p5, %r30, 3;
	mov.u32 	%r40, 0;
	@%p5 bra 	$L__BB70_17;

	sub.s32 	%r39, %r21, %r42;
	neg.s32 	%r37, %r4;
	mul.wide.s32 	%rd24, %r5, 2;
	add.s64 	%rd25, %rd24, 4;
	add.s64 	%rd29, %rd1, %rd25;
	add.s64 	%rd28, %rd2, %rd25;
	// begin inline asm
	{sub.f16 %rs39,%rs3,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs42,%rs2,%rs39;
}
	// end inline asm

$L__BB70_4:
	add.s64 	%rd8, %rd28, -4;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB70_6;

	ld.global.nc.u16 	%rs35, [%rd8];
	// begin inline asm
	{neg.f16 %rs34,%rs35;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs122,%rs2,%rs34;
}
	// end inline asm
	bra.uni 	$L__BB70_7;

$L__BB70_6:
	mov.u16 	%rs122, %rs42;

$L__BB70_7:
	add.s64 	%rd9, %rd29, -4;
	ld.global.nc.u16 	%rs46, [%rd29+-4];
	// begin inline asm
	{mul.f16 %rs45,%rs46,%rs122;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs48,%rs130,%rs45;
}
	// end inline asm
	add.s32 	%r32, %r40, 1;
	setp.eq.s32 	%p7, %r4, %r32;
	@%p7 bra 	$L__BB70_9;
	bra.uni 	$L__BB70_8;

$L__BB70_9:
	mov.u16 	%rs123, %rs42;
	bra.uni 	$L__BB70_10;

$L__BB70_8:
	ld.global.nc.u16 	%rs52, [%rd8+2];
	// begin inline asm
	{neg.f16 %rs51,%rs52;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs123,%rs2,%rs51;
}
	// end inline asm

$L__BB70_10:
	ld.global.nc.u16 	%rs63, [%rd9+2];
	// begin inline asm
	{mul.f16 %rs62,%rs63,%rs123;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs65,%rs48,%rs62;
}
	// end inline asm
	add.s32 	%r33, %r40, 2;
	setp.eq.s32 	%p8, %r4, %r33;
	@%p8 bra 	$L__BB70_12;
	bra.uni 	$L__BB70_11;

$L__BB70_12:
	mov.u16 	%rs124, %rs42;
	bra.uni 	$L__BB70_13;

$L__BB70_11:
	ld.global.nc.u16 	%rs69, [%rd8+4];
	// begin inline asm
	{neg.f16 %rs68,%rs69;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs124,%rs2,%rs68;
}
	// end inline asm

$L__BB70_13:
	ld.global.nc.u16 	%rs80, [%rd9+4];
	// begin inline asm
	{mul.f16 %rs79,%rs80,%rs124;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs82,%rs65,%rs79;
}
	// end inline asm
	add.s32 	%r34, %r40, 3;
	setp.eq.s32 	%p9, %r4, %r34;
	@%p9 bra 	$L__BB70_15;
	bra.uni 	$L__BB70_14;

$L__BB70_15:
	mov.u16 	%rs125, %rs42;
	bra.uni 	$L__BB70_16;

$L__BB70_14:
	ld.global.nc.u16 	%rs86, [%rd8+6];
	// begin inline asm
	{neg.f16 %rs85,%rs86;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs125,%rs2,%rs85;
}
	// end inline asm

$L__BB70_16:
	ld.global.nc.u16 	%rs97, [%rd9+6];
	// begin inline asm
	{mul.f16 %rs96,%rs97,%rs125;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs130,%rs82,%rs96;
}
	// end inline asm
	add.s32 	%r40, %r40, 4;
	add.s32 	%r37, %r37, 4;
	add.s64 	%rd29, %rd29, 8;
	add.s64 	%rd28, %rd28, 8;
	add.s32 	%r39, %r39, -4;
	setp.ne.s32 	%p10, %r39, 0;
	@%p10 bra 	$L__BB70_4;

$L__BB70_17:
	setp.eq.s32 	%p11, %r42, 0;
	@%p11 bra 	$L__BB70_23;

	sub.s32 	%r35, %r40, %r2;
	sub.s32 	%r41, %r35, %r3;
	add.s32 	%r36, %r40, %r5;
	mul.wide.s32 	%rd26, %r36, 2;
	add.s64 	%rd31, %rd1, %rd26;
	add.s64 	%rd30, %rd2, %rd26;
	// begin inline asm
	{sub.f16 %rs107,%rs3,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs110,%rs2,%rs107;
}
	// end inline asm

$L__BB70_19:
	.pragma "nounroll";
	setp.eq.s32 	%p12, %r41, 0;
	@%p12 bra 	$L__BB70_21;

	ld.global.nc.u16 	%rs103, [%rd30];
	// begin inline asm
	{neg.f16 %rs102,%rs103;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs129,%rs2,%rs102;
}
	// end inline asm
	bra.uni 	$L__BB70_22;

$L__BB70_21:
	mov.u16 	%rs129, %rs110;

$L__BB70_22:
	ld.global.nc.u16 	%rs114, [%rd31];
	// begin inline asm
	{mul.f16 %rs113,%rs114,%rs129;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs130,%rs130,%rs113;
}
	// end inline asm
	add.s32 	%r41, %r41, 1;
	add.s64 	%rd31, %rd31, 2;
	add.s64 	%rd30, %rd30, 2;
	add.s32 	%r42, %r42, -1;
	setp.ne.s32 	%p13, %r42, 0;
	@%p13 bra 	$L__BB70_19;

$L__BB70_23:
	setp.eq.s16 	%p14, %rs130, 31744;
	@%p14 bra 	$L__BB70_26;
	bra.uni 	$L__BB70_24;

$L__BB70_26:
	ld.const.u16 	%rs130, [sh+24];
	bra.uni 	$L__BB70_27;

$L__BB70_24:
	setp.ne.s16 	%p15, %rs130, -1024;
	@%p15 bra 	$L__BB70_27;

	ld.const.u16 	%rs120, [sh+24];
	// begin inline asm
	{neg.f16 %rs130,%rs120;
}
	// end inline asm

$L__BB70_27:
	st.global.u16 	[%rd3], %rs130;

$L__BB70_28:
	ret;

}

