//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32415258
// Cuda compilation tools, release 12.1, V12.1.66
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	fill
.global .align 4 .u32 SharedMemorySize = 32768;
.global .align 4 .u32 BLOCK_DIM = 32;
.global .align 4 .u32 TILE_WIDTH = 32;
.const .align 2 .b8 sh[34];
// _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax has been demoted
// _ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx has been demoted
// _ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax has been demoted
// _ZZ32reduceMaxIdxOptimizedShared_TYPEE12sharedMaxIdx has been demoted
// _ZZ19sharedMem_transposeE8M_Shared has been demoted
// _ZZ24sharedMem_transpose_TYPEE8M_Shared has been demoted
.global .align 4 .u32 BLOCK_SIZE = 32;
// _ZZ13matvec_kernelE8x_shared has been demoted
// _ZZ18matvec_kernel_TYPEE8x_shared has been demoted
// _ZZ11matrixMultiE4ds_A has been demoted
// _ZZ11matrixMultiE4ds_B has been demoted
// _ZZ16matrixMulti_TYPEE4ds_A has been demoted
// _ZZ16matrixMulti_TYPEE4ds_B has been demoted
// _ZZ13convolution2DE4N_ds has been demoted
// _ZZ22transposeConvolution2DE4N_ds has been demoted

.visible .entry fill(
	.param .u64 fill_param_0,
	.param .align 2 .b8 fill_param_1[2],
	.param .u32 fill_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs1, [fill_param_1];
	ld.param.u64 	%rd1, [fill_param_0];
	ld.param.u32 	%r2, [fill_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u16 	[%rd4], %rs1;

$L__BB0_2:
	ret;

}
	// .globl	fill_float
.visible .entry fill_float(
	.param .u64 fill_float_param_0,
	.param .f32 fill_float_param_1,
	.param .u32 fill_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_float_param_0];
	ld.param.f32 	%f1, [fill_float_param_1];
	ld.param.u32 	%r2, [fill_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f32 	[%rd4], %f1;

$L__BB1_2:
	ret;

}
	// .globl	float2TYPEVector
.visible .entry float2TYPEVector(
	.param .u64 float2TYPEVector_param_0,
	.param .u64 float2TYPEVector_param_1,
	.param .u32 float2TYPEVector_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [float2TYPEVector_param_0];
	ld.param.u64 	%rd3, [float2TYPEVector_param_1];
	ld.param.u32 	%r3, [float2TYPEVector_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB2_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r7, [%rd6];
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs4, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs6, 32767, %rs4, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB2_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs5, %rs6, 1;
	setp.eq.b16 	%p5, %rs5, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB2_4;

$L__BB2_3:
	add.s16 	%rs6, %rs6, 1;

$L__BB2_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs6;

$L__BB2_5:
	ret;

}
	// .globl	TYPE2FloatVector
.visible .entry TYPE2FloatVector(
	.param .u64 TYPE2FloatVector_param_0,
	.param .u64 TYPE2FloatVector_param_1,
	.param .u32 TYPE2FloatVector_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [TYPE2FloatVector_param_0];
	ld.param.u64 	%rd2, [TYPE2FloatVector_param_1];
	ld.param.u32 	%r2, [TYPE2FloatVector_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB3_2:
	ret;

}
	// .globl	gelu
.visible .entry gelu(
	.param .u64 gelu_param_0,
	.param .u64 gelu_param_1,
	.param .u32 gelu_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [gelu_param_0];
	ld.param.u64 	%rd3, [gelu_param_1];
	ld.param.u32 	%r2, [gelu_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.ftz.f32 	%f7, %f1, %f1;
	mul.ftz.f32 	%f8, %f1, %f7;
	mul.ftz.f32 	%f9, %f8, 0f3D122277;
	fma.rn.ftz.f32 	%f2, %f1, 0f3F4C422A, %f9;
	abs.ftz.f32 	%f3, %f2;
	setp.ltu.ftz.f32 	%p2, %f3, 0f3F19999A;
	@%p2 bra 	$L__BB4_3;
	bra.uni 	$L__BB4_2;

$L__BB4_3:
	mul.ftz.f32 	%f18, %f2, %f2;
	mov.f32 	%f19, 0fBD563CAE;
	mov.f32 	%f20, 0f3C80F082;
	fma.rn.ftz.f32 	%f21, %f20, %f18, %f19;
	mov.f32 	%f22, 0f3E085941;
	fma.rn.ftz.f32 	%f23, %f21, %f18, %f22;
	mov.f32 	%f24, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f25, %f23, %f18, %f24;
	mov.f32 	%f26, 0f00000000;
	fma.rn.ftz.f32 	%f27, %f25, %f18, %f26;
	fma.rn.ftz.f32 	%f31, %f27, %f2, %f2;
	bra.uni 	$L__BB4_4;

$L__BB4_2:
	mul.ftz.f32 	%f10, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f11, %f10;
	add.ftz.f32 	%f12, %f11, 0f3F800000;
	mov.f32 	%f13, 0f3F800000;
	rcp.approx.ftz.f32 	%f14, %f12;
	mov.f32 	%f15, 0fC0000000;
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	setp.ge.ftz.f32 	%p3, %f3, 0f41102CB4;
	selp.f32 	%f17, 0f3F800000, %f16, %p3;
	mov.b32 	%r6, %f17;
	mov.b32 	%r7, %f2;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f31, %r9;

$L__BB4_4:
	add.ftz.f32 	%f28, %f31, 0f3F800000;
	mul.ftz.f32 	%f29, %f1, 0f3F000000;
	mul.ftz.f32 	%f30, %f29, %f28;
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f30;

$L__BB4_5:
	ret;

}
	// .globl	gelu_TYPE
.visible .entry gelu_TYPE(
	.param .u64 gelu_TYPE_param_0,
	.param .u64 gelu_TYPE_param_1,
	.param .u32 gelu_TYPE_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [gelu_TYPE_param_0];
	ld.param.u64 	%rd3, [gelu_TYPE_param_1];
	ld.param.u32 	%r3, [gelu_TYPE_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB5_8;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs4, [%rd6];
	// begin inline asm
	{ mov.b32 %f7, {0,%rs4};}

	// end inline asm
	mul.ftz.f32 	%f8, %f7, %f7;
	mul.ftz.f32 	%f9, %f7, %f8;
	mul.ftz.f32 	%f10, %f9, 0f3D122277;
	fma.rn.ftz.f32 	%f2, %f7, 0f3F4C422A, %f10;
	abs.ftz.f32 	%f3, %f2;
	setp.ltu.ftz.f32 	%p2, %f3, 0f3F19999A;
	@%p2 bra 	$L__BB5_3;
	bra.uni 	$L__BB5_2;

$L__BB5_3:
	mul.ftz.f32 	%f19, %f2, %f2;
	mov.f32 	%f20, 0fBD563CAE;
	mov.f32 	%f21, 0f3C80F082;
	fma.rn.ftz.f32 	%f22, %f21, %f19, %f20;
	mov.f32 	%f23, 0f3E085941;
	fma.rn.ftz.f32 	%f24, %f22, %f19, %f23;
	mov.f32 	%f25, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f26, %f24, %f19, %f25;
	mov.f32 	%f27, 0f00000000;
	fma.rn.ftz.f32 	%f28, %f26, %f19, %f27;
	fma.rn.ftz.f32 	%f32, %f28, %f2, %f2;
	bra.uni 	$L__BB5_4;

$L__BB5_2:
	mul.ftz.f32 	%f11, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f12, %f11;
	add.ftz.f32 	%f13, %f12, 0f3F800000;
	mov.f32 	%f14, 0f3F800000;
	rcp.approx.ftz.f32 	%f15, %f13;
	mov.f32 	%f16, 0fC0000000;
	fma.rn.ftz.f32 	%f17, %f15, %f16, %f14;
	setp.ge.ftz.f32 	%p3, %f3, 0f41102CB4;
	selp.f32 	%f18, 0f3F800000, %f17, %p3;
	mov.b32 	%r7, %f18;
	mov.b32 	%r8, %f2;
	and.b32  	%r9, %r8, -2147483648;
	or.b32  	%r10, %r9, %r7;
	mov.b32 	%f32, %r10;

$L__BB5_4:
	add.ftz.f32 	%f29, %f32, 0f3F800000;
	mul.ftz.f32 	%f30, %f7, 0f3F000000;
	mul.ftz.f32 	%f31, %f30, %f29;
	mov.b32 	%r11, %f31;
	and.b32  	%r12, %r11, 2147483647;
	setp.gt.u32 	%p4, %r12, 2139095040;
	shl.b32 	%r13, %r11, 16;
	shr.u32 	%r14, %r11, 16;
	cvt.u16.u32 	%rs5, %r14;
	selp.b32 	%r2, 0, %r13, %p4;
	selp.b16 	%rs7, 32767, %rs5, %p4;
	setp.gt.u32 	%p5, %r2, -2147483648;
	@%p5 bra 	$L__BB5_6;

	setp.ne.s32 	%p6, %r2, -2147483648;
	and.b16  	%rs6, %rs7, 1;
	setp.eq.b16 	%p7, %rs6, 1;
	not.pred 	%p8, %p7;
	or.pred  	%p9, %p6, %p8;
	@%p9 bra 	$L__BB5_7;

$L__BB5_6:
	add.s16 	%rs7, %rs7, 1;

$L__BB5_7:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs7;

$L__BB5_8:
	ret;

}
	// .globl	matAdd
.visible .entry matAdd(
	.param .u64 matAdd_param_0,
	.param .u64 matAdd_param_1,
	.param .u32 matAdd_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [matAdd_param_0];
	ld.param.u64 	%rd2, [matAdd_param_1];
	ld.param.u32 	%r2, [matAdd_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f1, [%rd7];
	ld.global.f32 	%f2, [%rd5];
	add.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

$L__BB6_2:
	ret;

}
	// .globl	matAdd_TYPE
.visible .entry matAdd_TYPE(
	.param .u64 matAdd_TYPE_param_0,
	.param .u64 matAdd_TYPE_param_1,
	.param .u32 matAdd_TYPE_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd2, [matAdd_TYPE_param_0];
	ld.param.u64 	%rd3, [matAdd_TYPE_param_1];
	ld.param.u32 	%r3, [matAdd_TYPE_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB7_5;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.u16 	%rs4, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs5, [%rd7];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs5};}

	// end inline asm
	add.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r7, %f3;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs6, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs8, 32767, %rs6, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB7_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs7, %rs8, 1;
	setp.eq.b16 	%p5, %rs7, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB7_4;

$L__BB7_3:
	add.s16 	%rs8, %rs8, 1;

$L__BB7_4:
	st.global.u16 	[%rd1], %rs8;

$L__BB7_5:
	ret;

}
	// .globl	matAdd_
.visible .entry matAdd_(
	.param .u64 matAdd__param_0,
	.param .u64 matAdd__param_1,
	.param .u32 matAdd__param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [matAdd__param_0];
	ld.param.u64 	%rd2, [matAdd__param_1];
	ld.param.u32 	%r2, [matAdd__param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB8_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f1, [%rd7];
	ld.global.f32 	%f2, [%rd5];
	add.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

$L__BB8_2:
	ret;

}
	// .globl	matAdd_TYPE_
.visible .entry matAdd_TYPE_(
	.param .u64 matAdd_TYPE__param_0,
	.param .u64 matAdd_TYPE__param_1,
	.param .u32 matAdd_TYPE__param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd2, [matAdd_TYPE__param_0];
	ld.param.u64 	%rd3, [matAdd_TYPE__param_1];
	ld.param.u32 	%r3, [matAdd_TYPE__param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB9_5;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.u16 	%rs4, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs5, [%rd7];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs5};}

	// end inline asm
	add.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r7, %f3;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs6, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs8, 32767, %rs6, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB9_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs7, %rs8, 1;
	setp.eq.b16 	%p5, %rs7, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB9_4;

$L__BB9_3:
	add.s16 	%rs8, %rs8, 1;

$L__BB9_4:
	st.global.u16 	[%rd1], %rs8;

$L__BB9_5:
	ret;

}
	// .globl	imageVector
.visible .entry imageVector(
	.param .u64 imageVector_param_0,
	.param .u64 imageVector_param_1,
	.param .u32 imageVector_param_2,
	.param .u32 imageVector_param_3,
	.param .u32 imageVector_param_4,
	.param .u32 imageVector_param_5
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [imageVector_param_0];
	ld.param.u64 	%rd14, [imageVector_param_1];
	ld.param.u32 	%r27, [imageVector_param_2];
	ld.param.u32 	%r24, [imageVector_param_3];
	ld.param.u32 	%r25, [imageVector_param_4];
	ld.param.u32 	%r26, [imageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r28, %ctaid.x;
	mov.u32 	%r29, %ntid.x;
	mov.u32 	%r30, %tid.x;
	mad.lo.s32 	%r1, %r29, %r28, %r30;
	mul.lo.s32 	%r2, %r1, %r26;
	mov.u32 	%r31, %ctaid.y;
	mov.u32 	%r32, %ntid.y;
	mov.u32 	%r33, %tid.y;
	mad.lo.s32 	%r34, %r32, %r31, %r33;
	mul.lo.s32 	%r3, %r34, %r26;
	mov.u32 	%r35, %ctaid.z;
	mov.u32 	%r36, %ntid.z;
	mov.u32 	%r37, %tid.z;
	mad.lo.s32 	%r4, %r36, %r35, %r37;
	setp.ge.s32 	%p1, %r2, %r27;
	setp.ge.s32 	%p2, %r3, %r24;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r4, %r26;
	or.pred  	%p5, %p4, %p3;
	setp.lt.s32 	%p6, %r26, 1;
	or.pred  	%p7, %p5, %p6;
	@%p7 bra 	$L__BB10_11;

	add.s32 	%r38, %r2, %r4;
	mad.lo.s32 	%r5, %r38, %r24, %r3;
	add.s32 	%r6, %r3, %r4;
	setp.lt.s32 	%p8, %r25, 1;
	@%p8 bra 	$L__BB10_11;

	add.s32 	%r7, %r25, -1;
	and.b32  	%r8, %r25, 3;
	sub.s32 	%r9, %r25, %r8;
	add.s64 	%rd3, %rd2, 8;
	add.s64 	%rd4, %rd1, 8;
	mad.lo.s32 	%r40, %r24, %r1, %r6;
	mul.lo.s32 	%r10, %r26, %r40;
	mul.lo.s32 	%r41, %r2, %r24;
	mad.lo.s32 	%r11, %r6, %r26, %r41;
	mov.u32 	%r39, 0;
	mov.u32 	%r48, %r39;

$L__BB10_3:
	add.s32 	%r43, %r5, %r48;
	mul.lo.s32 	%r53, %r43, %r25;
	mul.lo.s32 	%r14, %r48, %r25;
	setp.lt.u32 	%p9, %r7, 3;
	mov.u32 	%r52, %r39;
	@%p9 bra 	$L__BB10_6;

	mul.wide.s32 	%rd15, %r53, 4;
	add.s64 	%rd20, %rd3, %rd15;
	add.s32 	%r45, %r10, %r14;
	mul.wide.s32 	%rd16, %r45, 4;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r52, 0;
	mov.u32 	%r51, %r9;

$L__BB10_5:
	ld.global.nc.f32 	%f1, [%rd20+-8];
	st.global.f32 	[%rd19+-8], %f1;
	ld.global.nc.f32 	%f2, [%rd20+-4];
	st.global.f32 	[%rd19+-4], %f2;
	ld.global.nc.f32 	%f3, [%rd20];
	st.global.f32 	[%rd19], %f3;
	ld.global.nc.f32 	%f4, [%rd20+4];
	st.global.f32 	[%rd19+4], %f4;
	add.s32 	%r52, %r52, 4;
	add.s32 	%r53, %r53, 4;
	add.s64 	%rd20, %rd20, 16;
	add.s64 	%rd19, %rd19, 16;
	add.s32 	%r51, %r51, -4;
	setp.ne.s32 	%p10, %r51, 0;
	@%p10 bra 	$L__BB10_5;

$L__BB10_6:
	setp.eq.s32 	%p11, %r8, 0;
	@%p11 bra 	$L__BB10_10;

	setp.eq.s32 	%p12, %r8, 1;
	mul.wide.s32 	%rd17, %r53, 4;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.f32 	%f5, [%rd11];
	add.s32 	%r46, %r11, %r14;
	add.s32 	%r47, %r46, %r52;
	mul.wide.s32 	%rd18, %r47, 4;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.f32 	[%rd12], %f5;
	@%p12 bra 	$L__BB10_10;

	setp.eq.s32 	%p13, %r8, 2;
	ld.global.nc.f32 	%f6, [%rd11+4];
	st.global.f32 	[%rd12+4], %f6;
	@%p13 bra 	$L__BB10_10;

	ld.global.nc.f32 	%f7, [%rd11+8];
	st.global.f32 	[%rd12+8], %f7;

$L__BB10_10:
	add.s32 	%r48, %r48, 1;
	setp.lt.s32 	%p14, %r48, %r26;
	@%p14 bra 	$L__BB10_3;

$L__BB10_11:
	ret;

}
	// .globl	backImageVector
.visible .entry backImageVector(
	.param .u64 backImageVector_param_0,
	.param .u64 backImageVector_param_1,
	.param .u32 backImageVector_param_2,
	.param .u32 backImageVector_param_3,
	.param .u32 backImageVector_param_4,
	.param .u32 backImageVector_param_5
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [backImageVector_param_0];
	ld.param.u64 	%rd14, [backImageVector_param_1];
	ld.param.u32 	%r27, [backImageVector_param_2];
	ld.param.u32 	%r24, [backImageVector_param_3];
	ld.param.u32 	%r25, [backImageVector_param_4];
	ld.param.u32 	%r26, [backImageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r28, %ctaid.x;
	mov.u32 	%r29, %ntid.x;
	mov.u32 	%r30, %tid.x;
	mad.lo.s32 	%r1, %r29, %r28, %r30;
	mul.lo.s32 	%r2, %r1, %r26;
	mov.u32 	%r31, %ctaid.y;
	mov.u32 	%r32, %ntid.y;
	mov.u32 	%r33, %tid.y;
	mad.lo.s32 	%r34, %r32, %r31, %r33;
	mul.lo.s32 	%r3, %r34, %r26;
	mov.u32 	%r35, %ctaid.z;
	mov.u32 	%r36, %ntid.z;
	mov.u32 	%r37, %tid.z;
	mad.lo.s32 	%r4, %r36, %r35, %r37;
	setp.ge.s32 	%p1, %r2, %r27;
	setp.ge.s32 	%p2, %r3, %r24;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r4, %r26;
	or.pred  	%p5, %p4, %p3;
	setp.lt.s32 	%p6, %r26, 1;
	or.pred  	%p7, %p5, %p6;
	@%p7 bra 	$L__BB11_11;

	add.s32 	%r38, %r2, %r4;
	mad.lo.s32 	%r5, %r38, %r24, %r3;
	add.s32 	%r6, %r3, %r4;
	setp.lt.s32 	%p8, %r25, 1;
	@%p8 bra 	$L__BB11_11;

	add.s32 	%r7, %r25, -1;
	and.b32  	%r8, %r25, 3;
	sub.s32 	%r9, %r25, %r8;
	add.s64 	%rd3, %rd1, 8;
	add.s64 	%rd4, %rd2, 8;
	mad.lo.s32 	%r40, %r24, %r1, %r6;
	mul.lo.s32 	%r10, %r26, %r40;
	mul.lo.s32 	%r41, %r2, %r24;
	mad.lo.s32 	%r11, %r6, %r26, %r41;
	mov.u32 	%r39, 0;
	mov.u32 	%r48, %r39;

$L__BB11_3:
	add.s32 	%r43, %r5, %r48;
	mul.lo.s32 	%r53, %r43, %r25;
	mul.lo.s32 	%r14, %r48, %r25;
	setp.lt.u32 	%p9, %r7, 3;
	mov.u32 	%r52, %r39;
	@%p9 bra 	$L__BB11_6;

	mul.wide.s32 	%rd15, %r53, 4;
	add.s64 	%rd20, %rd3, %rd15;
	add.s32 	%r45, %r10, %r14;
	mul.wide.s32 	%rd16, %r45, 4;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r52, 0;
	mov.u32 	%r51, %r9;

$L__BB11_5:
	ld.global.nc.f32 	%f1, [%rd19+-8];
	st.global.f32 	[%rd20+-8], %f1;
	ld.global.nc.f32 	%f2, [%rd19+-4];
	st.global.f32 	[%rd20+-4], %f2;
	ld.global.nc.f32 	%f3, [%rd19];
	st.global.f32 	[%rd20], %f3;
	ld.global.nc.f32 	%f4, [%rd19+4];
	st.global.f32 	[%rd20+4], %f4;
	add.s32 	%r52, %r52, 4;
	add.s32 	%r53, %r53, 4;
	add.s64 	%rd20, %rd20, 16;
	add.s64 	%rd19, %rd19, 16;
	add.s32 	%r51, %r51, -4;
	setp.ne.s32 	%p10, %r51, 0;
	@%p10 bra 	$L__BB11_5;

$L__BB11_6:
	setp.eq.s32 	%p11, %r8, 0;
	@%p11 bra 	$L__BB11_10;

	setp.eq.s32 	%p12, %r8, 1;
	add.s32 	%r46, %r11, %r14;
	add.s32 	%r47, %r46, %r52;
	mul.wide.s32 	%rd17, %r47, 4;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.f32 	%f5, [%rd11];
	mul.wide.s32 	%rd18, %r53, 4;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.f32 	[%rd12], %f5;
	@%p12 bra 	$L__BB11_10;

	setp.eq.s32 	%p13, %r8, 2;
	ld.global.nc.f32 	%f6, [%rd11+4];
	st.global.f32 	[%rd12+4], %f6;
	@%p13 bra 	$L__BB11_10;

	ld.global.nc.f32 	%f7, [%rd11+8];
	st.global.f32 	[%rd12+8], %f7;

$L__BB11_10:
	add.s32 	%r48, %r48, 1;
	setp.lt.s32 	%p14, %r48, %r26;
	@%p14 bra 	$L__BB11_3;

$L__BB11_11:
	ret;

}
	// .globl	add3
.visible .entry add3(
	.param .u64 add3_param_0,
	.param .u64 add3_param_1,
	.param .u32 add3_param_2,
	.param .u32 add3_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [add3_param_0];
	ld.param.u64 	%rd2, [add3_param_1];
	ld.param.u32 	%r4, [add3_param_2];
	ld.param.u32 	%r3, [add3_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB12_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd4, %r11, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	add.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

$L__BB12_2:
	ret;

}
	// .globl	add3_TYPE
.visible .entry add3_TYPE(
	.param .u64 add3_TYPE_param_0,
	.param .u64 add3_TYPE_param_1,
	.param .u32 add3_TYPE_param_2,
	.param .u32 add3_TYPE_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [add3_TYPE_param_0];
	ld.param.u64 	%rd3, [add3_TYPE_param_1];
	ld.param.u32 	%r5, [add3_TYPE_param_2];
	ld.param.u32 	%r4, [add3_TYPE_param_3];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r10, %r9, %r11;
	setp.ge.s32 	%p1, %r1, %r5;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB13_5;

	cvta.to.global.u64 	%rd4, %rd3;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	mul.wide.s32 	%rd5, %r12, 2;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.u16 	%rs4, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r2, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u16 	%rs5, [%rd8];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs5};}

	// end inline asm
	add.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r13, %f3;
	and.b32  	%r14, %r13, 2147483647;
	setp.gt.u32 	%p4, %r14, 2139095040;
	shl.b32 	%r15, %r13, 16;
	shr.u32 	%r16, %r13, 16;
	cvt.u16.u32 	%rs6, %r16;
	selp.b32 	%r3, 0, %r15, %p4;
	selp.b16 	%rs8, 32767, %rs6, %p4;
	setp.gt.u32 	%p5, %r3, -2147483648;
	@%p5 bra 	$L__BB13_3;

	setp.ne.s32 	%p6, %r3, -2147483648;
	and.b16  	%rs7, %rs8, 1;
	setp.eq.b16 	%p7, %rs7, 1;
	not.pred 	%p8, %p7;
	or.pred  	%p9, %p6, %p8;
	@%p9 bra 	$L__BB13_4;

$L__BB13_3:
	add.s16 	%rs8, %rs8, 1;

$L__BB13_4:
	st.global.u16 	[%rd1], %rs8;

$L__BB13_5:
	ret;

}
	// .globl	dotT_VectorAndMatrix
.visible .entry dotT_VectorAndMatrix(
	.param .u64 dotT_VectorAndMatrix_param_0,
	.param .u64 dotT_VectorAndMatrix_param_1,
	.param .u64 dotT_VectorAndMatrix_param_2,
	.param .u32 dotT_VectorAndMatrix_param_3,
	.param .u32 dotT_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd17, [dotT_VectorAndMatrix_param_0];
	ld.param.u64 	%rd18, [dotT_VectorAndMatrix_param_1];
	ld.param.u64 	%rd16, [dotT_VectorAndMatrix_param_2];
	ld.param.u32 	%r11, [dotT_VectorAndMatrix_param_3];
	ld.param.u32 	%r12, [dotT_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd17;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB14_9;

	ld.const.u16 	%rs1, [sh];
	// begin inline asm
	{ mov.b32 %f28, {0,%rs1};}

	// end inline asm
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB14_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB14_5;

	sub.s32 	%r21, %r11, %r23;
	mul.wide.s32 	%rd19, %r1, 4;
	add.s64 	%rd29, %rd1, %rd19;
	mul.wide.s32 	%rd4, %r12, 4;
	mov.u64 	%rd28, %rd2;

$L__BB14_4:
	ld.global.nc.f32 	%f11, [%rd29];
	ld.global.nc.f32 	%f12, [%rd28];
	fma.rn.ftz.f32 	%f13, %f12, %f11, %f28;
	add.s64 	%rd20, %rd29, %rd4;
	ld.global.nc.f32 	%f14, [%rd20];
	ld.global.nc.f32 	%f15, [%rd28+4];
	fma.rn.ftz.f32 	%f16, %f15, %f14, %f13;
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.nc.f32 	%f17, [%rd21];
	ld.global.nc.f32 	%f18, [%rd28+8];
	fma.rn.ftz.f32 	%f19, %f18, %f17, %f16;
	add.s64 	%rd22, %rd21, %rd4;
	add.s64 	%rd29, %rd22, %rd4;
	ld.global.nc.f32 	%f20, [%rd22];
	ld.global.nc.f32 	%f21, [%rd28+12];
	fma.rn.ftz.f32 	%f28, %f21, %f20, %f19;
	add.s32 	%r22, %r22, 4;
	add.s64 	%rd28, %rd28, 16;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB14_4;

$L__BB14_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB14_8;

	mul.wide.s32 	%rd23, %r22, 4;
	add.s64 	%rd31, %rd2, %rd23;
	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd24, %r19, 4;
	add.s64 	%rd30, %rd1, %rd24;
	mul.wide.s32 	%rd11, %r12, 4;

$L__BB14_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f22, [%rd30];
	ld.global.nc.f32 	%f23, [%rd31];
	fma.rn.ftz.f32 	%f28, %f23, %f22, %f28;
	add.s64 	%rd31, %rd31, 4;
	add.s64 	%rd30, %rd30, %rd11;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB14_7;

$L__BB14_8:
	cvta.to.global.u64 	%rd25, %rd16;
	mul.wide.s32 	%rd26, %r1, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f28;

$L__BB14_9:
	ret;

}
	// .globl	toHotVector
.visible .entry toHotVector(
	.param .u64 toHotVector_param_0,
	.param .u64 toHotVector_param_1,
	.param .u32 toHotVector_param_2,
	.param .u32 toHotVector_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [toHotVector_param_0];
	ld.param.u64 	%rd2, [toHotVector_param_1];
	ld.param.u32 	%r2, [toHotVector_param_2];
	ld.param.u32 	%r3, [toHotVector_param_3];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB15_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f1, [%rd5];
	cvt.rzi.ftz.s32.f32 	%r7, %f1;
	mad.lo.s32 	%r8, %r1, %r2, %r7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r8, 4;
	add.s64 	%rd8, %rd6, %rd7;
	mov.u32 	%r9, 1065353216;
	st.global.u32 	[%rd8], %r9;

$L__BB15_2:
	ret;

}
	// .globl	dotT_VectorAndMatrix_TYPE
.visible .entry dotT_VectorAndMatrix_TYPE(
	.param .u64 dotT_VectorAndMatrix_TYPE_param_0,
	.param .u64 dotT_VectorAndMatrix_TYPE_param_1,
	.param .u64 dotT_VectorAndMatrix_TYPE_param_2,
	.param .u32 dotT_VectorAndMatrix_TYPE_param_3,
	.param .u32 dotT_VectorAndMatrix_TYPE_param_4
)
{
	.reg .pred 	%p<37>;
	.reg .b16 	%rs<58>;
	.reg .f32 	%f<21>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd15, [dotT_VectorAndMatrix_TYPE_param_0];
	ld.param.u64 	%rd16, [dotT_VectorAndMatrix_TYPE_param_1];
	ld.param.u64 	%rd14, [dotT_VectorAndMatrix_TYPE_param_2];
	ld.param.u32 	%r20, [dotT_VectorAndMatrix_TYPE_param_3];
	ld.param.u32 	%r21, [dotT_VectorAndMatrix_TYPE_param_4];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %ntid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	setp.ge.s32 	%p1, %r1, %r21;
	@%p1 bra 	$L__BB16_24;

	ld.const.u16 	%rs55, [sh];
	setp.lt.s32 	%p2, %r20, 1;
	@%p2 bra 	$L__BB16_23;

	add.s32 	%r26, %r20, -1;
	and.b32  	%r53, %r20, 3;
	setp.lt.u32 	%p3, %r26, 3;
	mov.u32 	%r51, 0;
	@%p3 bra 	$L__BB16_17;

	shl.b32 	%r3, %r21, 2;
	sub.s32 	%r4, %r53, %r20;
	mul.wide.s32 	%rd3, %r21, 2;
	mov.u32 	%r49, %r1;
	mov.u64 	%rd25, %rd2;

$L__BB16_4:
	// begin inline asm
	{ mov.b32 %f1, {0,%rs55};}

	// end inline asm
	ld.global.nc.u16 	%rs24, [%rd25];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs24};}

	// end inline asm
	mul.wide.s32 	%rd17, %r49, 2;
	add.s64 	%rd6, %rd1, %rd17;
	ld.global.nc.u16 	%rs25, [%rd6];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs25};}

	// end inline asm
	fma.rn.ftz.f32 	%f4, %f2, %f3, %f1;
	mov.b32 	%r28, %f4;
	and.b32  	%r29, %r28, 2147483647;
	setp.gt.u32 	%p4, %r29, 2139095040;
	shl.b32 	%r30, %r28, 16;
	shr.u32 	%r31, %r28, 16;
	cvt.u16.u32 	%rs26, %r31;
	selp.b32 	%r7, 0, %r30, %p4;
	selp.b16 	%rs49, 32767, %rs26, %p4;
	setp.gt.u32 	%p5, %r7, -2147483648;
	@%p5 bra 	$L__BB16_6;

	setp.ne.s32 	%p6, %r7, -2147483648;
	and.b16  	%rs27, %rs49, 1;
	setp.eq.b16 	%p7, %rs27, 1;
	not.pred 	%p8, %p7;
	or.pred  	%p9, %p6, %p8;
	@%p9 bra 	$L__BB16_7;

$L__BB16_6:
	add.s16 	%rs49, %rs49, 1;

$L__BB16_7:
	// begin inline asm
	{ mov.b32 %f5, {0,%rs49};}

	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd25+2];
	// begin inline asm
	{ mov.b32 %f6, {0,%rs29};}

	// end inline asm
	add.s64 	%rd8, %rd6, %rd3;
	ld.global.nc.u16 	%rs30, [%rd8];
	// begin inline asm
	{ mov.b32 %f7, {0,%rs30};}

	// end inline asm
	fma.rn.ftz.f32 	%f8, %f6, %f7, %f5;
	mov.b32 	%r32, %f8;
	and.b32  	%r33, %r32, 2147483647;
	setp.gt.u32 	%p10, %r33, 2139095040;
	shl.b32 	%r34, %r32, 16;
	shr.u32 	%r35, %r32, 16;
	cvt.u16.u32 	%rs31, %r35;
	selp.b32 	%r8, 0, %r34, %p10;
	selp.b16 	%rs50, 32767, %rs31, %p10;
	setp.gt.u32 	%p11, %r8, -2147483648;
	@%p11 bra 	$L__BB16_9;

	setp.ne.s32 	%p12, %r8, -2147483648;
	and.b16  	%rs32, %rs50, 1;
	setp.eq.b16 	%p13, %rs32, 1;
	not.pred 	%p14, %p13;
	or.pred  	%p15, %p12, %p14;
	@%p15 bra 	$L__BB16_10;

$L__BB16_9:
	add.s16 	%rs50, %rs50, 1;

$L__BB16_10:
	// begin inline asm
	{ mov.b32 %f9, {0,%rs50};}

	// end inline asm
	ld.global.nc.u16 	%rs34, [%rd25+4];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs34};}

	// end inline asm
	add.s64 	%rd9, %rd8, %rd3;
	ld.global.nc.u16 	%rs35, [%rd9];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs35};}

	// end inline asm
	fma.rn.ftz.f32 	%f12, %f10, %f11, %f9;
	mov.b32 	%r36, %f12;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p16, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs36, %r39;
	selp.b32 	%r9, 0, %r38, %p16;
	selp.b16 	%rs51, 32767, %rs36, %p16;
	setp.gt.u32 	%p17, %r9, -2147483648;
	@%p17 bra 	$L__BB16_12;

	setp.ne.s32 	%p18, %r9, -2147483648;
	and.b16  	%rs37, %rs51, 1;
	setp.eq.b16 	%p19, %rs37, 1;
	not.pred 	%p20, %p19;
	or.pred  	%p21, %p18, %p20;
	@%p21 bra 	$L__BB16_13;

$L__BB16_12:
	add.s16 	%rs51, %rs51, 1;

$L__BB16_13:
	// begin inline asm
	{ mov.b32 %f13, {0,%rs51};}

	// end inline asm
	ld.global.nc.u16 	%rs39, [%rd25+6];
	// begin inline asm
	{ mov.b32 %f14, {0,%rs39};}

	// end inline asm
	add.s64 	%rd18, %rd9, %rd3;
	ld.global.nc.u16 	%rs40, [%rd18];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs40};}

	// end inline asm
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	mov.b32 	%r40, %f16;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs41, %r43;
	selp.b32 	%r10, 0, %r42, %p22;
	selp.b16 	%rs55, 32767, %rs41, %p22;
	setp.gt.u32 	%p23, %r10, -2147483648;
	@%p23 bra 	$L__BB16_15;

	setp.ne.s32 	%p24, %r10, -2147483648;
	and.b16  	%rs42, %rs55, 1;
	setp.eq.b16 	%p25, %rs42, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB16_16;

$L__BB16_15:
	add.s16 	%rs55, %rs55, 1;

$L__BB16_16:
	add.s64 	%rd25, %rd25, 8;
	add.s32 	%r49, %r49, %r3;
	add.s32 	%r51, %r51, 4;
	add.s32 	%r44, %r4, %r51;
	setp.ne.s32 	%p28, %r44, 0;
	@%p28 bra 	$L__BB16_4;

$L__BB16_17:
	setp.eq.s32 	%p29, %r53, 0;
	@%p29 bra 	$L__BB16_23;

	mul.wide.s32 	%rd19, %r51, 2;
	add.s64 	%rd26, %rd2, %rd19;
	mad.lo.s32 	%r52, %r51, %r21, %r1;

$L__BB16_19:
	.pragma "nounroll";
	// begin inline asm
	{ mov.b32 %f17, {0,%rs55};}

	// end inline asm
	ld.global.nc.u16 	%rs44, [%rd26];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs44};}

	// end inline asm
	mul.wide.s32 	%rd20, %r52, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.u16 	%rs45, [%rd21];
	// begin inline asm
	{ mov.b32 %f19, {0,%rs45};}

	// end inline asm
	fma.rn.ftz.f32 	%f20, %f18, %f19, %f17;
	mov.b32 	%r45, %f20;
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs46, %r48;
	selp.b32 	%r17, 0, %r47, %p30;
	selp.b16 	%rs55, 32767, %rs46, %p30;
	setp.gt.u32 	%p31, %r17, -2147483648;
	@%p31 bra 	$L__BB16_21;

	setp.ne.s32 	%p32, %r17, -2147483648;
	and.b16  	%rs47, %rs55, 1;
	setp.eq.b16 	%p33, %rs47, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB16_22;

$L__BB16_21:
	add.s16 	%rs55, %rs55, 1;

$L__BB16_22:
	add.s64 	%rd26, %rd26, 2;
	add.s32 	%r52, %r52, %r21;
	add.s32 	%r53, %r53, -1;
	setp.ne.s32 	%p36, %r53, 0;
	@%p36 bra 	$L__BB16_19;

$L__BB16_23:
	cvta.to.global.u64 	%rd22, %rd14;
	mul.wide.s32 	%rd23, %r1, 2;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u16 	[%rd24], %rs55;

$L__BB16_24:
	ret;

}
	// .globl	derivativeWeight
.visible .entry derivativeWeight(
	.param .u64 derivativeWeight_param_0,
	.param .u64 derivativeWeight_param_1,
	.param .u64 derivativeWeight_param_2,
	.param .u32 derivativeWeight_param_3,
	.param .u32 derivativeWeight_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [derivativeWeight_param_0];
	ld.param.u64 	%rd2, [derivativeWeight_param_1];
	ld.param.u64 	%rd3, [derivativeWeight_param_2];
	ld.param.u32 	%r4, [derivativeWeight_param_3];
	ld.param.u32 	%r3, [derivativeWeight_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB17_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd5, %r11, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r2, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.f32 	%f1, [%rd12];
	ld.global.nc.f32 	%f2, [%rd9];
	ld.global.f32 	%f3, [%rd6];
	fma.rn.ftz.f32 	%f4, %f2, %f1, %f3;
	st.global.f32 	[%rd6], %f4;

$L__BB17_2:
	ret;

}
	// .globl	derivativeWeight_TYPE
.visible .entry derivativeWeight_TYPE(
	.param .u64 derivativeWeight_TYPE_param_0,
	.param .u64 derivativeWeight_TYPE_param_1,
	.param .u64 derivativeWeight_TYPE_param_2,
	.param .u32 derivativeWeight_TYPE_param_3,
	.param .u32 derivativeWeight_TYPE_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [derivativeWeight_TYPE_param_0];
	ld.param.u64 	%rd3, [derivativeWeight_TYPE_param_1];
	ld.param.u64 	%rd4, [derivativeWeight_TYPE_param_2];
	ld.param.u32 	%r5, [derivativeWeight_TYPE_param_3];
	ld.param.u32 	%r4, [derivativeWeight_TYPE_param_4];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r2, %r10, %r9, %r11;
	setp.ge.s32 	%p1, %r1, %r5;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB18_5;

	cvta.to.global.u64 	%rd5, %rd4;
	mad.lo.s32 	%r12, %r1, %r4, %r2;
	mul.wide.s32 	%rd6, %r12, 2;
	add.s64 	%rd1, %rd5, %rd6;
	ld.global.u16 	%rs4, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.u16 	%rs5, [%rd9];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs5};}

	// end inline asm
	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.s32 	%rd11, %r2, 2;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.u16 	%rs6, [%rd12];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs6};}

	// end inline asm
	fma.rn.ftz.f32 	%f4, %f2, %f3, %f1;
	mov.b32 	%r13, %f4;
	and.b32  	%r14, %r13, 2147483647;
	setp.gt.u32 	%p4, %r14, 2139095040;
	shl.b32 	%r15, %r13, 16;
	shr.u32 	%r16, %r13, 16;
	cvt.u16.u32 	%rs7, %r16;
	selp.b32 	%r3, 0, %r15, %p4;
	selp.b16 	%rs9, 32767, %rs7, %p4;
	setp.gt.u32 	%p5, %r3, -2147483648;
	@%p5 bra 	$L__BB18_3;

	setp.ne.s32 	%p6, %r3, -2147483648;
	and.b16  	%rs8, %rs9, 1;
	setp.eq.b16 	%p7, %rs8, 1;
	not.pred 	%p8, %p7;
	or.pred  	%p9, %p6, %p8;
	@%p9 bra 	$L__BB18_4;

$L__BB18_3:
	add.s16 	%rs9, %rs9, 1;

$L__BB18_4:
	st.global.u16 	[%rd1], %rs9;

$L__BB18_5:
	ret;

}
	// .globl	addMatrix
.visible .entry addMatrix(
	.param .u64 addMatrix_param_0,
	.param .u64 addMatrix_param_1,
	.param .u32 addMatrix_param_2,
	.param .u32 addMatrix_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd13, [addMatrix_param_0];
	ld.param.u64 	%rd12, [addMatrix_param_1];
	ld.param.u32 	%r11, [addMatrix_param_2];
	ld.param.u32 	%r12, [addMatrix_param_3];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB19_9;

	cvta.to.global.u64 	%rd14, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd15, %r1, 4;
	add.s64 	%rd3, %rd14, %rd15;
	ld.global.f32 	%f22, [%rd3];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB19_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB19_5;

	sub.s32 	%r21, %r11, %r23;
	shl.b64 	%rd16, %rd2, 2;
	add.s64 	%rd21, %rd1, %rd16;
	mul.wide.s32 	%rd5, %r12, 4;

$L__BB19_4:
	ld.global.nc.f32 	%f10, [%rd21];
	add.ftz.f32 	%f11, %f22, %f10;
	add.s64 	%rd17, %rd21, %rd5;
	ld.global.nc.f32 	%f12, [%rd17];
	add.ftz.f32 	%f13, %f11, %f12;
	add.s64 	%rd18, %rd17, %rd5;
	ld.global.nc.f32 	%f14, [%rd18];
	add.ftz.f32 	%f15, %f13, %f14;
	add.s64 	%rd19, %rd18, %rd5;
	add.s64 	%rd21, %rd19, %rd5;
	ld.global.nc.f32 	%f16, [%rd19];
	add.ftz.f32 	%f22, %f15, %f16;
	add.s32 	%r22, %r22, 4;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB19_4;

$L__BB19_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB19_8;

	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd20, %r19, 4;
	add.s64 	%rd22, %rd1, %rd20;
	mul.wide.s32 	%rd9, %r12, 4;

$L__BB19_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f17, [%rd22];
	add.ftz.f32 	%f22, %f22, %f17;
	add.s64 	%rd22, %rd22, %rd9;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB19_7;

$L__BB19_8:
	st.global.f32 	[%rd3], %f22;

$L__BB19_9:
	ret;

}
	// .globl	addMatrix_TYPE
.visible .entry addMatrix_TYPE(
	.param .u64 addMatrix_TYPE_param_0,
	.param .u64 addMatrix_TYPE_param_1,
	.param .u32 addMatrix_TYPE_param_2,
	.param .u32 addMatrix_TYPE_param_3
)
{
	.reg .pred 	%p<37>;
	.reg .b16 	%rs<53>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<64>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd6, [addMatrix_TYPE_param_0];
	ld.param.u64 	%rd7, [addMatrix_TYPE_param_1];
	ld.param.u32 	%r19, [addMatrix_TYPE_param_2];
	ld.param.u32 	%r20, [addMatrix_TYPE_param_3];
	mov.u32 	%r21, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r59, %r21, %r22, %r23;
	setp.ge.s32 	%p1, %r59, %r20;
	@%p1 bra 	$L__BB20_24;

	cvta.to.global.u64 	%rd8, %rd7;
	mul.wide.s32 	%rd9, %r59, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u16 	%rs50, [%rd10];
	setp.lt.s32 	%p2, %r19, 1;
	@%p2 bra 	$L__BB20_23;

	add.s32 	%r25, %r19, -1;
	setp.lt.u32 	%p3, %r25, 3;
	mov.u32 	%r61, 0;
	@%p3 bra 	$L__BB20_17;

	and.b32  	%r27, %r19, 3;
	sub.s32 	%r2, %r27, %r19;
	mul.wide.s32 	%rd1, %r20, 2;
	cvta.to.global.u64 	%rd11, %rd6;

$L__BB20_4:
	// begin inline asm
	{ mov.b32 %f1, {0,%rs50};}

	// end inline asm
	mul.wide.s32 	%rd12, %r59, 2;
	add.s64 	%rd2, %rd11, %rd12;
	ld.global.nc.u16 	%rs24, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs24};}

	// end inline asm
	add.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r28, %f3;
	and.b32  	%r29, %r28, 2147483647;
	setp.gt.u32 	%p4, %r29, 2139095040;
	shl.b32 	%r30, %r28, 16;
	shr.u32 	%r31, %r28, 16;
	cvt.u16.u32 	%rs25, %r31;
	selp.b32 	%r5, 0, %r30, %p4;
	selp.b16 	%rs44, 32767, %rs25, %p4;
	setp.gt.u32 	%p5, %r5, -2147483648;
	@%p5 bra 	$L__BB20_6;

	setp.ne.s32 	%p6, %r5, -2147483648;
	and.b16  	%rs26, %rs44, 1;
	setp.eq.b16 	%p7, %rs26, 1;
	not.pred 	%p8, %p7;
	or.pred  	%p9, %p6, %p8;
	@%p9 bra 	$L__BB20_7;

$L__BB20_6:
	add.s16 	%rs44, %rs44, 1;

$L__BB20_7:
	// begin inline asm
	{ mov.b32 %f4, {0,%rs44};}

	// end inline asm
	add.s64 	%rd3, %rd2, %rd1;
	ld.global.nc.u16 	%rs28, [%rd3];
	// begin inline asm
	{ mov.b32 %f5, {0,%rs28};}

	// end inline asm
	add.ftz.f32 	%f6, %f4, %f5;
	mov.b32 	%r32, %f6;
	and.b32  	%r33, %r32, 2147483647;
	setp.gt.u32 	%p10, %r33, 2139095040;
	shl.b32 	%r34, %r32, 16;
	shr.u32 	%r35, %r32, 16;
	cvt.u16.u32 	%rs29, %r35;
	selp.b32 	%r6, 0, %r34, %p10;
	selp.b16 	%rs45, 32767, %rs29, %p10;
	setp.gt.u32 	%p11, %r6, -2147483648;
	@%p11 bra 	$L__BB20_9;

	setp.ne.s32 	%p12, %r6, -2147483648;
	and.b16  	%rs30, %rs45, 1;
	setp.eq.b16 	%p13, %rs30, 1;
	not.pred 	%p14, %p13;
	or.pred  	%p15, %p12, %p14;
	@%p15 bra 	$L__BB20_10;

$L__BB20_9:
	add.s16 	%rs45, %rs45, 1;

$L__BB20_10:
	// begin inline asm
	{ mov.b32 %f7, {0,%rs45};}

	// end inline asm
	add.s64 	%rd4, %rd3, %rd1;
	ld.global.nc.u16 	%rs32, [%rd4];
	// begin inline asm
	{ mov.b32 %f8, {0,%rs32};}

	// end inline asm
	add.ftz.f32 	%f9, %f7, %f8;
	mov.b32 	%r36, %f9;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p16, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs33, %r39;
	selp.b32 	%r7, 0, %r38, %p16;
	selp.b16 	%rs46, 32767, %rs33, %p16;
	setp.gt.u32 	%p17, %r7, -2147483648;
	@%p17 bra 	$L__BB20_12;

	setp.ne.s32 	%p18, %r7, -2147483648;
	and.b16  	%rs34, %rs46, 1;
	setp.eq.b16 	%p19, %rs34, 1;
	not.pred 	%p20, %p19;
	or.pred  	%p21, %p18, %p20;
	@%p21 bra 	$L__BB20_13;

$L__BB20_12:
	add.s16 	%rs46, %rs46, 1;

$L__BB20_13:
	// begin inline asm
	{ mov.b32 %f10, {0,%rs46};}

	// end inline asm
	add.s64 	%rd13, %rd4, %rd1;
	ld.global.nc.u16 	%rs36, [%rd13];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs36};}

	// end inline asm
	add.ftz.f32 	%f12, %f10, %f11;
	mov.b32 	%r40, %f12;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs37, %r43;
	selp.b32 	%r8, 0, %r42, %p22;
	selp.b16 	%rs50, 32767, %rs37, %p22;
	setp.gt.u32 	%p23, %r8, -2147483648;
	@%p23 bra 	$L__BB20_15;

	setp.ne.s32 	%p24, %r8, -2147483648;
	and.b16  	%rs38, %rs50, 1;
	setp.eq.b16 	%p25, %rs38, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB20_16;

$L__BB20_15:
	add.s16 	%rs50, %rs50, 1;

$L__BB20_16:
	shl.b32 	%r44, %r20, 2;
	add.s32 	%r59, %r59, %r44;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r45, %r2, %r61;
	setp.ne.s32 	%p28, %r45, 0;
	@%p28 bra 	$L__BB20_4;

$L__BB20_17:
	and.b32  	%r63, %r19, 3;
	setp.eq.s32 	%p29, %r63, 0;
	@%p29 bra 	$L__BB20_23;

	mad.lo.s32 	%r50, %r21, %r22, %r23;
	mad.lo.s32 	%r62, %r61, %r20, %r50;
	cvta.to.global.u64 	%rd5, %rd6;

$L__BB20_19:
	.pragma "nounroll";
	// begin inline asm
	{ mov.b32 %f13, {0,%rs50};}

	// end inline asm
	mul.wide.s32 	%rd14, %r62, 2;
	add.s64 	%rd15, %rd5, %rd14;
	ld.global.nc.u16 	%rs40, [%rd15];
	// begin inline asm
	{ mov.b32 %f14, {0,%rs40};}

	// end inline asm
	add.ftz.f32 	%f15, %f13, %f14;
	mov.b32 	%r51, %f15;
	and.b32  	%r52, %r51, 2147483647;
	setp.gt.u32 	%p30, %r52, 2139095040;
	shl.b32 	%r53, %r51, 16;
	shr.u32 	%r54, %r51, 16;
	cvt.u16.u32 	%rs41, %r54;
	selp.b32 	%r16, 0, %r53, %p30;
	selp.b16 	%rs50, 32767, %rs41, %p30;
	setp.gt.u32 	%p31, %r16, -2147483648;
	@%p31 bra 	$L__BB20_21;

	setp.ne.s32 	%p32, %r16, -2147483648;
	and.b16  	%rs42, %rs50, 1;
	setp.eq.b16 	%p33, %rs42, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB20_22;

$L__BB20_21:
	add.s16 	%rs50, %rs50, 1;

$L__BB20_22:
	add.s32 	%r62, %r62, %r20;
	add.s32 	%r63, %r63, -1;
	setp.ne.s32 	%p36, %r63, 0;
	@%p36 bra 	$L__BB20_19;

$L__BB20_23:
	mad.lo.s32 	%r58, %r21, %r22, %r23;
	mul.wide.s32 	%rd17, %r58, 2;
	add.s64 	%rd18, %rd8, %rd17;
	st.global.u16 	[%rd18], %rs50;

$L__BB20_24:
	ret;

}
	// .globl	reduceMaxIdxOptimizedShared
.visible .entry reduceMaxIdxOptimizedShared(
	.param .u64 reduceMaxIdxOptimizedShared_param_0,
	.param .u32 reduceMaxIdxOptimizedShared_param_1,
	.param .u64 reduceMaxIdxOptimizedShared_param_2,
	.param .u64 reduceMaxIdxOptimizedShared_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .f32 _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax;
	// demoted variable
	.shared .align 4 .u32 _ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx;

	ld.param.u64 	%rd2, [reduceMaxIdxOptimizedShared_param_0];
	ld.param.u32 	%r12, [reduceMaxIdxOptimizedShared_param_1];
	ld.param.u64 	%rd3, [reduceMaxIdxOptimizedShared_param_2];
	ld.param.u64 	%rd4, [reduceMaxIdxOptimizedShared_param_3];
	mov.u32 	%r18, %tid.x;
	setp.ne.s32 	%p1, %r18, 0;
	@%p1 bra 	$L__BB21_2;

	mov.u32 	%r13, 0;
	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax], %r13;
	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx], %r13;

$L__BB21_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r18, %r12;
	mov.u32 	%r20, 0;
	mov.f32 	%f12, 0f00000000;
	@%p2 bra 	$L__BB21_5;

	mov.u32 	%r2, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB21_4:
	mul.wide.s32 	%rd5, %r18, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f7, [%rd6];
	setp.lt.ftz.f32 	%p3, %f12, %f7;
	selp.f32 	%f12, %f7, %f12, %p3;
	selp.b32 	%r20, %r18, %r20, %p3;
	add.s32 	%r18, %r18, %r2;
	setp.lt.s32 	%p4, %r18, %r12;
	@%p4 bra 	$L__BB21_4;

$L__BB21_5:
	ld.shared.f32 	%f4, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	setp.ge.ftz.f32 	%p5, %f4, %f12;
	@%p5 bra 	$L__BB21_9;

	mov.b32 	%r21, %f4;
	mov.b32 	%r9, %f12;

$L__BB21_7:
	mov.b32 	%f8, %r21;
	setp.le.ftz.f32 	%p6, %f12, %f8;
	@%p6 bra 	$L__BB21_9;

	mov.u32 	%r16, _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax;
	atom.shared.cas.b32 	%r11, [%r16], %r21, %r9;
	setp.ne.s32 	%p7, %r21, %r11;
	mov.u32 	%r21, %r11;
	@%p7 bra 	$L__BB21_7;

$L__BB21_9:
	bar.sync 	0;
	ld.shared.f32 	%f9, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	setp.neu.ftz.f32 	%p8, %f9, %f12;
	@%p8 bra 	$L__BB21_11;

	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx], %r20;

$L__BB21_11:
	bar.sync 	0;
	@%p1 bra 	$L__BB21_13;

	ld.shared.f32 	%f10, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	cvta.to.global.u64 	%rd7, %rd3;
	st.global.f32 	[%rd7], %f10;
	ld.shared.u32 	%r17, [_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx];
	cvta.to.global.u64 	%rd8, %rd4;
	st.global.u32 	[%rd8], %r17;

$L__BB21_13:
	ret;

}
	// .globl	reduceMaxIdxOptimizedShared_TYPE
.visible .entry reduceMaxIdxOptimizedShared_TYPE(
	.param .u64 reduceMaxIdxOptimizedShared_TYPE_param_0,
	.param .u32 reduceMaxIdxOptimizedShared_TYPE_param_1,
	.param .u64 reduceMaxIdxOptimizedShared_TYPE_param_2,
	.param .u64 reduceMaxIdxOptimizedShared_TYPE_param_3
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<23>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax[2];
	// demoted variable
	.shared .align 4 .u32 _ZZ32reduceMaxIdxOptimizedShared_TYPEE12sharedMaxIdx;

	ld.param.u64 	%rd2, [reduceMaxIdxOptimizedShared_TYPE_param_0];
	ld.param.u32 	%r12, [reduceMaxIdxOptimizedShared_TYPE_param_1];
	ld.param.u64 	%rd3, [reduceMaxIdxOptimizedShared_TYPE_param_2];
	ld.param.u64 	%rd4, [reduceMaxIdxOptimizedShared_TYPE_param_3];
	mov.u32 	%r22, %tid.x;
	setp.ne.s32 	%p1, %r22, 0;
	ld.const.u16 	%rs21, [sh];
	@%p1 bra 	$L__BB22_2;

	mov.u32 	%r13, 0;
	st.shared.u16 	[_ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax], %rs21;
	st.shared.u32 	[_ZZ32reduceMaxIdxOptimizedShared_TYPEE12sharedMaxIdx], %r13;

$L__BB22_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r22, %r12;
	mov.u32 	%r24, 0;
	@%p2 bra 	$L__BB22_5;

	mov.u32 	%r2, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB22_4:
	mul.wide.s32 	%rd5, %r22, 2;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.u16 	%rs9, [%rd6];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs21};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs9};}

	// end inline asm
	setp.lt.ftz.f32 	%p3, %f1, %f2;
	selp.b16 	%rs21, %rs9, %rs21, %p3;
	selp.b32 	%r24, %r22, %r24, %p3;
	add.s32 	%r22, %r22, %r2;
	setp.lt.s32 	%p4, %r22, %r12;
	@%p4 bra 	$L__BB22_4;

$L__BB22_5:
	ld.shared.u16 	%rs10, [_ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs10};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f4, {0,%rs21};}

	// end inline asm
	setp.ge.ftz.f32 	%p5, %f3, %f4;
	@%p5 bra 	$L__BB22_12;

	ld.shared.u32 	%r25, [_ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax];
	// begin inline asm
	{ mov.b32 %f6, {0,%rs21};}

	// end inline asm

$L__BB22_7:
	and.b32  	%r16, %r25, 2147483647;
	setp.gt.u32 	%p6, %r16, 2139095040;
	shr.u32 	%r17, %r25, 16;
	cvt.u16.u32 	%rs12, %r17;
	shl.b32 	%r18, %r25, 16;
	selp.b32 	%r10, 0, %r18, %p6;
	selp.b16 	%rs22, 32767, %rs12, %p6;
	setp.gt.u32 	%p7, %r10, -2147483648;
	@%p7 bra 	$L__BB22_9;

	setp.ne.s32 	%p8, %r10, -2147483648;
	and.b16  	%rs13, %rs22, 1;
	setp.eq.b16 	%p9, %rs13, 1;
	not.pred 	%p10, %p9;
	or.pred  	%p11, %p8, %p10;
	@%p11 bra 	$L__BB22_10;

$L__BB22_9:
	add.s16 	%rs22, %rs22, 1;

$L__BB22_10:
	// begin inline asm
	{ mov.b32 %f5, {0,%rs22};}

	// end inline asm
	setp.ge.ftz.f32 	%p12, %f5, %f6;
	@%p12 bra 	$L__BB22_12;

	mov.b32 	%r19, %f6;
	mov.u32 	%r20, _ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax;
	atom.shared.cas.b32 	%r11, [%r20], %r25, %r19;
	setp.ne.s32 	%p13, %r25, %r11;
	mov.u32 	%r25, %r11;
	@%p13 bra 	$L__BB22_7;

$L__BB22_12:
	bar.sync 	0;
	ld.shared.u16 	%rs17, [_ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax];
	// begin inline asm
	{ mov.b32 %f8, {0,%rs17};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f9, {0,%rs21};}

	// end inline asm
	setp.neu.ftz.f32 	%p14, %f8, %f9;
	@%p14 bra 	$L__BB22_14;

	st.shared.u32 	[_ZZ32reduceMaxIdxOptimizedShared_TYPEE12sharedMaxIdx], %r24;

$L__BB22_14:
	bar.sync 	0;
	@%p1 bra 	$L__BB22_16;

	ld.shared.u16 	%rs19, [_ZZ32reduceMaxIdxOptimizedShared_TYPEE9sharedMax];
	cvta.to.global.u64 	%rd7, %rd3;
	st.global.u16 	[%rd7], %rs19;
	ld.shared.u32 	%r21, [_ZZ32reduceMaxIdxOptimizedShared_TYPEE12sharedMaxIdx];
	cvta.to.global.u64 	%rd8, %rd4;
	st.global.u32 	[%rd8], %r21;

$L__BB22_16:
	ret;

}
	// .globl	reverse
.visible .entry reverse(
	.param .u64 reverse_param_0,
	.param .u32 reverse_param_1,
	.param .u32 reverse_param_2,
	.param .u32 reverse_param_3
)
{



	ret;

}
	// .globl	sharedMem_transpose
.visible .entry sharedMem_transpose(
	.param .u64 sharedMem_transpose_param_0,
	.param .u64 sharedMem_transpose_param_1,
	.param .u32 sharedMem_transpose_param_2,
	.param .u32 sharedMem_transpose_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ19sharedMem_transposeE8M_Shared[4096];

	ld.param.u64 	%rd1, [sharedMem_transpose_param_0];
	ld.param.u64 	%rd2, [sharedMem_transpose_param_1];
	ld.param.u32 	%r5, [sharedMem_transpose_param_2];
	ld.param.u32 	%r6, [sharedMem_transpose_param_3];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	shl.b32 	%r11, %r10, 5;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r2, %r11, %r12;
	setp.lt.s32 	%p2, %r1, %r5;
	setp.lt.s32 	%p3, %r2, %r6;
	and.pred  	%p1, %p2, %p3;
	shl.b32 	%r13, %r12, 7;
	mov.u32 	%r14, _ZZ19sharedMem_transposeE8M_Shared;
	add.s32 	%r15, %r14, %r13;
	shl.b32 	%r16, %r9, 2;
	add.s32 	%r3, %r15, %r16;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB24_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r17, %r1, %r6, %r2;
	mul.wide.s32 	%rd4, %r17, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	st.shared.f32 	[%r3], %f1;

$L__BB24_2:
	bar.sync 	0;
	mad.lo.s32 	%r4, %r2, %r5, %r1;
	@%p4 bra 	$L__BB24_4;

	ld.shared.f32 	%f2, [%r3];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f2;

$L__BB24_4:
	ret;

}
	// .globl	sharedMem_transpose_TYPE
.visible .entry sharedMem_transpose_TYPE(
	.param .u64 sharedMem_transpose_TYPE_param_0,
	.param .u64 sharedMem_transpose_TYPE_param_1,
	.param .u32 sharedMem_transpose_TYPE_param_2,
	.param .u32 sharedMem_transpose_TYPE_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ24sharedMem_transpose_TYPEE8M_Shared[2048];

	ld.param.u64 	%rd1, [sharedMem_transpose_TYPE_param_0];
	ld.param.u64 	%rd2, [sharedMem_transpose_TYPE_param_1];
	ld.param.u32 	%r5, [sharedMem_transpose_TYPE_param_2];
	ld.param.u32 	%r6, [sharedMem_transpose_TYPE_param_3];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	shl.b32 	%r11, %r10, 5;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r2, %r11, %r12;
	setp.lt.s32 	%p2, %r1, %r5;
	setp.lt.s32 	%p3, %r2, %r6;
	and.pred  	%p1, %p2, %p3;
	shl.b32 	%r13, %r12, 6;
	mov.u32 	%r14, _ZZ24sharedMem_transpose_TYPEE8M_Shared;
	add.s32 	%r15, %r14, %r13;
	shl.b32 	%r16, %r9, 1;
	add.s32 	%r3, %r15, %r16;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB25_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r17, %r1, %r6, %r2;
	mul.wide.s32 	%rd4, %r17, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	st.shared.u16 	[%r3], %rs1;

$L__BB25_2:
	bar.sync 	0;
	mad.lo.s32 	%r4, %r2, %r5, %r1;
	@%p4 bra 	$L__BB25_4;

	ld.shared.u16 	%rs2, [%r3];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r4, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB25_4:
	ret;

}
	// .globl	matrixDiv
.visible .entry matrixDiv(
	.param .u64 matrixDiv_param_0,
	.param .align 2 .b8 matrixDiv_param_1[2],
	.param .u32 matrixDiv_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs4, [matrixDiv_param_1];
	ld.param.u64 	%rd2, [matrixDiv_param_0];
	ld.param.u32 	%r3, [matrixDiv_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB26_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs5, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs5};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs4};}

	// end inline asm
	div.approx.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r7, %f3;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs7, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs9, 32767, %rs7, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB26_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs8, %rs9, 1;
	setp.eq.b16 	%p5, %rs8, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB26_4;

$L__BB26_3:
	add.s16 	%rs9, %rs9, 1;

$L__BB26_4:
	st.global.u16 	[%rd1], %rs9;

$L__BB26_5:
	ret;

}
	// .globl	matrixDiv_float
.visible .entry matrixDiv_float(
	.param .u64 matrixDiv_float_param_0,
	.param .f32 matrixDiv_float_param_1,
	.param .u32 matrixDiv_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [matrixDiv_float_param_0];
	ld.param.f32 	%f1, [matrixDiv_float_param_1];
	ld.param.u32 	%r2, [matrixDiv_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB27_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	div.approx.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB27_2:
	ret;

}
	// .globl	addCopy
.visible .entry addCopy(
	.param .u64 addCopy_param_0,
	.param .u64 addCopy_param_1,
	.param .u32 addCopy_param_2,
	.param .u32 addCopy_param_3,
	.param .u32 addCopy_param_4,
	.param .u32 addCopy_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addCopy_param_0];
	ld.param.u64 	%rd2, [addCopy_param_1];
	ld.param.u32 	%r6, [addCopy_param_2];
	ld.param.u32 	%r3, [addCopy_param_3];
	ld.param.u32 	%r4, [addCopy_param_4];
	ld.param.u32 	%r5, [addCopy_param_5];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB28_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r15, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB28_2:
	ret;

}
	// .globl	addCopy_TYPE
.visible .entry addCopy_TYPE(
	.param .u64 addCopy_TYPE_param_0,
	.param .u64 addCopy_TYPE_param_1,
	.param .u32 addCopy_TYPE_param_2,
	.param .u32 addCopy_TYPE_param_3,
	.param .u32 addCopy_TYPE_param_4,
	.param .u32 addCopy_TYPE_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addCopy_TYPE_param_0];
	ld.param.u64 	%rd2, [addCopy_TYPE_param_1];
	ld.param.u32 	%r6, [addCopy_TYPE_param_2];
	ld.param.u32 	%r3, [addCopy_TYPE_param_3];
	ld.param.u32 	%r4, [addCopy_TYPE_param_4];
	ld.param.u32 	%r5, [addCopy_TYPE_param_5];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB29_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r15, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r14, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB29_2:
	ret;

}
	// .globl	NormalizationLayerForward2D
.visible .entry NormalizationLayerForward2D(
	.param .u64 NormalizationLayerForward2D_param_0,
	.param .u64 NormalizationLayerForward2D_param_1,
	.param .u64 NormalizationLayerForward2D_param_2,
	.param .u32 NormalizationLayerForward2D_param_3,
	.param .u32 NormalizationLayerForward2D_param_4,
	.param .u32 NormalizationLayerForward2D_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<99>;


	ld.param.u64 	%rd45, [NormalizationLayerForward2D_param_0];
	ld.param.u64 	%rd46, [NormalizationLayerForward2D_param_1];
	ld.param.u64 	%rd47, [NormalizationLayerForward2D_param_2];
	ld.param.u32 	%r35, [NormalizationLayerForward2D_param_3];
	ld.param.u32 	%r34, [NormalizationLayerForward2D_param_4];
	ld.param.u32 	%r36, [NormalizationLayerForward2D_param_5];
	cvta.to.global.u64 	%rd1, %rd47;
	cvta.to.global.u64 	%rd2, %rd46;
	cvta.to.global.u64 	%rd3, %rd45;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r38, %r37, %r39;
	mov.u32 	%r40, %ctaid.y;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r42, %tid.y;
	mad.lo.s32 	%r2, %r41, %r40, %r42;
	setp.ge.s32 	%p1, %r1, %r36;
	setp.ge.s32 	%p2, %r2, %r35;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB30_22;

	ld.global.u64 	%rd48, [%rd3];
	cvta.to.global.u64 	%rd49, %rd48;
	cvt.s64.s32 	%rd4, %r1;
	mul.wide.s32 	%rd50, %r1, 8;
	add.s64 	%rd51, %rd49, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	cvta.to.global.u64 	%rd5, %rd52;
	ld.global.u64 	%rd53, [%rd3+8];
	cvta.to.global.u64 	%rd54, %rd53;
	add.s64 	%rd55, %rd54, %rd50;
	ld.global.u64 	%rd56, [%rd55];
	cvta.to.global.u64 	%rd57, %rd56;
	cvt.s64.s32 	%rd6, %r2;
	mul.wide.s32 	%rd58, %r2, 4;
	add.s64 	%rd7, %rd57, %rd58;
	ld.global.f32 	%f80, [%rd7];
	mul.lo.s32 	%r60, %r2, %r34;
	setp.lt.s32 	%p4, %r34, 1;
	@%p4 bra 	$L__BB30_8;

	add.s32 	%r43, %r34, -1;
	and.b32  	%r51, %r34, 3;
	setp.lt.u32 	%p5, %r43, 3;
	mov.u32 	%r50, %r60;
	@%p5 bra 	$L__BB30_5;

	sub.s32 	%r49, %r34, %r51;
	mul.wide.s32 	%rd59, %r60, 4;
	add.s64 	%rd60, %rd5, %rd59;
	add.s64 	%rd87, %rd60, 8;
	mov.u32 	%r50, %r60;

$L__BB30_4:
	ld.global.f32 	%f21, [%rd87+-8];
	add.ftz.f32 	%f22, %f80, %f21;
	ld.global.f32 	%f23, [%rd87+-4];
	add.ftz.f32 	%f24, %f22, %f23;
	ld.global.f32 	%f25, [%rd87];
	add.ftz.f32 	%f26, %f24, %f25;
	ld.global.f32 	%f27, [%rd87+4];
	add.ftz.f32 	%f80, %f26, %f27;
	add.s32 	%r50, %r50, 4;
	add.s64 	%rd87, %rd87, 16;
	add.s32 	%r49, %r49, -4;
	setp.ne.s32 	%p6, %r49, 0;
	@%p6 bra 	$L__BB30_4;

$L__BB30_5:
	setp.eq.s32 	%p7, %r51, 0;
	@%p7 bra 	$L__BB30_8;

	mul.wide.s32 	%rd61, %r50, 4;
	add.s64 	%rd88, %rd5, %rd61;

$L__BB30_7:
	.pragma "nounroll";
	ld.global.f32 	%f28, [%rd88];
	add.ftz.f32 	%f80, %f80, %f28;
	add.s64 	%rd88, %rd88, 4;
	add.s32 	%r51, %r51, -1;
	setp.ne.s32 	%p8, %r51, 0;
	@%p8 bra 	$L__BB30_7;

$L__BB30_8:
	cvt.rn.f32.s32 	%f9, %r34;
	div.approx.ftz.f32 	%f29, %f80, %f9;
	st.global.f32 	[%rd7], %f29;
	ld.global.u64 	%rd62, [%rd3+16];
	cvta.to.global.u64 	%rd63, %rd62;
	shl.b64 	%rd64, %rd4, 3;
	add.s64 	%rd65, %rd63, %rd64;
	ld.global.u64 	%rd66, [%rd65];
	cvta.to.global.u64 	%rd67, %rd66;
	shl.b64 	%rd68, %rd6, 2;
	add.s64 	%rd15, %rd67, %rd68;
	ld.global.f32 	%f85, [%rd15];
	ld.global.u64 	%rd69, [%rd3+8];
	cvta.to.global.u64 	%rd70, %rd69;
	add.s64 	%rd71, %rd70, %rd64;
	ld.global.u64 	%rd72, [%rd71];
	cvta.to.global.u64 	%rd73, %rd72;
	add.s64 	%rd74, %rd73, %rd68;
	ld.global.f32 	%f11, [%rd74];
	@%p4 bra 	$L__BB30_15;

	add.s32 	%r44, %r34, -1;
	and.b32  	%r55, %r34, 3;
	setp.lt.u32 	%p10, %r44, 3;
	mov.u32 	%r54, %r60;
	@%p10 bra 	$L__BB30_12;

	sub.s32 	%r53, %r34, %r55;
	mul.wide.s32 	%rd75, %r60, 4;
	add.s64 	%rd76, %rd5, %rd75;
	add.s64 	%rd89, %rd76, 8;
	mov.u32 	%r54, %r60;

$L__BB30_11:
	ld.global.f32 	%f31, [%rd89+-8];
	sub.ftz.f32 	%f32, %f31, %f11;
	fma.rn.ftz.f32 	%f33, %f32, %f32, %f85;
	ld.global.f32 	%f34, [%rd89+-4];
	sub.ftz.f32 	%f35, %f34, %f11;
	fma.rn.ftz.f32 	%f36, %f35, %f35, %f33;
	ld.global.f32 	%f37, [%rd89];
	sub.ftz.f32 	%f38, %f37, %f11;
	fma.rn.ftz.f32 	%f39, %f38, %f38, %f36;
	ld.global.f32 	%f40, [%rd89+4];
	sub.ftz.f32 	%f41, %f40, %f11;
	fma.rn.ftz.f32 	%f85, %f41, %f41, %f39;
	add.s32 	%r54, %r54, 4;
	add.s64 	%rd89, %rd89, 16;
	add.s32 	%r53, %r53, -4;
	setp.ne.s32 	%p11, %r53, 0;
	@%p11 bra 	$L__BB30_11;

$L__BB30_12:
	setp.eq.s32 	%p12, %r55, 0;
	@%p12 bra 	$L__BB30_15;

	mul.wide.s32 	%rd77, %r54, 4;
	add.s64 	%rd90, %rd5, %rd77;

$L__BB30_14:
	.pragma "nounroll";
	ld.global.f32 	%f42, [%rd90];
	sub.ftz.f32 	%f43, %f42, %f11;
	fma.rn.ftz.f32 	%f85, %f43, %f43, %f85;
	add.s64 	%rd90, %rd90, 4;
	add.s32 	%r55, %r55, -1;
	setp.ne.s32 	%p13, %r55, 0;
	@%p13 bra 	$L__BB30_14;

$L__BB30_15:
	div.approx.ftz.f32 	%f44, %f85, %f9;
	st.global.f32 	[%rd15], %f44;
	add.ftz.f32 	%f45, %f44, 0f3A83126F;
	sqrt.approx.ftz.f32 	%f19, %f45;
	ld.global.u64 	%rd78, [%rd3+24];
	cvta.to.global.u64 	%rd79, %rd78;
	add.s64 	%rd81, %rd79, %rd64;
	ld.global.u64 	%rd82, [%rd81];
	cvta.to.global.u64 	%rd22, %rd82;
	@%p4 bra 	$L__BB30_22;

	add.s32 	%r46, %r34, -1;
	and.b32  	%r61, %r34, 3;
	setp.lt.u32 	%p15, %r46, 3;
	mov.u32 	%r59, 0;
	@%p15 bra 	$L__BB30_19;

	sub.s32 	%r58, %r34, %r61;
	mul.wide.s32 	%rd83, %r60, 4;
	add.s64 	%rd84, %rd83, 8;
	add.s64 	%rd94, %rd5, %rd84;
	add.s64 	%rd93, %rd22, %rd84;
	mov.u64 	%rd91, %rd2;
	mov.u64 	%rd92, %rd1;

$L__BB30_18:
	ld.global.f32 	%f46, [%rd94+-8];
	sub.ftz.f32 	%f47, %f46, %f11;
	div.approx.ftz.f32 	%f48, %f47, %f19;
	ld.global.nc.f32 	%f49, [%rd91];
	ld.global.nc.f32 	%f50, [%rd92];
	fma.rn.ftz.f32 	%f51, %f48, %f49, %f50;
	st.global.f32 	[%rd93+-8], %f51;
	ld.global.f32 	%f52, [%rd94+-4];
	sub.ftz.f32 	%f53, %f52, %f11;
	div.approx.ftz.f32 	%f54, %f53, %f19;
	ld.global.nc.f32 	%f55, [%rd91+4];
	ld.global.nc.f32 	%f56, [%rd92+4];
	fma.rn.ftz.f32 	%f57, %f54, %f55, %f56;
	st.global.f32 	[%rd93+-4], %f57;
	ld.global.f32 	%f58, [%rd94];
	sub.ftz.f32 	%f59, %f58, %f11;
	div.approx.ftz.f32 	%f60, %f59, %f19;
	ld.global.nc.f32 	%f61, [%rd91+8];
	ld.global.nc.f32 	%f62, [%rd92+8];
	fma.rn.ftz.f32 	%f63, %f60, %f61, %f62;
	st.global.f32 	[%rd93], %f63;
	ld.global.f32 	%f64, [%rd94+4];
	sub.ftz.f32 	%f65, %f64, %f11;
	div.approx.ftz.f32 	%f66, %f65, %f19;
	ld.global.nc.f32 	%f67, [%rd91+12];
	ld.global.nc.f32 	%f68, [%rd92+12];
	fma.rn.ftz.f32 	%f69, %f66, %f67, %f68;
	st.global.f32 	[%rd93+4], %f69;
	add.s32 	%r59, %r59, 4;
	add.s32 	%r60, %r60, 4;
	add.s64 	%rd94, %rd94, 16;
	add.s64 	%rd93, %rd93, 16;
	add.s64 	%rd92, %rd92, 16;
	add.s64 	%rd91, %rd91, 16;
	add.s32 	%r58, %r58, -4;
	setp.ne.s32 	%p16, %r58, 0;
	@%p16 bra 	$L__BB30_18;

$L__BB30_19:
	setp.eq.s32 	%p17, %r61, 0;
	@%p17 bra 	$L__BB30_22;

	mul.wide.s32 	%rd85, %r59, 4;
	add.s64 	%rd98, %rd1, %rd85;
	add.s64 	%rd97, %rd2, %rd85;
	mul.wide.s32 	%rd86, %r60, 4;
	add.s64 	%rd96, %rd22, %rd86;
	add.s64 	%rd95, %rd5, %rd86;

$L__BB30_21:
	.pragma "nounroll";
	ld.global.f32 	%f70, [%rd95];
	sub.ftz.f32 	%f71, %f70, %f11;
	div.approx.ftz.f32 	%f72, %f71, %f19;
	ld.global.nc.f32 	%f73, [%rd97];
	ld.global.nc.f32 	%f74, [%rd98];
	fma.rn.ftz.f32 	%f75, %f72, %f73, %f74;
	st.global.f32 	[%rd96], %f75;
	add.s64 	%rd98, %rd98, 4;
	add.s64 	%rd97, %rd97, 4;
	add.s64 	%rd96, %rd96, 4;
	add.s64 	%rd95, %rd95, 4;
	add.s32 	%r61, %r61, -1;
	setp.ne.s32 	%p18, %r61, 0;
	@%p18 bra 	$L__BB30_21;

$L__BB30_22:
	ret;

}
	// .globl	NormalizationLayerBackward2D
.visible .entry NormalizationLayerBackward2D(
	.param .u64 NormalizationLayerBackward2D_param_0,
	.param .u64 NormalizationLayerBackward2D_param_1,
	.param .u64 NormalizationLayerBackward2D_param_2,
	.param .u64 NormalizationLayerBackward2D_param_3,
	.param .u64 NormalizationLayerBackward2D_param_4,
	.param .u32 NormalizationLayerBackward2D_param_5,
	.param .u32 NormalizationLayerBackward2D_param_6,
	.param .u32 NormalizationLayerBackward2D_param_7,
	.param .u32 NormalizationLayerBackward2D_param_8,
	.param .u32 NormalizationLayerBackward2D_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<189>;


	ld.param.u64 	%rd98, [NormalizationLayerBackward2D_param_0];
	ld.param.u64 	%rd100, [NormalizationLayerBackward2D_param_1];
	ld.param.u64 	%rd99, [NormalizationLayerBackward2D_param_2];
	ld.param.u64 	%rd101, [NormalizationLayerBackward2D_param_3];
	ld.param.u64 	%rd102, [NormalizationLayerBackward2D_param_4];
	ld.param.u32 	%r53, [NormalizationLayerBackward2D_param_7];
	ld.param.u32 	%r52, [NormalizationLayerBackward2D_param_8];
	ld.param.u32 	%r54, [NormalizationLayerBackward2D_param_9];
	cvta.to.global.u64 	%rd1, %rd101;
	cvta.to.global.u64 	%rd2, %rd100;
	cvta.to.global.u64 	%rd3, %rd102;
	mov.u32 	%r55, %ctaid.x;
	mov.u32 	%r56, %ntid.x;
	mov.u32 	%r57, %tid.x;
	mad.lo.s32 	%r1, %r56, %r55, %r57;
	mov.u32 	%r58, %ctaid.y;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r60, %tid.y;
	mad.lo.s32 	%r2, %r59, %r58, %r60;
	setp.ge.s32 	%p1, %r1, %r54;
	setp.ge.s32 	%p2, %r2, %r53;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB31_28;

	cvta.to.global.u64 	%rd103, %rd98;
	ld.global.u64 	%rd104, [%rd103];
	cvta.to.global.u64 	%rd105, %rd104;
	mul.wide.s32 	%rd106, %r1, 8;
	add.s64 	%rd107, %rd105, %rd106;
	ld.global.u64 	%rd108, [%rd107];
	cvta.to.global.u64 	%rd4, %rd108;
	ld.global.u64 	%rd109, [%rd103+8];
	cvta.to.global.u64 	%rd110, %rd109;
	add.s64 	%rd111, %rd110, %rd106;
	ld.global.u64 	%rd112, [%rd111];
	cvta.to.global.u64 	%rd113, %rd112;
	mul.wide.s32 	%rd114, %r2, 4;
	add.s64 	%rd115, %rd113, %rd114;
	ld.global.u64 	%rd116, [%rd103+16];
	cvta.to.global.u64 	%rd117, %rd116;
	add.s64 	%rd118, %rd117, %rd106;
	ld.global.u64 	%rd119, [%rd118];
	cvta.to.global.u64 	%rd5, %rd119;
	ld.global.u64 	%rd120, [%rd103+24];
	cvta.to.global.u64 	%rd121, %rd120;
	add.s64 	%rd122, %rd121, %rd106;
	ld.global.u64 	%rd123, [%rd122];
	cvta.to.global.u64 	%rd124, %rd123;
	add.s64 	%rd125, %rd124, %rd114;
	ld.global.f32 	%f1, [%rd125];
	ld.global.u64 	%rd126, [%rd103+32];
	cvta.to.global.u64 	%rd127, %rd126;
	add.s64 	%rd128, %rd127, %rd106;
	ld.global.u64 	%rd129, [%rd128];
	cvta.to.global.u64 	%rd6, %rd129;
	ld.global.u64 	%rd130, [%rd103+40];
	cvta.to.global.u64 	%rd131, %rd130;
	add.s64 	%rd132, %rd131, %rd106;
	ld.global.u64 	%rd133, [%rd132];
	cvta.to.global.u64 	%rd7, %rd133;
	ld.global.f32 	%f2, [%rd115];
	mul.lo.s32 	%r93, %r2, %r52;
	setp.lt.s32 	%p4, %r52, 1;
	mov.f32 	%f211, 0f00000000;
	mov.f32 	%f202, %f211;
	@%p4 bra 	$L__BB31_8;

	add.s32 	%r62, %r52, -1;
	and.b32  	%r76, %r52, 3;
	setp.lt.u32 	%p5, %r62, 3;
	mov.f32 	%f202, 0f00000000;
	mov.u32 	%r74, 0;
	mov.u32 	%r75, %r93;
	@%p5 bra 	$L__BB31_5;

	sub.s32 	%r73, %r52, %r76;
	mul.wide.s32 	%rd134, %r93, 4;
	add.s64 	%rd135, %rd134, 8;
	add.s64 	%rd159, %rd4, %rd135;
	add.s64 	%rd158, %rd5, %rd135;
	mov.u64 	%rd157, %rd2;
	mov.u32 	%r75, %r93;

$L__BB31_4:
	ld.global.nc.f32 	%f31, [%rd157];
	ld.global.f32 	%f32, [%rd159+-8];
	mul.ftz.f32 	%f33, %f32, %f31;
	ld.global.f32 	%f34, [%rd158+-8];
	sub.ftz.f32 	%f35, %f34, %f1;
	fma.rn.ftz.f32 	%f36, %f33, %f35, %f202;
	ld.global.nc.f32 	%f37, [%rd157+4];
	ld.global.f32 	%f38, [%rd159+-4];
	mul.ftz.f32 	%f39, %f38, %f37;
	ld.global.f32 	%f40, [%rd158+-4];
	sub.ftz.f32 	%f41, %f40, %f1;
	fma.rn.ftz.f32 	%f42, %f39, %f41, %f36;
	ld.global.nc.f32 	%f43, [%rd157+8];
	ld.global.f32 	%f44, [%rd159];
	mul.ftz.f32 	%f45, %f44, %f43;
	ld.global.f32 	%f46, [%rd158];
	sub.ftz.f32 	%f47, %f46, %f1;
	fma.rn.ftz.f32 	%f48, %f45, %f47, %f42;
	ld.global.nc.f32 	%f49, [%rd157+12];
	ld.global.f32 	%f50, [%rd159+4];
	mul.ftz.f32 	%f51, %f50, %f49;
	ld.global.f32 	%f52, [%rd158+4];
	sub.ftz.f32 	%f53, %f52, %f1;
	fma.rn.ftz.f32 	%f202, %f51, %f53, %f48;
	add.s32 	%r74, %r74, 4;
	add.s32 	%r75, %r75, 4;
	add.s64 	%rd159, %rd159, 16;
	add.s64 	%rd158, %rd158, 16;
	add.s64 	%rd157, %rd157, 16;
	add.s32 	%r73, %r73, -4;
	setp.ne.s32 	%p6, %r73, 0;
	@%p6 bra 	$L__BB31_4;

$L__BB31_5:
	setp.eq.s32 	%p7, %r76, 0;
	@%p7 bra 	$L__BB31_8;

	mul.wide.s32 	%rd136, %r74, 4;
	add.s64 	%rd162, %rd2, %rd136;
	mul.wide.s32 	%rd137, %r75, 4;
	add.s64 	%rd161, %rd5, %rd137;
	add.s64 	%rd160, %rd4, %rd137;

$L__BB31_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f54, [%rd162];
	ld.global.f32 	%f55, [%rd160];
	mul.ftz.f32 	%f56, %f55, %f54;
	ld.global.f32 	%f57, [%rd161];
	sub.ftz.f32 	%f58, %f57, %f1;
	fma.rn.ftz.f32 	%f202, %f56, %f58, %f202;
	add.s64 	%rd162, %rd162, 4;
	add.s64 	%rd161, %rd161, 4;
	add.s64 	%rd160, %rd160, 4;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p8, %r76, 0;
	@%p8 bra 	$L__BB31_7;

$L__BB31_8:
	mov.f32 	%f212, %f211;
	@%p4 bra 	$L__BB31_15;

	add.s32 	%r65, %r52, -1;
	and.b32  	%r82, %r52, 3;
	setp.lt.u32 	%p10, %r65, 3;
	mov.f32 	%f212, 0f00000000;
	mov.u32 	%r80, 0;
	mov.u32 	%r81, %r93;
	mov.f32 	%f211, %f212;
	@%p10 bra 	$L__BB31_12;

	sub.s32 	%r79, %r52, %r82;
	mul.wide.s32 	%rd138, %r93, 4;
	add.s64 	%rd139, %rd138, 8;
	add.s64 	%rd165, %rd4, %rd139;
	add.s64 	%rd164, %rd5, %rd139;
	mov.u64 	%rd163, %rd2;
	mov.u32 	%r81, %r93;

$L__BB31_11:
	ld.global.nc.f32 	%f66, [%rd163];
	ld.global.f32 	%f67, [%rd165+-8];
	fma.rn.ftz.f32 	%f68, %f67, %f66, %f212;
	ld.global.f32 	%f69, [%rd164+-8];
	sub.ftz.f32 	%f70, %f69, %f1;
	add.ftz.f32 	%f71, %f211, %f70;
	ld.global.nc.f32 	%f72, [%rd163+4];
	ld.global.f32 	%f73, [%rd165+-4];
	fma.rn.ftz.f32 	%f74, %f73, %f72, %f68;
	ld.global.f32 	%f75, [%rd164+-4];
	sub.ftz.f32 	%f76, %f75, %f1;
	add.ftz.f32 	%f77, %f71, %f76;
	ld.global.nc.f32 	%f78, [%rd163+8];
	ld.global.f32 	%f79, [%rd165];
	fma.rn.ftz.f32 	%f80, %f79, %f78, %f74;
	ld.global.f32 	%f81, [%rd164];
	sub.ftz.f32 	%f82, %f81, %f1;
	add.ftz.f32 	%f83, %f77, %f82;
	ld.global.nc.f32 	%f84, [%rd163+12];
	ld.global.f32 	%f85, [%rd165+4];
	fma.rn.ftz.f32 	%f212, %f85, %f84, %f80;
	ld.global.f32 	%f86, [%rd164+4];
	sub.ftz.f32 	%f87, %f86, %f1;
	add.ftz.f32 	%f211, %f83, %f87;
	add.s32 	%r80, %r80, 4;
	add.s32 	%r81, %r81, 4;
	add.s64 	%rd165, %rd165, 16;
	add.s64 	%rd164, %rd164, 16;
	add.s64 	%rd163, %rd163, 16;
	add.s32 	%r79, %r79, -4;
	setp.ne.s32 	%p11, %r79, 0;
	@%p11 bra 	$L__BB31_11;

$L__BB31_12:
	setp.eq.s32 	%p12, %r82, 0;
	@%p12 bra 	$L__BB31_15;

	mul.wide.s32 	%rd140, %r80, 4;
	add.s64 	%rd168, %rd2, %rd140;
	mul.wide.s32 	%rd141, %r81, 4;
	add.s64 	%rd167, %rd5, %rd141;
	add.s64 	%rd166, %rd4, %rd141;

$L__BB31_14:
	.pragma "nounroll";
	ld.global.nc.f32 	%f88, [%rd168];
	ld.global.f32 	%f89, [%rd166];
	fma.rn.ftz.f32 	%f212, %f89, %f88, %f212;
	ld.global.f32 	%f90, [%rd167];
	sub.ftz.f32 	%f91, %f90, %f1;
	add.ftz.f32 	%f211, %f211, %f91;
	add.s64 	%rd168, %rd168, 4;
	add.s64 	%rd167, %rd167, 4;
	add.s64 	%rd166, %rd166, 4;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p13, %r82, 0;
	@%p13 bra 	$L__BB31_14;

$L__BB31_15:
	add.ftz.f32 	%f92, %f2, 0f3A83126F;
	lg2.approx.ftz.f32 	%f93, %f92;
	mul.ftz.f32 	%f94, %f93, 0fBFC00000;
	ex2.approx.ftz.f32 	%f95, %f94;
	mul.ftz.f32 	%f96, %f95, 0fBF000000;
	mul.ftz.f32 	%f97, %f96, %f202;
	sqrt.approx.ftz.f32 	%f98, %f92;
	mul.ftz.f32 	%f99, %f97, 0fC0000000;
	mul.ftz.f32 	%f100, %f99, %f211;
	cvt.rn.f32.s32 	%f101, %r52;
	div.approx.ftz.f32 	%f102, %f100, %f101;
	mov.f32 	%f103, 0fBF800000;
	div.approx.ftz.f32 	%f104, %f103, %f98;
	fma.rn.ftz.f32 	%f105, %f104, %f212, %f102;
	div.approx.ftz.f32 	%f24, %f105, %f101;
	mov.f32 	%f106, 0f40000000;
	div.approx.ftz.f32 	%f107, %f106, %f101;
	mul.ftz.f32 	%f25, %f97, %f107;
	rsqrt.approx.ftz.f32 	%f26, %f92;
	@%p4 bra 	$L__BB31_28;

	add.s32 	%r28, %r52, -1;
	and.b32  	%r94, %r52, 3;
	setp.lt.u32 	%p15, %r28, 3;
	mov.u32 	%r86, 0;
	mov.u32 	%r87, %r93;
	@%p15 bra 	$L__BB31_19;

	sub.s32 	%r85, %r52, %r94;
	mul.wide.s32 	%rd142, %r93, 4;
	add.s64 	%rd143, %rd142, 8;
	add.s64 	%rd172, %rd4, %rd143;
	add.s64 	%rd171, %rd5, %rd143;
	add.s64 	%rd169, %rd6, %rd143;
	mov.u64 	%rd170, %rd2;
	mov.u32 	%r87, %r93;

$L__BB31_18:
	ld.global.nc.f32 	%f108, [%rd170];
	ld.global.f32 	%f109, [%rd172+-8];
	mul.ftz.f32 	%f110, %f109, %f108;
	ld.global.f32 	%f111, [%rd171+-8];
	sub.ftz.f32 	%f112, %f111, %f1;
	mul.ftz.f32 	%f113, %f25, %f112;
	fma.rn.ftz.f32 	%f114, %f26, %f110, %f113;
	add.ftz.f32 	%f115, %f24, %f114;
	st.global.f32 	[%rd169+-8], %f115;
	ld.global.nc.f32 	%f116, [%rd170+4];
	ld.global.f32 	%f117, [%rd172+-4];
	mul.ftz.f32 	%f118, %f117, %f116;
	ld.global.f32 	%f119, [%rd171+-4];
	sub.ftz.f32 	%f120, %f119, %f1;
	mul.ftz.f32 	%f121, %f25, %f120;
	fma.rn.ftz.f32 	%f122, %f26, %f118, %f121;
	add.ftz.f32 	%f123, %f24, %f122;
	st.global.f32 	[%rd169+-4], %f123;
	ld.global.nc.f32 	%f124, [%rd170+8];
	ld.global.f32 	%f125, [%rd172];
	mul.ftz.f32 	%f126, %f125, %f124;
	ld.global.f32 	%f127, [%rd171];
	sub.ftz.f32 	%f128, %f127, %f1;
	mul.ftz.f32 	%f129, %f25, %f128;
	fma.rn.ftz.f32 	%f130, %f26, %f126, %f129;
	add.ftz.f32 	%f131, %f24, %f130;
	st.global.f32 	[%rd169], %f131;
	ld.global.nc.f32 	%f132, [%rd170+12];
	ld.global.f32 	%f133, [%rd172+4];
	mul.ftz.f32 	%f134, %f133, %f132;
	ld.global.f32 	%f135, [%rd171+4];
	sub.ftz.f32 	%f136, %f135, %f1;
	mul.ftz.f32 	%f137, %f25, %f136;
	fma.rn.ftz.f32 	%f138, %f26, %f134, %f137;
	add.ftz.f32 	%f139, %f24, %f138;
	st.global.f32 	[%rd169+4], %f139;
	add.s32 	%r86, %r86, 4;
	add.s32 	%r87, %r87, 4;
	add.s64 	%rd172, %rd172, 16;
	add.s64 	%rd171, %rd171, 16;
	add.s64 	%rd170, %rd170, 16;
	add.s64 	%rd169, %rd169, 16;
	add.s32 	%r85, %r85, -4;
	setp.ne.s32 	%p16, %r85, 0;
	@%p16 bra 	$L__BB31_18;

$L__BB31_19:
	setp.eq.s32 	%p17, %r94, 0;
	@%p17 bra 	$L__BB31_22;

	mul.wide.s32 	%rd144, %r86, 4;
	add.s64 	%rd176, %rd2, %rd144;
	mul.wide.s32 	%rd145, %r87, 4;
	add.s64 	%rd175, %rd6, %rd145;
	add.s64 	%rd174, %rd5, %rd145;
	add.s64 	%rd173, %rd4, %rd145;
	mov.u32 	%r88, %r94;

$L__BB31_21:
	.pragma "nounroll";
	ld.global.nc.f32 	%f140, [%rd176];
	ld.global.f32 	%f141, [%rd173];
	mul.ftz.f32 	%f142, %f141, %f140;
	ld.global.f32 	%f143, [%rd174];
	sub.ftz.f32 	%f144, %f143, %f1;
	mul.ftz.f32 	%f145, %f25, %f144;
	fma.rn.ftz.f32 	%f146, %f26, %f142, %f145;
	add.ftz.f32 	%f147, %f24, %f146;
	st.global.f32 	[%rd175], %f147;
	add.s64 	%rd176, %rd176, 4;
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, 4;
	add.s64 	%rd173, %rd173, 4;
	add.s32 	%r88, %r88, -1;
	setp.ne.s32 	%p18, %r88, 0;
	@%p18 bra 	$L__BB31_21;

$L__BB31_22:
	mov.u32 	%r92, 0;
	@%p15 bra 	$L__BB31_25;

	sub.s32 	%r91, %r52, %r94;
	mul.wide.s32 	%rd146, %r93, 4;
	add.s64 	%rd147, %rd146, 8;
	add.s64 	%rd182, %rd4, %rd147;
	add.s64 	%rd181, %rd7, %rd147;
	cvta.to.global.u64 	%rd178, %rd99;
	mov.u64 	%rd177, %rd1;
	mov.u64 	%rd179, %rd3;
	mov.u64 	%rd180, %rd2;

$L__BB31_24:
	ld.global.f32 	%f148, [%rd182+-8];
	atom.global.add.f32 	%f149, [%rd179], %f148;
	ld.global.nc.f32 	%f150, [%rd178];
	ld.global.f32 	%f151, [%rd181+-8];
	sub.ftz.f32 	%f152, %f151, %f150;
	ld.global.nc.f32 	%f153, [%rd180];
	div.approx.ftz.f32 	%f154, %f152, %f153;
	ld.global.f32 	%f155, [%rd182+-8];
	mul.ftz.f32 	%f156, %f155, %f154;
	atom.global.add.f32 	%f157, [%rd177], %f156;
	ld.global.f32 	%f158, [%rd182+-4];
	add.s64 	%rd148, %rd179, 4;
	atom.global.add.f32 	%f159, [%rd148], %f158;
	ld.global.nc.f32 	%f160, [%rd178+4];
	ld.global.f32 	%f161, [%rd181+-4];
	sub.ftz.f32 	%f162, %f161, %f160;
	ld.global.nc.f32 	%f163, [%rd180+4];
	div.approx.ftz.f32 	%f164, %f162, %f163;
	ld.global.f32 	%f165, [%rd182+-4];
	mul.ftz.f32 	%f166, %f165, %f164;
	add.s64 	%rd149, %rd177, 4;
	atom.global.add.f32 	%f167, [%rd149], %f166;
	ld.global.f32 	%f168, [%rd182];
	add.s64 	%rd150, %rd179, 8;
	atom.global.add.f32 	%f169, [%rd150], %f168;
	ld.global.nc.f32 	%f170, [%rd178+8];
	ld.global.f32 	%f171, [%rd181];
	sub.ftz.f32 	%f172, %f171, %f170;
	ld.global.nc.f32 	%f173, [%rd180+8];
	div.approx.ftz.f32 	%f174, %f172, %f173;
	ld.global.f32 	%f175, [%rd182];
	mul.ftz.f32 	%f176, %f175, %f174;
	add.s64 	%rd151, %rd177, 8;
	atom.global.add.f32 	%f177, [%rd151], %f176;
	ld.global.f32 	%f178, [%rd182+4];
	add.s64 	%rd152, %rd179, 12;
	atom.global.add.f32 	%f179, [%rd152], %f178;
	ld.global.nc.f32 	%f180, [%rd178+12];
	ld.global.f32 	%f181, [%rd181+4];
	sub.ftz.f32 	%f182, %f181, %f180;
	ld.global.nc.f32 	%f183, [%rd180+12];
	div.approx.ftz.f32 	%f184, %f182, %f183;
	ld.global.f32 	%f185, [%rd182+4];
	mul.ftz.f32 	%f186, %f185, %f184;
	add.s64 	%rd153, %rd177, 12;
	atom.global.add.f32 	%f187, [%rd153], %f186;
	add.s32 	%r92, %r92, 4;
	add.s32 	%r93, %r93, 4;
	add.s64 	%rd182, %rd182, 16;
	add.s64 	%rd181, %rd181, 16;
	add.s64 	%rd180, %rd180, 16;
	add.s64 	%rd179, %rd179, 16;
	add.s64 	%rd178, %rd178, 16;
	add.s64 	%rd177, %rd177, 16;
	add.s32 	%r91, %r91, -4;
	setp.ne.s32 	%p20, %r91, 0;
	@%p20 bra 	$L__BB31_24;

$L__BB31_25:
	@%p17 bra 	$L__BB31_28;

	mul.wide.s32 	%rd154, %r92, 4;
	add.s64 	%rd188, %rd2, %rd154;
	cvta.to.global.u64 	%rd155, %rd99;
	add.s64 	%rd187, %rd155, %rd154;
	add.s64 	%rd186, %rd1, %rd154;
	add.s64 	%rd185, %rd3, %rd154;
	mul.wide.s32 	%rd156, %r93, 4;
	add.s64 	%rd184, %rd7, %rd156;
	add.s64 	%rd183, %rd4, %rd156;

$L__BB31_27:
	.pragma "nounroll";
	ld.global.f32 	%f188, [%rd183];
	atom.global.add.f32 	%f189, [%rd185], %f188;
	ld.global.nc.f32 	%f190, [%rd187];
	ld.global.f32 	%f191, [%rd184];
	sub.ftz.f32 	%f192, %f191, %f190;
	ld.global.nc.f32 	%f193, [%rd188];
	div.approx.ftz.f32 	%f194, %f192, %f193;
	ld.global.f32 	%f195, [%rd183];
	mul.ftz.f32 	%f196, %f195, %f194;
	atom.global.add.f32 	%f197, [%rd186], %f196;
	add.s64 	%rd188, %rd188, 4;
	add.s64 	%rd187, %rd187, 4;
	add.s64 	%rd186, %rd186, 4;
	add.s64 	%rd185, %rd185, 4;
	add.s64 	%rd184, %rd184, 4;
	add.s64 	%rd183, %rd183, 4;
	add.s32 	%r94, %r94, -1;
	setp.ne.s32 	%p22, %r94, 0;
	@%p22 bra 	$L__BB31_27;

$L__BB31_28:
	ret;

}
	// .globl	dropout
.visible .entry dropout(
	.param .u64 dropout_param_0,
	.param .u64 dropout_param_1,
	.param .align 2 .b8 dropout_param_2[2],
	.param .u32 dropout_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<22>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs8, [dropout_param_2];
	ld.param.u64 	%rd3, [dropout_param_0];
	ld.param.u64 	%rd4, [dropout_param_1];
	ld.param.u32 	%r4, [dropout_param_3];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	setp.ge.s32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB32_9;

	ld.const.u16 	%rs9, [sh+8];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs9};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs9};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f3, {0,%rs8};}

	// end inline asm
	sub.ftz.f32 	%f4, %f2, %f3;
	div.approx.ftz.f32 	%f5, %f1, %f4;
	mov.b32 	%r8, %f5;
	and.b32  	%r9, %r8, 2147483647;
	setp.gt.u32 	%p2, %r9, 2139095040;
	shl.b32 	%r10, %r8, 16;
	shr.u32 	%r11, %r8, 16;
	cvt.u16.u32 	%rs12, %r11;
	selp.b32 	%r2, 0, %r10, %p2;
	selp.b16 	%rs20, 32767, %rs12, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB32_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs13, %rs20, 1;
	setp.eq.b16 	%p5, %rs13, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB32_4;

$L__BB32_3:
	add.s16 	%rs20, %rs20, 1;

$L__BB32_4:
	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd5, %rd4;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u16 	%rs14, [%rd7];
	// begin inline asm
	{ mov.b32 %f6, {0,%rs14};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f7, {0,%rs8};}

	// end inline asm
	setp.leu.ftz.f32 	%p8, %f6, %f7;
	@%p8 bra 	$L__BB32_9;

	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd2, %rd8, %rd9;
	ld.global.u16 	%rs16, [%rd2];
	// begin inline asm
	{ mov.b32 %f8, {0,%rs16};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f9, {0,%rs20};}

	// end inline asm
	mul.ftz.f32 	%f10, %f8, %f9;
	mov.b32 	%r12, %f10;
	and.b32  	%r13, %r12, 2147483647;
	setp.gt.u32 	%p9, %r13, 2139095040;
	shl.b32 	%r14, %r12, 16;
	shr.u32 	%r15, %r12, 16;
	cvt.u16.u32 	%rs18, %r15;
	selp.b32 	%r3, 0, %r14, %p9;
	selp.b16 	%rs21, 32767, %rs18, %p9;
	setp.gt.u32 	%p10, %r3, -2147483648;
	@%p10 bra 	$L__BB32_7;

	setp.ne.s32 	%p11, %r3, -2147483648;
	and.b16  	%rs19, %rs21, 1;
	setp.eq.b16 	%p12, %rs19, 1;
	not.pred 	%p13, %p12;
	or.pred  	%p14, %p11, %p13;
	@%p14 bra 	$L__BB32_8;

$L__BB32_7:
	add.s16 	%rs21, %rs21, 1;

$L__BB32_8:
	st.global.u16 	[%rd2], %rs21;

$L__BB32_9:
	ret;

}
	// .globl	sub_gpu
.visible .entry sub_gpu(
	.param .u64 sub_gpu_param_0,
	.param .u64 sub_gpu_param_1,
	.param .u64 sub_gpu_param_2,
	.param .u32 sub_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_param_0];
	ld.param.u64 	%rd2, [sub_gpu_param_1];
	ld.param.u64 	%rd3, [sub_gpu_param_2];
	ld.param.u32 	%r2, [sub_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB33_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.nc.f32 	%f1, [%rd8];
	ld.global.nc.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

$L__BB33_2:
	ret;

}
	// .globl	sub_gpu_TYPE
.visible .entry sub_gpu_TYPE(
	.param .u64 sub_gpu_TYPE_param_0,
	.param .u64 sub_gpu_TYPE_param_1,
	.param .u64 sub_gpu_TYPE_param_2,
	.param .u32 sub_gpu_TYPE_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [sub_gpu_TYPE_param_0];
	ld.param.u64 	%rd3, [sub_gpu_TYPE_param_1];
	ld.param.u64 	%rd4, [sub_gpu_TYPE_param_2];
	ld.param.u32 	%r3, [sub_gpu_TYPE_param_3];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB34_5;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs4, [%rd7];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.nc.u16 	%rs5, [%rd9];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs5};}

	// end inline asm
	sub.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r7, %f3;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs6, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs8, 32767, %rs6, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB34_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs7, %rs8, 1;
	setp.eq.b16 	%p5, %rs7, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB34_4;

$L__BB34_3:
	add.s16 	%rs8, %rs8, 1;

$L__BB34_4:
	cvta.to.global.u64 	%rd10, %rd4;
	shl.b64 	%rd11, %rd1, 1;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u16 	[%rd12], %rs8;

$L__BB34_5:
	ret;

}
	// .globl	sub_bfloatAndFloat
.visible .entry sub_bfloatAndFloat(
	.param .u64 sub_bfloatAndFloat_param_0,
	.param .u64 sub_bfloatAndFloat_param_1,
	.param .u64 sub_bfloatAndFloat_param_2,
	.param .u32 sub_bfloatAndFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [sub_bfloatAndFloat_param_0];
	ld.param.u64 	%rd2, [sub_bfloatAndFloat_param_1];
	ld.param.u64 	%rd3, [sub_bfloatAndFloat_param_2];
	ld.param.u32 	%r2, [sub_bfloatAndFloat_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB35_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.f32 	%f2, [%rd9];
	sub.ftz.f32 	%f3, %f1, %f2;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd8;
	st.global.f32 	[%rd11], %f3;

$L__BB35_2:
	ret;

}
	// .globl	sub_floatAndbFloat
.visible .entry sub_floatAndbFloat(
	.param .u64 sub_floatAndbFloat_param_0,
	.param .u64 sub_floatAndbFloat_param_1,
	.param .u64 sub_floatAndbFloat_param_2,
	.param .u32 sub_floatAndbFloat_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [sub_floatAndbFloat_param_0];
	ld.param.u64 	%rd2, [sub_floatAndbFloat_param_1];
	ld.param.u64 	%rd3, [sub_floatAndbFloat_param_2];
	ld.param.u32 	%r2, [sub_floatAndbFloat_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB36_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.u16 	%rs1, [%rd9];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	sub.ftz.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f32 	[%rd11], %f3;

$L__BB36_2:
	ret;

}
	// .globl	mul
.visible .entry mul(
	.param .u64 mul_param_0,
	.param .align 2 .b8 mul_param_1[2],
	.param .u32 mul_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs4, [mul_param_1];
	ld.param.u64 	%rd2, [mul_param_0];
	ld.param.u32 	%r3, [mul_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB37_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs5, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs5};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs4};}

	// end inline asm
	mul.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r7, %f3;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs7, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs9, 32767, %rs7, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB37_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs8, %rs9, 1;
	setp.eq.b16 	%p5, %rs8, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB37_4;

$L__BB37_3:
	add.s16 	%rs9, %rs9, 1;

$L__BB37_4:
	st.global.u16 	[%rd1], %rs9;

$L__BB37_5:
	ret;

}
	// .globl	mul_float
.visible .entry mul_float(
	.param .u64 mul_float_param_0,
	.param .f32 mul_float_param_1,
	.param .u32 mul_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [mul_float_param_0];
	ld.param.f32 	%f1, [mul_float_param_1];
	ld.param.u32 	%r2, [mul_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB38_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	mul.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB38_2:
	ret;

}
	// .globl	clip
.visible .entry clip(
	.param .u64 clip_param_0,
	.param .f32 clip_param_1,
	.param .f32 clip_param_2,
	.param .u32 clip_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [clip_param_0];
	ld.param.f32 	%f2, [clip_param_1];
	ld.param.f32 	%f3, [clip_param_2];
	ld.param.u32 	%r2, [clip_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB39_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd1];
	setp.gt.ftz.f32 	%p2, %f1, %f3;
	@%p2 bra 	$L__BB39_4;
	bra.uni 	$L__BB39_2;

$L__BB39_4:
	st.global.f32 	[%rd1], %f3;
	bra.uni 	$L__BB39_5;

$L__BB39_2:
	setp.geu.ftz.f32 	%p3, %f1, %f2;
	@%p3 bra 	$L__BB39_5;

	st.global.f32 	[%rd1], %f2;

$L__BB39_5:
	ret;

}
	// .globl	clip_TYPE
.visible .entry clip_TYPE(
	.param .u64 clip_TYPE_param_0,
	.param .align 2 .b8 clip_TYPE_param_1[2],
	.param .align 2 .b8 clip_TYPE_param_2[2],
	.param .u32 clip_TYPE_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs5, [clip_TYPE_param_2];
	ld.param.u16 	%rs4, [clip_TYPE_param_1];
	ld.param.u64 	%rd2, [clip_TYPE_param_0];
	ld.param.u32 	%r2, [clip_TYPE_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB40_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs2};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs5};}

	// end inline asm
	setp.gt.ftz.f32 	%p2, %f1, %f2;
	@%p2 bra 	$L__BB40_4;
	bra.uni 	$L__BB40_2;

$L__BB40_4:
	st.global.u16 	[%rd1], %rs5;
	bra.uni 	$L__BB40_5;

$L__BB40_2:
	// begin inline asm
	{ mov.b32 %f3, {0,%rs2};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f4, {0,%rs4};}

	// end inline asm
	setp.geu.ftz.f32 	%p3, %f3, %f4;
	@%p3 bra 	$L__BB40_5;

	st.global.u16 	[%rd1], %rs4;

$L__BB40_5:
	ret;

}
	// .globl	pow2
.visible .entry pow2(
	.param .u64 pow2_param_0,
	.param .u32 pow2_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [pow2_param_0];
	ld.param.u32 	%r2, [pow2_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB41_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	mul.ftz.f32 	%f2, %f1, %f1;
	st.global.f32 	[%rd4], %f2;

$L__BB41_2:
	ret;

}
	// .globl	pow2_TYPE
.visible .entry pow2_TYPE(
	.param .u64 pow2_TYPE_param_0,
	.param .u32 pow2_TYPE_param_1
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [pow2_TYPE_param_0];
	ld.param.u32 	%r3, [pow2_TYPE_param_1];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB42_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd1];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs4};}

	// end inline asm
	mul.ftz.f32 	%f3, %f1, %f2;
	mov.b32 	%r7, %f3;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs6, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs8, 32767, %rs6, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB42_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs7, %rs8, 1;
	setp.eq.b16 	%p5, %rs7, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB42_4;

$L__BB42_3:
	add.s16 	%rs8, %rs8, 1;

$L__BB42_4:
	st.global.u16 	[%rd1], %rs8;

$L__BB42_5:
	ret;

}
	// .globl	subAbs
.visible .entry subAbs(
	.param .u64 subAbs_param_0,
	.param .u64 subAbs_param_1,
	.param .u64 subAbs_param_2,
	.param .u32 subAbs_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subAbs_param_0];
	ld.param.u64 	%rd2, [subAbs_param_1];
	ld.param.u64 	%rd3, [subAbs_param_2];
	ld.param.u32 	%r2, [subAbs_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB43_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	abs.ftz.f32 	%f4, %f3;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f4;

$L__BB43_2:
	ret;

}
	// .globl	sum
.visible .entry sum(
	.param .u64 sum_param_0,
	.param .u64 sum_param_1,
	.param .u32 sum_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [sum_param_0];
	ld.param.u64 	%rd2, [sum_param_1];
	ld.param.u32 	%r2, [sum_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB44_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	atom.global.add.f32 	%f2, [%rd6], %f1;

$L__BB44_2:
	ret;

}
	// .globl	derAbs
.visible .entry derAbs(
	.param .u64 derAbs_param_0,
	.param .u64 derAbs_param_1,
	.param .u64 derAbs_param_2,
	.param .u32 derAbs_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [derAbs_param_0];
	ld.param.u64 	%rd2, [derAbs_param_1];
	ld.param.u64 	%rd3, [derAbs_param_2];
	ld.param.u32 	%r2, [derAbs_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB45_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	add.ftz.f32 	%f4, %f3, 0f322BCC77;
	abs.ftz.f32 	%f5, %f4;
	div.approx.ftz.f32 	%f6, %f3, %f5;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f6;

$L__BB45_2:
	ret;

}
	// .globl	fisnan
.visible .entry fisnan(
	.param .u64 fisnan_param_0,
	.param .u64 fisnan_param_1,
	.param .u32 fisnan_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [fisnan_param_0];
	ld.param.u64 	%rd3, [fisnan_param_1];
	ld.param.u32 	%r2, [fisnan_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB46_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB46_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	abs.ftz.f32 	%f2, %f1;
	setp.le.ftz.f32 	%p3, %f2, 0f7F800000;
	@%p3 bra 	$L__BB46_4;

	st.global.u32 	[%rd1], %r1;

$L__BB46_4:
	ret;

}
	// .globl	fisnan_float
.visible .entry fisnan_float(
	.param .u64 fisnan_float_param_0,
	.param .u64 fisnan_float_param_1,
	.param .u32 fisnan_float_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [fisnan_float_param_0];
	ld.param.u64 	%rd3, [fisnan_float_param_1];
	ld.param.u32 	%r2, [fisnan_float_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB47_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB47_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	cvt.ftz.f64.f32 	%fd1, %f1;
	abs.f64 	%fd2, %fd1;
	setp.le.f64 	%p3, %fd2, 0d7FF0000000000000;
	@%p3 bra 	$L__BB47_4;

	st.global.u32 	[%rd1], %r1;

$L__BB47_4:
	ret;

}
	// .globl	hisinf
.visible .entry hisinf(
	.param .u64 hisinf_param_0,
	.param .u64 hisinf_param_1,
	.param .u32 hisinf_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [hisinf_param_0];
	ld.param.u64 	%rd3, [hisinf_param_1];
	ld.param.u32 	%r2, [hisinf_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB48_5;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB48_5;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	abs.ftz.f32 	%f2, %f1;
	setp.neu.ftz.f32 	%p3, %f2, 0f7F800000;
	@%p3 bra 	$L__BB48_5;

	st.global.u32 	[%rd1], %r1;

$L__BB48_5:
	ret;

}
	// .globl	hisinf_float
.visible .entry hisinf_float(
	.param .u64 hisinf_float_param_0,
	.param .u64 hisinf_float_param_1,
	.param .u32 hisinf_float_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<10>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [hisinf_float_param_0];
	ld.param.u64 	%rd3, [hisinf_float_param_1];
	ld.param.u32 	%r2, [hisinf_float_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB49_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB49_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	cvt.ftz.f64.f32 	%fd1, %f1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd1;
	}
	and.b32  	%r9, %r8, 2147483647;
	setp.ne.s32 	%p3, %r9, 2146435072;
	setp.ne.s32 	%p4, %r7, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB49_4;

	st.global.u32 	[%rd1], %r1;

$L__BB49_4:
	ret;

}
	// .globl	momentum
.visible .entry momentum(
	.param .u64 momentum_param_0,
	.param .u64 momentum_param_1,
	.param .align 2 .b8 momentum_param_2[2],
	.param .align 2 .b8 momentum_param_3[2],
	.param .u32 momentum_param_4
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs5, [momentum_param_3];
	ld.param.u16 	%rs4, [momentum_param_2];
	ld.param.u64 	%rd2, [momentum_param_0];
	ld.param.u64 	%rd3, [momentum_param_1];
	ld.param.u32 	%r3, [momentum_param_4];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB50_5;

	cvta.to.global.u64 	%rd4, %rd2;
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.u16 	%rs7, [%rd1];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs7};}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs8, [%rd7];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs8};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f4, {0,%rs5};}

	// end inline asm
	mul.ftz.f32 	%f5, %f3, %f4;
	fma.rn.ftz.f32 	%f6, %f1, %f2, %f5;
	mov.b32 	%r7, %f6;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs10, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs12, 32767, %rs10, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB50_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs11, %rs12, 1;
	setp.eq.b16 	%p5, %rs11, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB50_4;

$L__BB50_3:
	add.s16 	%rs12, %rs12, 1;

$L__BB50_4:
	st.global.u16 	[%rd1], %rs12;

$L__BB50_5:
	ret;

}
	// .globl	momentum_float
.visible .entry momentum_float(
	.param .u64 momentum_float_param_0,
	.param .u64 momentum_float_param_1,
	.param .f32 momentum_float_param_2,
	.param .f32 momentum_float_param_3,
	.param .u32 momentum_float_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [momentum_float_param_0];
	ld.param.u64 	%rd2, [momentum_float_param_1];
	ld.param.f32 	%f1, [momentum_float_param_2];
	ld.param.f32 	%f2, [momentum_float_param_3];
	ld.param.u32 	%r2, [momentum_float_param_4];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB51_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f3, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f4, [%rd7];
	mul.ftz.f32 	%f5, %f4, %f2;
	fma.rn.ftz.f32 	%f6, %f3, %f1, %f5;
	st.global.f32 	[%rd5], %f6;

$L__BB51_2:
	ret;

}
	// .globl	momentumPow2
.visible .entry momentumPow2(
	.param .u64 momentumPow2_param_0,
	.param .u64 momentumPow2_param_1,
	.param .align 2 .b8 momentumPow2_param_2[2],
	.param .align 2 .b8 momentumPow2_param_3[2],
	.param .u32 momentumPow2_param_4
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<14>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs5, [momentumPow2_param_3];
	ld.param.u16 	%rs4, [momentumPow2_param_2];
	ld.param.u64 	%rd2, [momentumPow2_param_0];
	ld.param.u64 	%rd3, [momentumPow2_param_1];
	ld.param.u32 	%r3, [momentumPow2_param_4];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB52_5;

	cvta.to.global.u64 	%rd4, %rd2;
	// begin inline asm
	{ mov.b32 %f1, {0,%rs4};}

	// end inline asm
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.u16 	%rs7, [%rd1];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs7};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f3, {0,%rs5};}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs9, [%rd7];
	// begin inline asm
	{ mov.b32 %f4, {0,%rs9};}

	// end inline asm
	mul.ftz.f32 	%f6, %f3, %f4;
	// begin inline asm
	{ mov.b32 %f5, {0,%rs9};}

	// end inline asm
	mul.ftz.f32 	%f7, %f6, %f5;
	fma.rn.ftz.f32 	%f8, %f1, %f2, %f7;
	mov.b32 	%r7, %f8;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p2, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs11, %r10;
	selp.b32 	%r2, 0, %r9, %p2;
	selp.b16 	%rs13, 32767, %rs11, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB52_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs12, %rs13, 1;
	setp.eq.b16 	%p5, %rs12, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB52_4;

$L__BB52_3:
	add.s16 	%rs13, %rs13, 1;

$L__BB52_4:
	st.global.u16 	[%rd1], %rs13;

$L__BB52_5:
	ret;

}
	// .globl	momentumPow2_float
.visible .entry momentumPow2_float(
	.param .u64 momentumPow2_float_param_0,
	.param .u64 momentumPow2_float_param_1,
	.param .f32 momentumPow2_float_param_2,
	.param .f32 momentumPow2_float_param_3,
	.param .u32 momentumPow2_float_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [momentumPow2_float_param_0];
	ld.param.u64 	%rd2, [momentumPow2_float_param_1];
	ld.param.f32 	%f1, [momentumPow2_float_param_2];
	ld.param.f32 	%f2, [momentumPow2_float_param_3];
	ld.param.u32 	%r2, [momentumPow2_float_param_4];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB53_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f3, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f4, [%rd7];
	mul.ftz.f32 	%f5, %f4, %f2;
	mul.ftz.f32 	%f6, %f4, %f5;
	fma.rn.ftz.f32 	%f7, %f3, %f1, %f6;
	st.global.f32 	[%rd5], %f7;

$L__BB53_2:
	ret;

}
	// .globl	subDivSqrtNorm_TYPE
.visible .entry subDivSqrtNorm_TYPE(
	.param .u64 subDivSqrtNorm_TYPE_param_0,
	.param .u64 subDivSqrtNorm_TYPE_param_1,
	.param .align 2 .b8 subDivSqrtNorm_TYPE_param_2[2],
	.param .align 2 .b8 subDivSqrtNorm_TYPE_param_3[2],
	.param .align 2 .b8 subDivSqrtNorm_TYPE_param_4[2],
	.param .u64 subDivSqrtNorm_TYPE_param_5,
	.param .u32 subDivSqrtNorm_TYPE_param_6
)
{
	.reg .pred 	%p<26>;
	.reg .b16 	%rs<40>;
	.reg .f32 	%f<21>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<11>;


	ld.param.u16 	%rs16, [subDivSqrtNorm_TYPE_param_4];
	ld.param.u16 	%rs15, [subDivSqrtNorm_TYPE_param_3];
	ld.param.u16 	%rs14, [subDivSqrtNorm_TYPE_param_2];
	ld.param.u64 	%rd2, [subDivSqrtNorm_TYPE_param_0];
	ld.param.u64 	%rd3, [subDivSqrtNorm_TYPE_param_1];
	ld.param.u64 	%rd4, [subDivSqrtNorm_TYPE_param_5];
	ld.param.u32 	%r6, [subDivSqrtNorm_TYPE_param_6];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r8, %r7, %r9;
	setp.ge.s32 	%p1, %r1, %r6;
	@%p1 bra 	$L__BB54_14;

	ld.const.u16 	%rs1, [sh+10];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs14};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f4, {0,%rs15};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f5, {0,%rs1};}

	// end inline asm
	add.ftz.f32 	%f6, %f4, %f5;
	div.approx.ftz.f32 	%f7, %f3, %f6;
	mov.b32 	%r10, %f7;
	and.b32  	%r11, %r10, 2147483647;
	setp.gt.u32 	%p2, %r11, 2139095040;
	shl.b32 	%r12, %r10, 16;
	shr.u32 	%r13, %r10, 16;
	cvt.u16.u32 	%rs20, %r13;
	selp.b32 	%r2, 0, %r12, %p2;
	selp.b16 	%rs36, 32767, %rs20, %p2;
	setp.gt.u32 	%p3, %r2, -2147483648;
	@%p3 bra 	$L__BB54_3;

	setp.ne.s32 	%p4, %r2, -2147483648;
	and.b16  	%rs21, %rs36, 1;
	setp.eq.b16 	%p5, %rs21, 1;
	not.pred 	%p6, %p5;
	or.pred  	%p7, %p4, %p6;
	@%p7 bra 	$L__BB54_4;

$L__BB54_3:
	add.s16 	%rs36, %rs36, 1;

$L__BB54_4:
	cvta.to.global.u64 	%rd5, %rd4;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd1, %rd5, %rd6;
	ld.global.u16 	%rs22, [%rd1];
	// begin inline asm
	{ mov.b32 %f8, {0,%rs22};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f9, {0,%rs36};}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd6;
	ld.global.nc.u16 	%rs24, [%rd8];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs24};}

	// end inline asm
	mul.ftz.f32 	%f2, %f9, %f10;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd6;
	ld.global.nc.u16 	%rs25, [%rd10];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs25};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f12, {0,%rs16};}

	// end inline asm
	div.approx.ftz.f32 	%f13, %f11, %f12;
	mov.b32 	%r14, %f13;
	and.b32  	%r15, %r14, 2147483647;
	setp.gt.u32 	%p8, %r15, 2139095040;
	shl.b32 	%r16, %r14, 16;
	shr.u32 	%r17, %r14, 16;
	cvt.u16.u32 	%rs27, %r17;
	selp.b32 	%r3, 0, %r16, %p8;
	selp.b16 	%rs37, 32767, %rs27, %p8;
	setp.gt.u32 	%p9, %r3, -2147483648;
	@%p9 bra 	$L__BB54_6;

	setp.ne.s32 	%p10, %r3, -2147483648;
	and.b16  	%rs28, %rs37, 1;
	setp.eq.b16 	%p11, %rs28, 1;
	not.pred 	%p12, %p11;
	or.pred  	%p13, %p10, %p12;
	@%p13 bra 	$L__BB54_7;

$L__BB54_6:
	add.s16 	%rs37, %rs37, 1;

$L__BB54_7:
	// begin inline asm
	{ mov.b32 %f14, {0,%rs37};}

	// end inline asm
	sqrt.approx.ftz.f32 	%f15, %f14;
	mov.b32 	%r18, %f15;
	and.b32  	%r19, %r18, 2147483647;
	setp.gt.u32 	%p14, %r19, 2139095040;
	shl.b32 	%r20, %r18, 16;
	shr.u32 	%r21, %r18, 16;
	cvt.u16.u32 	%rs30, %r21;
	selp.b32 	%r4, 0, %r20, %p14;
	selp.b16 	%rs38, 32767, %rs30, %p14;
	setp.gt.u32 	%p15, %r4, -2147483648;
	@%p15 bra 	$L__BB54_9;

	setp.ne.s32 	%p16, %r4, -2147483648;
	and.b16  	%rs31, %rs38, 1;
	setp.eq.b16 	%p17, %rs31, 1;
	not.pred 	%p18, %p17;
	or.pred  	%p19, %p16, %p18;
	@%p19 bra 	$L__BB54_10;

$L__BB54_9:
	add.s16 	%rs38, %rs38, 1;

$L__BB54_10:
	// begin inline asm
	{ mov.b32 %f16, {0,%rs38};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f17, {0,%rs1};}

	// end inline asm
	add.ftz.f32 	%f18, %f16, %f17;
	div.approx.ftz.f32 	%f19, %f2, %f18;
	sub.ftz.f32 	%f20, %f8, %f19;
	mov.b32 	%r22, %f20;
	and.b32  	%r23, %r22, 2147483647;
	setp.gt.u32 	%p20, %r23, 2139095040;
	shl.b32 	%r24, %r22, 16;
	shr.u32 	%r25, %r22, 16;
	cvt.u16.u32 	%rs34, %r25;
	selp.b32 	%r5, 0, %r24, %p20;
	selp.b16 	%rs39, 32767, %rs34, %p20;
	setp.gt.u32 	%p21, %r5, -2147483648;
	@%p21 bra 	$L__BB54_12;

	setp.ne.s32 	%p22, %r5, -2147483648;
	and.b16  	%rs35, %rs39, 1;
	setp.eq.b16 	%p23, %rs35, 1;
	not.pred 	%p24, %p23;
	or.pred  	%p25, %p22, %p24;
	@%p25 bra 	$L__BB54_13;

$L__BB54_12:
	add.s16 	%rs39, %rs39, 1;

$L__BB54_13:
	st.global.u16 	[%rd1], %rs39;

$L__BB54_14:
	ret;

}
	// .globl	subDivSqrtNorm
.visible .entry subDivSqrtNorm(
	.param .u64 subDivSqrtNorm_param_0,
	.param .u64 subDivSqrtNorm_param_1,
	.param .f32 subDivSqrtNorm_param_2,
	.param .f32 subDivSqrtNorm_param_3,
	.param .f32 subDivSqrtNorm_param_4,
	.param .u64 subDivSqrtNorm_param_5,
	.param .u32 subDivSqrtNorm_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subDivSqrtNorm_param_0];
	ld.param.u64 	%rd2, [subDivSqrtNorm_param_1];
	ld.param.f32 	%f1, [subDivSqrtNorm_param_2];
	ld.param.f32 	%f2, [subDivSqrtNorm_param_3];
	ld.param.f32 	%f3, [subDivSqrtNorm_param_4];
	ld.param.u64 	%rd3, [subDivSqrtNorm_param_5];
	ld.param.u32 	%r2, [subDivSqrtNorm_param_6];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB55_2;

	cvta.to.global.u64 	%rd4, %rd1;
	add.ftz.f32 	%f4, %f2, 0f33D6BF95;
	div.approx.ftz.f32 	%f5, %f1, %f4;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f6, [%rd6];
	mul.ftz.f32 	%f7, %f5, %f6;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.nc.f32 	%f8, [%rd8];
	div.approx.ftz.f32 	%f9, %f8, %f3;
	sqrt.approx.ftz.f32 	%f10, %f9;
	add.ftz.f32 	%f11, %f10, 0f33D6BF95;
	div.approx.ftz.f32 	%f12, %f7, %f11;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	ld.global.f32 	%f13, [%rd10];
	sub.ftz.f32 	%f14, %f13, %f12;
	st.global.f32 	[%rd10], %f14;

$L__BB55_2:
	ret;

}
	// .globl	addBackCopy
.visible .entry addBackCopy(
	.param .u64 addBackCopy_param_0,
	.param .u32 addBackCopy_param_1,
	.param .u32 addBackCopy_param_2,
	.param .u32 addBackCopy_param_3,
	.param .u32 addBackCopy_param_4,
	.param .u64 addBackCopy_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addBackCopy_param_0];
	ld.param.u32 	%r3, [addBackCopy_param_1];
	ld.param.u32 	%r6, [addBackCopy_param_2];
	ld.param.u32 	%r4, [addBackCopy_param_3];
	ld.param.u32 	%r5, [addBackCopy_param_4];
	ld.param.u64 	%rd2, [addBackCopy_param_5];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB56_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r14, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB56_2:
	ret;

}
	// .globl	addBackCopy_TYPE
.visible .entry addBackCopy_TYPE(
	.param .u64 addBackCopy_TYPE_param_0,
	.param .u32 addBackCopy_TYPE_param_1,
	.param .u32 addBackCopy_TYPE_param_2,
	.param .u32 addBackCopy_TYPE_param_3,
	.param .u32 addBackCopy_TYPE_param_4,
	.param .u64 addBackCopy_TYPE_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addBackCopy_TYPE_param_0];
	ld.param.u32 	%r3, [addBackCopy_TYPE_param_1];
	ld.param.u32 	%r6, [addBackCopy_TYPE_param_2];
	ld.param.u32 	%r4, [addBackCopy_TYPE_param_3];
	ld.param.u32 	%r5, [addBackCopy_TYPE_param_4];
	ld.param.u64 	%rd2, [addBackCopy_TYPE_param_5];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB57_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r14, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r15, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB57_2:
	ret;

}
	// .globl	dropoutBack
.visible .entry dropoutBack(
	.param .u64 dropoutBack_param_0,
	.param .u64 dropoutBack_param_1,
	.param .align 2 .b8 dropoutBack_param_2[2],
	.param .u64 dropoutBack_param_3,
	.param .u32 dropoutBack_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<12>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<14>;


	ld.param.u16 	%rs4, [dropoutBack_param_2];
	ld.param.u64 	%rd2, [dropoutBack_param_0];
	ld.param.u64 	%rd3, [dropoutBack_param_1];
	ld.param.u64 	%rd4, [dropoutBack_param_3];
	ld.param.u32 	%r3, [dropoutBack_param_4];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB58_6;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs5, [%rd7];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs5};}

	// end inline asm
	ld.const.u16 	%rs6, [sh];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs6};}

	// end inline asm
	setp.eq.ftz.f32 	%p2, %f1, %f2;
	@%p2 bra 	$L__BB58_6;

	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs7, [%rd10];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs7};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f4, {0,%rs4};}

	// end inline asm
	mul.ftz.f32 	%f5, %f3, %f4;
	mov.b32 	%r7, %f5;
	and.b32  	%r8, %r7, 2147483647;
	setp.gt.u32 	%p3, %r8, 2139095040;
	shl.b32 	%r9, %r7, 16;
	shr.u32 	%r10, %r7, 16;
	cvt.u16.u32 	%rs9, %r10;
	selp.b32 	%r2, 0, %r9, %p3;
	selp.b16 	%rs11, 32767, %rs9, %p3;
	setp.gt.u32 	%p4, %r2, -2147483648;
	@%p4 bra 	$L__BB58_4;

	setp.ne.s32 	%p5, %r2, -2147483648;
	and.b16  	%rs10, %rs11, 1;
	setp.eq.b16 	%p6, %rs10, 1;
	not.pred 	%p7, %p6;
	or.pred  	%p8, %p5, %p7;
	@%p8 bra 	$L__BB58_5;

$L__BB58_4:
	add.s16 	%rs11, %rs11, 1;

$L__BB58_5:
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd13, %rd11, %rd9;
	st.global.u16 	[%rd13], %rs11;

$L__BB58_6:
	ret;

}
	// .globl	mask
.visible .entry mask(
	.param .u64 mask_param_0,
	.param .f32 mask_param_1,
	.param .f32 mask_param_2,
	.param .u64 mask_param_3,
	.param .u32 mask_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [mask_param_0];
	ld.param.f32 	%f1, [mask_param_1];
	ld.param.f32 	%f2, [mask_param_2];
	ld.param.u64 	%rd3, [mask_param_3];
	ld.param.u32 	%r2, [mask_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB59_3;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f3, [%rd6];
	setp.neu.ftz.f32 	%p2, %f3, %f1;
	@%p2 bra 	$L__BB59_3;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f2;

$L__BB59_3:
	ret;

}
	// .globl	mask_TYPE
.visible .entry mask_TYPE(
	.param .u64 mask_TYPE_param_0,
	.param .align 2 .b8 mask_TYPE_param_1[2],
	.param .align 2 .b8 mask_TYPE_param_2[2],
	.param .u64 mask_TYPE_param_3,
	.param .u32 mask_TYPE_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs2, [mask_TYPE_param_2];
	ld.param.u16 	%rs1, [mask_TYPE_param_1];
	ld.param.u64 	%rd2, [mask_TYPE_param_0];
	ld.param.u64 	%rd3, [mask_TYPE_param_3];
	ld.param.u32 	%r2, [mask_TYPE_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB60_3;

	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs3, [%rd6];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs3};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs1};}

	// end inline asm
	setp.neu.ftz.f32 	%p2, %f1, %f2;
	@%p2 bra 	$L__BB60_3;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs2;

$L__BB60_3:
	ret;

}
	// .globl	fillUnderDiagonal
.visible .entry fillUnderDiagonal(
	.param .u32 fillUnderDiagonal_param_0,
	.param .f32 fillUnderDiagonal_param_1,
	.param .u64 fillUnderDiagonal_param_2,
	.param .u32 fillUnderDiagonal_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r11, [fillUnderDiagonal_param_0];
	ld.param.f32 	%f1, [fillUnderDiagonal_param_1];
	ld.param.u64 	%rd8, [fillUnderDiagonal_param_2];
	ld.param.u32 	%r12, [fillUnderDiagonal_param_3];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	setp.lt.s32 	%p2, %r1, 0;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB61_7;

	add.s32 	%r17, %r1, 1;
	and.b32  	%r24, %r17, 3;
	setp.lt.u32 	%p4, %r1, 3;
	mov.u32 	%r23, 0;
	@%p4 bra 	$L__BB61_4;

	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd9, %r19, 4;
	add.s64 	%rd10, %rd1, %rd9;
	add.s64 	%rd12, %rd10, 8;
	sub.s32 	%r21, %r1, %r24;

$L__BB61_3:
	st.global.f32 	[%rd12+-8], %f1;
	st.global.f32 	[%rd12+-4], %f1;
	st.global.f32 	[%rd12], %f1;
	st.global.f32 	[%rd12+4], %f1;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd12, %rd12, 16;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p5, %r21, -1;
	@%p5 bra 	$L__BB61_3;

$L__BB61_4:
	setp.eq.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB61_7;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd11, %r20, 4;
	add.s64 	%rd13, %rd1, %rd11;

$L__BB61_6:
	.pragma "nounroll";
	st.global.f32 	[%rd13], %f1;
	add.s64 	%rd13, %rd13, 4;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p7, %r24, 0;
	@%p7 bra 	$L__BB61_6;

$L__BB61_7:
	ret;

}
	// .globl	crossEntropy
.visible .entry crossEntropy(
	.param .u64 crossEntropy_param_0,
	.param .u64 crossEntropy_param_1,
	.param .u64 crossEntropy_param_2,
	.param .u32 crossEntropy_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [crossEntropy_param_0];
	ld.param.u64 	%rd2, [crossEntropy_param_1];
	ld.param.u64 	%rd3, [crossEntropy_param_2];
	ld.param.u32 	%r2, [crossEntropy_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB62_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	add.ftz.f32 	%f2, %f1, 0f322BCC77;
	lg2.approx.ftz.f32 	%f3, %f2;
	mul.ftz.f32 	%f4, %f3, 0f3F317218;
	ld.global.f32 	%f5, [%rd6];
	mul.ftz.f32 	%f6, %f5, %f4;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f6;

$L__BB62_2:
	ret;

}
	// .globl	derCrossEntropy
.visible .entry derCrossEntropy(
	.param .u64 derCrossEntropy_param_0,
	.param .u64 derCrossEntropy_param_1,
	.param .u64 derCrossEntropy_param_2,
	.param .u32 derCrossEntropy_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [derCrossEntropy_param_0];
	ld.param.u64 	%rd2, [derCrossEntropy_param_1];
	ld.param.u64 	%rd3, [derCrossEntropy_param_2];
	ld.param.u32 	%r2, [derCrossEntropy_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB63_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	neg.ftz.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f3, [%rd8];
	add.ftz.f32 	%f4, %f3, 0f322BCC77;
	div.approx.ftz.f32 	%f5, %f2, %f4;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f5;

$L__BB63_2:
	ret;

}
	// .globl	stride
.visible .entry stride(
	.param .u64 stride_param_0,
	.param .u64 stride_param_1,
	.param .u32 stride_param_2,
	.param .f32 stride_param_3,
	.param .f32 stride_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd3, [stride_param_0];
	ld.param.u64 	%rd4, [stride_param_1];
	ld.param.u32 	%r10, [stride_param_2];
	ld.param.f32 	%f3, [stride_param_3];
	ld.param.f32 	%f2, [stride_param_4];
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r12, %r11, %r13;
	cvt.rn.f32.s32 	%f1, %r1;
	setp.geu.ftz.f32 	%p1, %f1, %f3;
	@%p1 bra 	$L__BB64_4;

	setp.leu.ftz.f32 	%p2, %f2, 0f00000000;
	@%p2 bra 	$L__BB64_4;

	mul.lo.s32 	%r15, %r1, %r10;
	cvt.rn.f32.s32 	%f4, %r15;
	mul.ftz.f32 	%f5, %f4, %f2;
	cvt.rzi.ftz.s32.f32 	%r17, %f5;
	mul.ftz.f32 	%f6, %f1, %f2;
	cvt.rzi.ftz.s32.f32 	%r18, %f6;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r16, 0;

$L__BB64_3:
	mul.wide.s32 	%rd5, %r18, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.nc.f32 	%f7, [%rd6];
	mul.wide.s32 	%rd7, %r17, 4;
	add.s64 	%rd8, %rd1, %rd7;
	st.global.f32 	[%rd8], %f7;
	add.s32 	%r18, %r18, 1;
	add.s32 	%r17, %r17, 1;
	add.s32 	%r16, %r16, 1;
	cvt.rn.f32.s32 	%f8, %r16;
	setp.lt.ftz.f32 	%p3, %f8, %f2;
	@%p3 bra 	$L__BB64_3;

$L__BB64_4:
	ret;

}
	// .globl	fillUnderDiagonal_TYPE
.visible .entry fillUnderDiagonal_TYPE(
	.param .u32 fillUnderDiagonal_TYPE_param_0,
	.param .align 2 .b8 fillUnderDiagonal_TYPE_param_1[2],
	.param .u64 fillUnderDiagonal_TYPE_param_2,
	.param .u32 fillUnderDiagonal_TYPE_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<14>;


	ld.param.u16 	%rs2, [fillUnderDiagonal_TYPE_param_1];
	ld.param.u32 	%r11, [fillUnderDiagonal_TYPE_param_0];
	ld.param.u64 	%rd8, [fillUnderDiagonal_TYPE_param_2];
	ld.param.u32 	%r12, [fillUnderDiagonal_TYPE_param_3];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	setp.lt.s32 	%p2, %r1, 0;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB65_7;

	add.s32 	%r17, %r1, 1;
	and.b32  	%r24, %r17, 3;
	setp.lt.u32 	%p4, %r1, 3;
	mov.u32 	%r23, 0;
	@%p4 bra 	$L__BB65_4;

	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd9, %r19, 2;
	add.s64 	%rd10, %rd1, %rd9;
	add.s64 	%rd12, %rd10, 4;
	sub.s32 	%r21, %r1, %r24;

$L__BB65_3:
	st.global.u16 	[%rd12+-4], %rs2;
	st.global.u16 	[%rd12+-2], %rs2;
	st.global.u16 	[%rd12], %rs2;
	st.global.u16 	[%rd12+2], %rs2;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd12, %rd12, 8;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p5, %r21, -1;
	@%p5 bra 	$L__BB65_3;

$L__BB65_4:
	setp.eq.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB65_7;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd11, %r20, 2;
	add.s64 	%rd13, %rd1, %rd11;

$L__BB65_6:
	.pragma "nounroll";
	st.global.u16 	[%rd13], %rs2;
	add.s64 	%rd13, %rd13, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p7, %r24, 0;
	@%p7 bra 	$L__BB65_6;

$L__BB65_7:
	ret;

}
	// .globl	derGelu
.visible .entry derGelu(
	.param .u64 derGelu_param_0,
	.param .u64 derGelu_param_1,
	.param .u64 derGelu_param_2,
	.param .u32 derGelu_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [derGelu_param_0];
	ld.param.u64 	%rd3, [derGelu_param_1];
	ld.param.u64 	%rd4, [derGelu_param_2];
	ld.param.u32 	%r2, [derGelu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB66_5;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.f32 	%f1, [%rd7];
	mul.ftz.f32 	%f7, %f1, 0f3D122277;
	mul.ftz.f32 	%f8, %f1, %f7;
	mul.ftz.f32 	%f9, %f1, %f8;
	fma.rn.ftz.f32 	%f2, %f1, 0f3F4C422A, %f9;
	abs.ftz.f32 	%f3, %f2;
	setp.ltu.ftz.f32 	%p2, %f3, 0f3F19999A;
	@%p2 bra 	$L__BB66_3;
	bra.uni 	$L__BB66_2;

$L__BB66_3:
	mul.ftz.f32 	%f18, %f2, %f2;
	mov.f32 	%f19, 0fBD563CAE;
	mov.f32 	%f20, 0f3C80F082;
	fma.rn.ftz.f32 	%f21, %f20, %f18, %f19;
	mov.f32 	%f22, 0f3E085941;
	fma.rn.ftz.f32 	%f23, %f21, %f18, %f22;
	mov.f32 	%f24, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f25, %f23, %f18, %f24;
	mov.f32 	%f26, 0f00000000;
	fma.rn.ftz.f32 	%f27, %f25, %f18, %f26;
	fma.rn.ftz.f32 	%f39, %f27, %f2, %f2;
	bra.uni 	$L__BB66_4;

$L__BB66_2:
	mul.ftz.f32 	%f10, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f11, %f10;
	add.ftz.f32 	%f12, %f11, 0f3F800000;
	mov.f32 	%f13, 0f3F800000;
	rcp.approx.ftz.f32 	%f14, %f12;
	mov.f32 	%f15, 0fC0000000;
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	setp.ge.ftz.f32 	%p3, %f3, 0f41102CB4;
	selp.f32 	%f17, 0f3F800000, %f16, %p3;
	mov.b32 	%r6, %f17;
	mov.b32 	%r7, %f2;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f39, %r9;

$L__BB66_4:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.f32 	%f28, [%rd10];
	mul.ftz.f32 	%f29, %f28, 0f3F000000;
	mul.ftz.f32 	%f30, %f39, %f39;
	mov.f32 	%f31, 0f3F800000;
	sub.ftz.f32 	%f32, %f31, %f30;
	mul.ftz.f32 	%f33, %f1, %f32;
	mul.ftz.f32 	%f34, %f1, 0f3DDB33B3;
	fma.rn.ftz.f32 	%f35, %f1, %f34, 0f3F4C426B;
	add.ftz.f32 	%f36, %f39, 0f3F800000;
	fma.rn.ftz.f32 	%f37, %f35, %f33, %f36;
	mul.ftz.f32 	%f38, %f29, %f37;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.f32 	[%rd12], %f38;

$L__BB66_5:
	ret;

}
	// .globl	derGelu_TYPE
.visible .entry derGelu_TYPE(
	.param .u64 derGelu_TYPE_param_0,
	.param .u64 derGelu_TYPE_param_1,
	.param .u64 derGelu_TYPE_param_2,
	.param .u32 derGelu_TYPE_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [derGelu_TYPE_param_0];
	ld.param.u64 	%rd3, [derGelu_TYPE_param_1];
	ld.param.u64 	%rd4, [derGelu_TYPE_param_2];
	ld.param.u32 	%r3, [derGelu_TYPE_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB67_8;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs4, [%rd7];
	// begin inline asm
	{ mov.b32 %f7, {0,%rs4};}

	// end inline asm
	mul.ftz.f32 	%f8, %f7, 0f3D122277;
	mul.ftz.f32 	%f9, %f7, %f8;
	mul.ftz.f32 	%f10, %f7, %f9;
	fma.rn.ftz.f32 	%f2, %f7, 0f3F4C422A, %f10;
	abs.ftz.f32 	%f3, %f2;
	setp.ltu.ftz.f32 	%p2, %f3, 0f3F19999A;
	@%p2 bra 	$L__BB67_3;
	bra.uni 	$L__BB67_2;

$L__BB67_3:
	mul.ftz.f32 	%f19, %f2, %f2;
	mov.f32 	%f20, 0fBD563CAE;
	mov.f32 	%f21, 0f3C80F082;
	fma.rn.ftz.f32 	%f22, %f21, %f19, %f20;
	mov.f32 	%f23, 0f3E085941;
	fma.rn.ftz.f32 	%f24, %f22, %f19, %f23;
	mov.f32 	%f25, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f26, %f24, %f19, %f25;
	mov.f32 	%f27, 0f00000000;
	fma.rn.ftz.f32 	%f28, %f26, %f19, %f27;
	fma.rn.ftz.f32 	%f40, %f28, %f2, %f2;
	bra.uni 	$L__BB67_4;

$L__BB67_2:
	mul.ftz.f32 	%f11, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f12, %f11;
	add.ftz.f32 	%f13, %f12, 0f3F800000;
	mov.f32 	%f14, 0f3F800000;
	rcp.approx.ftz.f32 	%f15, %f13;
	mov.f32 	%f16, 0fC0000000;
	fma.rn.ftz.f32 	%f17, %f15, %f16, %f14;
	setp.ge.ftz.f32 	%p3, %f3, 0f41102CB4;
	selp.f32 	%f18, 0f3F800000, %f17, %p3;
	mov.b32 	%r7, %f18;
	mov.b32 	%r8, %f2;
	and.b32  	%r9, %r8, -2147483648;
	or.b32  	%r10, %r9, %r7;
	mov.b32 	%f40, %r10;

$L__BB67_4:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs5, [%rd10];
	// begin inline asm
	{ mov.b32 %f29, {0,%rs5};}

	// end inline asm
	mul.ftz.f32 	%f30, %f29, 0f3F000000;
	mul.ftz.f32 	%f31, %f40, %f40;
	mov.f32 	%f32, 0f3F800000;
	sub.ftz.f32 	%f33, %f32, %f31;
	mul.ftz.f32 	%f34, %f7, %f33;
	mul.ftz.f32 	%f35, %f7, 0f3DDB33B3;
	fma.rn.ftz.f32 	%f36, %f7, %f35, 0f3F4C426B;
	add.ftz.f32 	%f37, %f40, 0f3F800000;
	fma.rn.ftz.f32 	%f38, %f36, %f34, %f37;
	mul.ftz.f32 	%f39, %f30, %f38;
	mov.b32 	%r11, %f39;
	and.b32  	%r12, %r11, 2147483647;
	setp.gt.u32 	%p4, %r12, 2139095040;
	shl.b32 	%r13, %r11, 16;
	shr.u32 	%r14, %r11, 16;
	cvt.u16.u32 	%rs6, %r14;
	selp.b32 	%r2, 0, %r13, %p4;
	selp.b16 	%rs8, 32767, %rs6, %p4;
	setp.gt.u32 	%p5, %r2, -2147483648;
	@%p5 bra 	$L__BB67_6;

	setp.ne.s32 	%p6, %r2, -2147483648;
	and.b16  	%rs7, %rs8, 1;
	setp.eq.b16 	%p7, %rs7, 1;
	not.pred 	%p8, %p7;
	or.pred  	%p9, %p6, %p8;
	@%p9 bra 	$L__BB67_7;

$L__BB67_6:
	add.s16 	%rs8, %rs8, 1;

$L__BB67_7:
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd13, %rd11, %rd9;
	st.global.u16 	[%rd13], %rs8;

$L__BB67_8:
	ret;

}
	// .globl	matvec_kernel
.visible .entry matvec_kernel(
	.param .u64 matvec_kernel_param_0,
	.param .u64 matvec_kernel_param_1,
	.param .u64 matvec_kernel_param_2,
	.param .u32 matvec_kernel_param_3,
	.param .u32 matvec_kernel_param_4
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<75>;
	// demoted variable
	.shared .align 4 .b8 _ZZ13matvec_kernelE8x_shared[128];

	ld.param.u64 	%rd3, [matvec_kernel_param_0];
	ld.param.u64 	%rd4, [matvec_kernel_param_1];
	ld.param.u64 	%rd5, [matvec_kernel_param_2];
	ld.param.u32 	%r13, [matvec_kernel_param_3];
	ld.param.u32 	%r14, [matvec_kernel_param_4];
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r51, %tid.x;
	mad.lo.s32 	%r2, %r16, %r15, %r51;
	add.s32 	%r17, %r14, 31;
	shr.u32 	%r3, %r17, 5;
	setp.eq.s32 	%p1, %r3, 0;
	mov.f32 	%f106, 0f00000000;
	@%p1 bra 	$L__BB68_5;

	cvta.to.global.u64 	%rd1, %rd3;
	shl.b32 	%r18, %r51, 2;
	mov.u32 	%r19, _ZZ13matvec_kernelE8x_shared;
	add.s32 	%r4, %r19, %r18;
	mul.lo.s32 	%r52, %r2, %r14;
	neg.s32 	%r53, %r3;
	cvta.to.global.u64 	%rd2, %rd4;

$L__BB68_2:
	setp.ge.u32 	%p2, %r51, %r14;
	mov.f32 	%f105, 0f00000000;
	@%p2 bra 	$L__BB68_4;

	mul.wide.u32 	%rd6, %r51, 4;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.nc.f32 	%f105, [%rd7];

$L__BB68_4:
	st.shared.f32 	[%r4], %f105;
	bar.sync 	0;
	mul.wide.u32 	%rd8, %r52, 4;
	add.s64 	%rd9, %rd1, %rd8;
	ld.shared.f32 	%f9, [_ZZ13matvec_kernelE8x_shared];
	ld.global.nc.f32 	%f10, [%rd9];
	fma.rn.ftz.f32 	%f11, %f10, %f9, %f106;
	add.s32 	%r20, %r52, 1;
	mul.wide.u32 	%rd10, %r20, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.shared.f32 	%f12, [_ZZ13matvec_kernelE8x_shared+4];
	ld.global.nc.f32 	%f13, [%rd11];
	fma.rn.ftz.f32 	%f14, %f13, %f12, %f11;
	add.s32 	%r21, %r52, 2;
	mul.wide.u32 	%rd12, %r21, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.shared.f32 	%f15, [_ZZ13matvec_kernelE8x_shared+8];
	ld.global.nc.f32 	%f16, [%rd13];
	fma.rn.ftz.f32 	%f17, %f16, %f15, %f14;
	add.s32 	%r22, %r52, 3;
	mul.wide.u32 	%rd14, %r22, 4;
	add.s64 	%rd15, %rd1, %rd14;
	ld.shared.f32 	%f18, [_ZZ13matvec_kernelE8x_shared+12];
	ld.global.nc.f32 	%f19, [%rd15];
	fma.rn.ftz.f32 	%f20, %f19, %f18, %f17;
	add.s32 	%r23, %r52, 4;
	mul.wide.u32 	%rd16, %r23, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ld.shared.f32 	%f21, [_ZZ13matvec_kernelE8x_shared+16];
	ld.global.nc.f32 	%f22, [%rd17];
	fma.rn.ftz.f32 	%f23, %f22, %f21, %f20;
	add.s32 	%r24, %r52, 5;
	mul.wide.u32 	%rd18, %r24, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.shared.f32 	%f24, [_ZZ13matvec_kernelE8x_shared+20];
	ld.global.nc.f32 	%f25, [%rd19];
	fma.rn.ftz.f32 	%f26, %f25, %f24, %f23;
	add.s32 	%r25, %r52, 6;
	mul.wide.u32 	%rd20, %r25, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.shared.f32 	%f27, [_ZZ13matvec_kernelE8x_shared+24];
	ld.global.nc.f32 	%f28, [%rd21];
	fma.rn.ftz.f32 	%f29, %f28, %f27, %f26;
	add.s32 	%r26, %r52, 7;
	mul.wide.u32 	%rd22, %r26, 4;
	add.s64 	%rd23, %rd1, %rd22;
	ld.shared.f32 	%f30, [_ZZ13matvec_kernelE8x_shared+28];
	ld.global.nc.f32 	%f31, [%rd23];
	fma.rn.ftz.f32 	%f32, %f31, %f30, %f29;
	add.s32 	%r27, %r52, 8;
	mul.wide.u32 	%rd24, %r27, 4;
	add.s64 	%rd25, %rd1, %rd24;
	ld.shared.f32 	%f33, [_ZZ13matvec_kernelE8x_shared+32];
	ld.global.nc.f32 	%f34, [%rd25];
	fma.rn.ftz.f32 	%f35, %f34, %f33, %f32;
	add.s32 	%r28, %r52, 9;
	mul.wide.u32 	%rd26, %r28, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.shared.f32 	%f36, [_ZZ13matvec_kernelE8x_shared+36];
	ld.global.nc.f32 	%f37, [%rd27];
	fma.rn.ftz.f32 	%f38, %f37, %f36, %f35;
	add.s32 	%r29, %r52, 10;
	mul.wide.u32 	%rd28, %r29, 4;
	add.s64 	%rd29, %rd1, %rd28;
	ld.shared.f32 	%f39, [_ZZ13matvec_kernelE8x_shared+40];
	ld.global.nc.f32 	%f40, [%rd29];
	fma.rn.ftz.f32 	%f41, %f40, %f39, %f38;
	add.s32 	%r30, %r52, 11;
	mul.wide.u32 	%rd30, %r30, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.shared.f32 	%f42, [_ZZ13matvec_kernelE8x_shared+44];
	ld.global.nc.f32 	%f43, [%rd31];
	fma.rn.ftz.f32 	%f44, %f43, %f42, %f41;
	add.s32 	%r31, %r52, 12;
	mul.wide.u32 	%rd32, %r31, 4;
	add.s64 	%rd33, %rd1, %rd32;
	ld.shared.f32 	%f45, [_ZZ13matvec_kernelE8x_shared+48];
	ld.global.nc.f32 	%f46, [%rd33];
	fma.rn.ftz.f32 	%f47, %f46, %f45, %f44;
	add.s32 	%r32, %r52, 13;
	mul.wide.u32 	%rd34, %r32, 4;
	add.s64 	%rd35, %rd1, %rd34;
	ld.shared.f32 	%f48, [_ZZ13matvec_kernelE8x_shared+52];
	ld.global.nc.f32 	%f49, [%rd35];
	fma.rn.ftz.f32 	%f50, %f49, %f48, %f47;
	add.s32 	%r33, %r52, 14;
	mul.wide.u32 	%rd36, %r33, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.shared.f32 	%f51, [_ZZ13matvec_kernelE8x_shared+56];
	ld.global.nc.f32 	%f52, [%rd37];
	fma.rn.ftz.f32 	%f53, %f52, %f51, %f50;
	add.s32 	%r34, %r52, 15;
	mul.wide.u32 	%rd38, %r34, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.shared.f32 	%f54, [_ZZ13matvec_kernelE8x_shared+60];
	ld.global.nc.f32 	%f55, [%rd39];
	fma.rn.ftz.f32 	%f56, %f55, %f54, %f53;
	add.s32 	%r35, %r52, 16;
	mul.wide.u32 	%rd40, %r35, 4;
	add.s64 	%rd41, %rd1, %rd40;
	ld.shared.f32 	%f57, [_ZZ13matvec_kernelE8x_shared+64];
	ld.global.nc.f32 	%f58, [%rd41];
	fma.rn.ftz.f32 	%f59, %f58, %f57, %f56;
	add.s32 	%r36, %r52, 17;
	mul.wide.u32 	%rd42, %r36, 4;
	add.s64 	%rd43, %rd1, %rd42;
	ld.shared.f32 	%f60, [_ZZ13matvec_kernelE8x_shared+68];
	ld.global.nc.f32 	%f61, [%rd43];
	fma.rn.ftz.f32 	%f62, %f61, %f60, %f59;
	add.s32 	%r37, %r52, 18;
	mul.wide.u32 	%rd44, %r37, 4;
	add.s64 	%rd45, %rd1, %rd44;
	ld.shared.f32 	%f63, [_ZZ13matvec_kernelE8x_shared+72];
	ld.global.nc.f32 	%f64, [%rd45];
	fma.rn.ftz.f32 	%f65, %f64, %f63, %f62;
	add.s32 	%r38, %r52, 19;
	mul.wide.u32 	%rd46, %r38, 4;
	add.s64 	%rd47, %rd1, %rd46;
	ld.shared.f32 	%f66, [_ZZ13matvec_kernelE8x_shared+76];
	ld.global.nc.f32 	%f67, [%rd47];
	fma.rn.ftz.f32 	%f68, %f67, %f66, %f65;
	add.s32 	%r39, %r52, 20;
	mul.wide.u32 	%rd48, %r39, 4;
	add.s64 	%rd49, %rd1, %rd48;
	ld.shared.f32 	%f69, [_ZZ13matvec_kernelE8x_shared+80];
	ld.global.nc.f32 	%f70, [%rd49];
	fma.rn.ftz.f32 	%f71, %f70, %f69, %f68;
	add.s32 	%r40, %r52, 21;
	mul.wide.u32 	%rd50, %r40, 4;
	add.s64 	%rd51, %rd1, %rd50;
	ld.shared.f32 	%f72, [_ZZ13matvec_kernelE8x_shared+84];
	ld.global.nc.f32 	%f73, [%rd51];
	fma.rn.ftz.f32 	%f74, %f73, %f72, %f71;
	add.s32 	%r41, %r52, 22;
	mul.wide.u32 	%rd52, %r41, 4;
	add.s64 	%rd53, %rd1, %rd52;
	ld.shared.f32 	%f75, [_ZZ13matvec_kernelE8x_shared+88];
	ld.global.nc.f32 	%f76, [%rd53];
	fma.rn.ftz.f32 	%f77, %f76, %f75, %f74;
	add.s32 	%r42, %r52, 23;
	mul.wide.u32 	%rd54, %r42, 4;
	add.s64 	%rd55, %rd1, %rd54;
	ld.shared.f32 	%f78, [_ZZ13matvec_kernelE8x_shared+92];
	ld.global.nc.f32 	%f79, [%rd55];
	fma.rn.ftz.f32 	%f80, %f79, %f78, %f77;
	add.s32 	%r43, %r52, 24;
	mul.wide.u32 	%rd56, %r43, 4;
	add.s64 	%rd57, %rd1, %rd56;
	ld.shared.f32 	%f81, [_ZZ13matvec_kernelE8x_shared+96];
	ld.global.nc.f32 	%f82, [%rd57];
	fma.rn.ftz.f32 	%f83, %f82, %f81, %f80;
	add.s32 	%r44, %r52, 25;
	mul.wide.u32 	%rd58, %r44, 4;
	add.s64 	%rd59, %rd1, %rd58;
	ld.shared.f32 	%f84, [_ZZ13matvec_kernelE8x_shared+100];
	ld.global.nc.f32 	%f85, [%rd59];
	fma.rn.ftz.f32 	%f86, %f85, %f84, %f83;
	add.s32 	%r45, %r52, 26;
	mul.wide.u32 	%rd60, %r45, 4;
	add.s64 	%rd61, %rd1, %rd60;
	ld.shared.f32 	%f87, [_ZZ13matvec_kernelE8x_shared+104];
	ld.global.nc.f32 	%f88, [%rd61];
	fma.rn.ftz.f32 	%f89, %f88, %f87, %f86;
	add.s32 	%r46, %r52, 27;
	mul.wide.u32 	%rd62, %r46, 4;
	add.s64 	%rd63, %rd1, %rd62;
	ld.shared.f32 	%f90, [_ZZ13matvec_kernelE8x_shared+108];
	ld.global.nc.f32 	%f91, [%rd63];
	fma.rn.ftz.f32 	%f92, %f91, %f90, %f89;
	add.s32 	%r47, %r52, 28;
	mul.wide.u32 	%rd64, %r47, 4;
	add.s64 	%rd65, %rd1, %rd64;
	ld.shared.f32 	%f93, [_ZZ13matvec_kernelE8x_shared+112];
	ld.global.nc.f32 	%f94, [%rd65];
	fma.rn.ftz.f32 	%f95, %f94, %f93, %f92;
	add.s32 	%r48, %r52, 29;
	mul.wide.u32 	%rd66, %r48, 4;
	add.s64 	%rd67, %rd1, %rd66;
	ld.shared.f32 	%f96, [_ZZ13matvec_kernelE8x_shared+116];
	ld.global.nc.f32 	%f97, [%rd67];
	fma.rn.ftz.f32 	%f98, %f97, %f96, %f95;
	add.s32 	%r49, %r52, 30;
	mul.wide.u32 	%rd68, %r49, 4;
	add.s64 	%rd69, %rd1, %rd68;
	ld.shared.f32 	%f99, [_ZZ13matvec_kernelE8x_shared+120];
	ld.global.nc.f32 	%f100, [%rd69];
	fma.rn.ftz.f32 	%f101, %f100, %f99, %f98;
	add.s32 	%r50, %r52, 31;
	mul.wide.u32 	%rd70, %r50, 4;
	add.s64 	%rd71, %rd1, %rd70;
	ld.shared.f32 	%f102, [_ZZ13matvec_kernelE8x_shared+124];
	ld.global.nc.f32 	%f103, [%rd71];
	fma.rn.ftz.f32 	%f106, %f103, %f102, %f101;
	bar.sync 	0;
	add.s32 	%r52, %r52, 32;
	add.s32 	%r51, %r51, 32;
	add.s32 	%r53, %r53, 1;
	setp.ne.s32 	%p3, %r53, 0;
	@%p3 bra 	$L__BB68_2;

$L__BB68_5:
	setp.ge.u32 	%p4, %r2, %r13;
	@%p4 bra 	$L__BB68_7;

	cvta.to.global.u64 	%rd72, %rd5;
	mul.wide.u32 	%rd73, %r2, 4;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.f32 	[%rd74], %f106;

$L__BB68_7:
	ret;

}
	// .globl	dot_VectorAndMatrix
.visible .entry dot_VectorAndMatrix(
	.param .u64 dot_VectorAndMatrix_param_0,
	.param .u64 dot_VectorAndMatrix_param_1,
	.param .u64 dot_VectorAndMatrix_param_2,
	.param .u32 dot_VectorAndMatrix_param_3,
	.param .u32 dot_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<24>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd15, [dot_VectorAndMatrix_param_0];
	ld.param.u64 	%rd16, [dot_VectorAndMatrix_param_1];
	ld.param.u64 	%rd14, [dot_VectorAndMatrix_param_2];
	ld.param.u32 	%r12, [dot_VectorAndMatrix_param_3];
	ld.param.u32 	%r11, [dot_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB69_10;

	setp.lt.s32 	%p2, %r11, 1;
	mov.f32 	%f19, 0f00000000;
	@%p2 bra 	$L__BB69_9;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r24, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.f64 	%fd23, 0d0000000000000000;
	mov.u32 	%r23, 0;
	@%p3 bra 	$L__BB69_5;

	sub.s32 	%r22, %r11, %r24;
	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd17, %r19, 4;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd25, %rd18, 8;
	mov.u64 	%rd24, %rd2;

$L__BB69_4:
	ld.global.nc.f32 	%f4, [%rd25+-8];
	ld.global.nc.f32 	%f5, [%rd24];
	mul.ftz.f32 	%f6, %f5, %f4;
	cvt.ftz.f64.f32 	%fd11, %f6;
	add.f64 	%fd12, %fd23, %fd11;
	ld.global.nc.f32 	%f7, [%rd25+-4];
	ld.global.nc.f32 	%f8, [%rd24+4];
	mul.ftz.f32 	%f9, %f8, %f7;
	cvt.ftz.f64.f32 	%fd13, %f9;
	add.f64 	%fd14, %fd12, %fd13;
	ld.global.nc.f32 	%f10, [%rd25];
	ld.global.nc.f32 	%f11, [%rd24+8];
	mul.ftz.f32 	%f12, %f11, %f10;
	cvt.ftz.f64.f32 	%fd15, %f12;
	add.f64 	%fd16, %fd14, %fd15;
	ld.global.nc.f32 	%f13, [%rd25+4];
	ld.global.nc.f32 	%f14, [%rd24+12];
	mul.ftz.f32 	%f15, %f14, %f13;
	cvt.ftz.f64.f32 	%fd17, %f15;
	add.f64 	%fd23, %fd16, %fd17;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd25, %rd25, 16;
	add.s64 	%rd24, %rd24, 16;
	add.s32 	%r22, %r22, -4;
	setp.ne.s32 	%p4, %r22, 0;
	@%p4 bra 	$L__BB69_4;

$L__BB69_5:
	setp.eq.s32 	%p5, %r24, 0;
	@%p5 bra 	$L__BB69_8;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd19, %r20, 4;
	add.s64 	%rd27, %rd1, %rd19;
	mul.wide.s32 	%rd20, %r23, 4;
	add.s64 	%rd26, %rd2, %rd20;

$L__BB69_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f16, [%rd27];
	ld.global.nc.f32 	%f17, [%rd26];
	mul.ftz.f32 	%f18, %f17, %f16;
	cvt.ftz.f64.f32 	%fd18, %f18;
	add.f64 	%fd23, %fd23, %fd18;
	add.s64 	%rd27, %rd27, 4;
	add.s64 	%rd26, %rd26, 4;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB69_7;

$L__BB69_8:
	cvt.rn.ftz.f32.f64 	%f19, %fd23;

$L__BB69_9:
	cvta.to.global.u64 	%rd21, %rd14;
	mul.wide.s32 	%rd22, %r1, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.f32 	[%rd23], %f19;

$L__BB69_10:
	ret;

}
	// .globl	matvec_kernel_TYPE
.visible .entry matvec_kernel_TYPE(
	.param .u64 matvec_kernel_TYPE_param_0,
	.param .u64 matvec_kernel_TYPE_param_1,
	.param .u64 matvec_kernel_TYPE_param_2,
	.param .u32 matvec_kernel_TYPE_param_3,
	.param .u32 matvec_kernel_TYPE_param_4
)
{
	.reg .pred 	%p<197>;
	.reg .b16 	%rs<299>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<217>;
	.reg .b64 	%rd<75>;
	// demoted variable
	.shared .align 2 .b8 _ZZ18matvec_kernel_TYPEE8x_shared[64];

	ld.param.u64 	%rd5, [matvec_kernel_TYPE_param_0];
	ld.param.u64 	%rd3, [matvec_kernel_TYPE_param_1];
	ld.param.u64 	%rd4, [matvec_kernel_TYPE_param_2];
	ld.param.u32 	%r48, [matvec_kernel_TYPE_param_3];
	ld.param.u32 	%r49, [matvec_kernel_TYPE_param_4];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r50, %ntid.x;
	mov.u32 	%r51, %ctaid.x;
	mov.u32 	%r213, %tid.x;
	mad.lo.s32 	%r2, %r51, %r50, %r213;
	add.s32 	%r52, %r49, 31;
	shr.u32 	%r3, %r52, 5;
	setp.eq.s32 	%p1, %r3, 0;
	mov.u16 	%rs264, 0;
	@%p1 bra 	$L__BB70_101;

	shl.b32 	%r53, %r213, 1;
	mov.u32 	%r54, _ZZ18matvec_kernel_TYPEE8x_shared;
	add.s32 	%r4, %r54, %r53;
	neg.s32 	%r216, %r3;
	mul.lo.s32 	%r214, %r2, %r49;
	add.s32 	%r215, %r214, 31;
	cvta.to.global.u64 	%rd2, %rd3;

$L__BB70_2:
	setp.ge.u32 	%p2, %r213, %r49;
	mov.u16 	%rs265, 0;
	@%p2 bra 	$L__BB70_4;

	mul.wide.u32 	%rd6, %r213, 2;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.nc.u16 	%rs265, [%rd7];

$L__BB70_4:
	st.shared.u16 	[%r4], %rs265;
	bar.sync 	0;
	// begin inline asm
	{ mov.b32 %f1, {0,%rs264};}

	// end inline asm
	mul.wide.u32 	%rd8, %r214, 2;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.nc.u16 	%rs105, [%rd9];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs105};}

	// end inline asm
	ld.shared.u16 	%rs106, [_ZZ18matvec_kernel_TYPEE8x_shared];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs106};}

	// end inline asm
	fma.rn.ftz.f32 	%f4, %f2, %f3, %f1;
	mov.b32 	%r55, %f4;
	and.b32  	%r56, %r55, 2147483647;
	setp.gt.u32 	%p3, %r56, 2139095040;
	shl.b32 	%r57, %r55, 16;
	shr.u32 	%r58, %r55, 16;
	cvt.u16.u32 	%rs107, %r58;
	selp.b32 	%r12, 0, %r57, %p3;
	selp.b16 	%rs266, 32767, %rs107, %p3;
	setp.gt.u32 	%p4, %r12, -2147483648;
	@%p4 bra 	$L__BB70_6;

	setp.ne.s32 	%p5, %r12, -2147483648;
	and.b16  	%rs108, %rs266, 1;
	setp.eq.b16 	%p6, %rs108, 1;
	not.pred 	%p7, %p6;
	or.pred  	%p8, %p5, %p7;
	@%p8 bra 	$L__BB70_7;

$L__BB70_6:
	add.s16 	%rs266, %rs266, 1;

$L__BB70_7:
	// begin inline asm
	{ mov.b32 %f5, {0,%rs266};}

	// end inline asm
	add.s32 	%r59, %r214, 1;
	mul.wide.u32 	%rd10, %r59, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.u16 	%rs110, [%rd11];
	// begin inline asm
	{ mov.b32 %f6, {0,%rs110};}

	// end inline asm
	ld.shared.u16 	%rs111, [_ZZ18matvec_kernel_TYPEE8x_shared+2];
	// begin inline asm
	{ mov.b32 %f7, {0,%rs111};}

	// end inline asm
	fma.rn.ftz.f32 	%f8, %f6, %f7, %f5;
	mov.b32 	%r60, %f8;
	and.b32  	%r61, %r60, 2147483647;
	setp.gt.u32 	%p9, %r61, 2139095040;
	shl.b32 	%r62, %r60, 16;
	shr.u32 	%r63, %r60, 16;
	cvt.u16.u32 	%rs112, %r63;
	selp.b32 	%r13, 0, %r62, %p9;
	selp.b16 	%rs267, 32767, %rs112, %p9;
	setp.gt.u32 	%p10, %r13, -2147483648;
	@%p10 bra 	$L__BB70_9;

	setp.ne.s32 	%p11, %r13, -2147483648;
	and.b16  	%rs113, %rs267, 1;
	setp.eq.b16 	%p12, %rs113, 1;
	not.pred 	%p13, %p12;
	or.pred  	%p14, %p11, %p13;
	@%p14 bra 	$L__BB70_10;

$L__BB70_9:
	add.s16 	%rs267, %rs267, 1;

$L__BB70_10:
	// begin inline asm
	{ mov.b32 %f9, {0,%rs267};}

	// end inline asm
	add.s32 	%r64, %r214, 2;
	mul.wide.u32 	%rd12, %r64, 2;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.u16 	%rs115, [%rd13];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs115};}

	// end inline asm
	ld.shared.u16 	%rs116, [_ZZ18matvec_kernel_TYPEE8x_shared+4];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs116};}

	// end inline asm
	fma.rn.ftz.f32 	%f12, %f10, %f11, %f9;
	mov.b32 	%r65, %f12;
	and.b32  	%r66, %r65, 2147483647;
	setp.gt.u32 	%p15, %r66, 2139095040;
	shl.b32 	%r67, %r65, 16;
	shr.u32 	%r68, %r65, 16;
	cvt.u16.u32 	%rs117, %r68;
	selp.b32 	%r14, 0, %r67, %p15;
	selp.b16 	%rs268, 32767, %rs117, %p15;
	setp.gt.u32 	%p16, %r14, -2147483648;
	@%p16 bra 	$L__BB70_12;

	setp.ne.s32 	%p17, %r14, -2147483648;
	and.b16  	%rs118, %rs268, 1;
	setp.eq.b16 	%p18, %rs118, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB70_13;

$L__BB70_12:
	add.s16 	%rs268, %rs268, 1;

$L__BB70_13:
	// begin inline asm
	{ mov.b32 %f13, {0,%rs268};}

	// end inline asm
	add.s32 	%r69, %r214, 3;
	mul.wide.u32 	%rd14, %r69, 2;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.nc.u16 	%rs120, [%rd15];
	// begin inline asm
	{ mov.b32 %f14, {0,%rs120};}

	// end inline asm
	ld.shared.u16 	%rs121, [_ZZ18matvec_kernel_TYPEE8x_shared+6];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs121};}

	// end inline asm
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	mov.b32 	%r70, %f16;
	and.b32  	%r71, %r70, 2147483647;
	setp.gt.u32 	%p21, %r71, 2139095040;
	shl.b32 	%r72, %r70, 16;
	shr.u32 	%r73, %r70, 16;
	cvt.u16.u32 	%rs122, %r73;
	selp.b32 	%r15, 0, %r72, %p21;
	selp.b16 	%rs269, 32767, %rs122, %p21;
	setp.gt.u32 	%p22, %r15, -2147483648;
	@%p22 bra 	$L__BB70_15;

	setp.ne.s32 	%p23, %r15, -2147483648;
	and.b16  	%rs123, %rs269, 1;
	setp.eq.b16 	%p24, %rs123, 1;
	not.pred 	%p25, %p24;
	or.pred  	%p26, %p23, %p25;
	@%p26 bra 	$L__BB70_16;

$L__BB70_15:
	add.s16 	%rs269, %rs269, 1;

$L__BB70_16:
	// begin inline asm
	{ mov.b32 %f17, {0,%rs269};}

	// end inline asm
	add.s32 	%r74, %r214, 4;
	mul.wide.u32 	%rd16, %r74, 2;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.u16 	%rs125, [%rd17];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs125};}

	// end inline asm
	ld.shared.u16 	%rs126, [_ZZ18matvec_kernel_TYPEE8x_shared+8];
	// begin inline asm
	{ mov.b32 %f19, {0,%rs126};}

	// end inline asm
	fma.rn.ftz.f32 	%f20, %f18, %f19, %f17;
	mov.b32 	%r75, %f20;
	and.b32  	%r76, %r75, 2147483647;
	setp.gt.u32 	%p27, %r76, 2139095040;
	shl.b32 	%r77, %r75, 16;
	shr.u32 	%r78, %r75, 16;
	cvt.u16.u32 	%rs127, %r78;
	selp.b32 	%r16, 0, %r77, %p27;
	selp.b16 	%rs270, 32767, %rs127, %p27;
	setp.gt.u32 	%p28, %r16, -2147483648;
	@%p28 bra 	$L__BB70_18;

	setp.ne.s32 	%p29, %r16, -2147483648;
	and.b16  	%rs128, %rs270, 1;
	setp.eq.b16 	%p30, %rs128, 1;
	not.pred 	%p31, %p30;
	or.pred  	%p32, %p29, %p31;
	@%p32 bra 	$L__BB70_19;

$L__BB70_18:
	add.s16 	%rs270, %rs270, 1;

$L__BB70_19:
	// begin inline asm
	{ mov.b32 %f21, {0,%rs270};}

	// end inline asm
	add.s32 	%r79, %r214, 5;
	mul.wide.u32 	%rd18, %r79, 2;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.nc.u16 	%rs130, [%rd19];
	// begin inline asm
	{ mov.b32 %f22, {0,%rs130};}

	// end inline asm
	ld.shared.u16 	%rs131, [_ZZ18matvec_kernel_TYPEE8x_shared+10];
	// begin inline asm
	{ mov.b32 %f23, {0,%rs131};}

	// end inline asm
	fma.rn.ftz.f32 	%f24, %f22, %f23, %f21;
	mov.b32 	%r80, %f24;
	and.b32  	%r81, %r80, 2147483647;
	setp.gt.u32 	%p33, %r81, 2139095040;
	shl.b32 	%r82, %r80, 16;
	shr.u32 	%r83, %r80, 16;
	cvt.u16.u32 	%rs132, %r83;
	selp.b32 	%r17, 0, %r82, %p33;
	selp.b16 	%rs271, 32767, %rs132, %p33;
	setp.gt.u32 	%p34, %r17, -2147483648;
	@%p34 bra 	$L__BB70_21;

	setp.ne.s32 	%p35, %r17, -2147483648;
	and.b16  	%rs133, %rs271, 1;
	setp.eq.b16 	%p36, %rs133, 1;
	not.pred 	%p37, %p36;
	or.pred  	%p38, %p35, %p37;
	@%p38 bra 	$L__BB70_22;

$L__BB70_21:
	add.s16 	%rs271, %rs271, 1;

$L__BB70_22:
	// begin inline asm
	{ mov.b32 %f25, {0,%rs271};}

	// end inline asm
	add.s32 	%r84, %r214, 6;
	mul.wide.u32 	%rd20, %r84, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.u16 	%rs135, [%rd21];
	// begin inline asm
	{ mov.b32 %f26, {0,%rs135};}

	// end inline asm
	ld.shared.u16 	%rs136, [_ZZ18matvec_kernel_TYPEE8x_shared+12];
	// begin inline asm
	{ mov.b32 %f27, {0,%rs136};}

	// end inline asm
	fma.rn.ftz.f32 	%f28, %f26, %f27, %f25;
	mov.b32 	%r85, %f28;
	and.b32  	%r86, %r85, 2147483647;
	setp.gt.u32 	%p39, %r86, 2139095040;
	shl.b32 	%r87, %r85, 16;
	shr.u32 	%r88, %r85, 16;
	cvt.u16.u32 	%rs137, %r88;
	selp.b32 	%r18, 0, %r87, %p39;
	selp.b16 	%rs272, 32767, %rs137, %p39;
	setp.gt.u32 	%p40, %r18, -2147483648;
	@%p40 bra 	$L__BB70_24;

	setp.ne.s32 	%p41, %r18, -2147483648;
	and.b16  	%rs138, %rs272, 1;
	setp.eq.b16 	%p42, %rs138, 1;
	not.pred 	%p43, %p42;
	or.pred  	%p44, %p41, %p43;
	@%p44 bra 	$L__BB70_25;

$L__BB70_24:
	add.s16 	%rs272, %rs272, 1;

$L__BB70_25:
	// begin inline asm
	{ mov.b32 %f29, {0,%rs272};}

	// end inline asm
	add.s32 	%r89, %r214, 7;
	mul.wide.u32 	%rd22, %r89, 2;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.u16 	%rs140, [%rd23];
	// begin inline asm
	{ mov.b32 %f30, {0,%rs140};}

	// end inline asm
	ld.shared.u16 	%rs141, [_ZZ18matvec_kernel_TYPEE8x_shared+14];
	// begin inline asm
	{ mov.b32 %f31, {0,%rs141};}

	// end inline asm
	fma.rn.ftz.f32 	%f32, %f30, %f31, %f29;
	mov.b32 	%r90, %f32;
	and.b32  	%r91, %r90, 2147483647;
	setp.gt.u32 	%p45, %r91, 2139095040;
	shl.b32 	%r92, %r90, 16;
	shr.u32 	%r93, %r90, 16;
	cvt.u16.u32 	%rs142, %r93;
	selp.b32 	%r19, 0, %r92, %p45;
	selp.b16 	%rs273, 32767, %rs142, %p45;
	setp.gt.u32 	%p46, %r19, -2147483648;
	@%p46 bra 	$L__BB70_27;

	setp.ne.s32 	%p47, %r19, -2147483648;
	and.b16  	%rs143, %rs273, 1;
	setp.eq.b16 	%p48, %rs143, 1;
	not.pred 	%p49, %p48;
	or.pred  	%p50, %p47, %p49;
	@%p50 bra 	$L__BB70_28;

$L__BB70_27:
	add.s16 	%rs273, %rs273, 1;

$L__BB70_28:
	// begin inline asm
	{ mov.b32 %f33, {0,%rs273};}

	// end inline asm
	add.s32 	%r94, %r214, 8;
	mul.wide.u32 	%rd24, %r94, 2;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.u16 	%rs145, [%rd25];
	// begin inline asm
	{ mov.b32 %f34, {0,%rs145};}

	// end inline asm
	ld.shared.u16 	%rs146, [_ZZ18matvec_kernel_TYPEE8x_shared+16];
	// begin inline asm
	{ mov.b32 %f35, {0,%rs146};}

	// end inline asm
	fma.rn.ftz.f32 	%f36, %f34, %f35, %f33;
	mov.b32 	%r95, %f36;
	and.b32  	%r96, %r95, 2147483647;
	setp.gt.u32 	%p51, %r96, 2139095040;
	shl.b32 	%r97, %r95, 16;
	shr.u32 	%r98, %r95, 16;
	cvt.u16.u32 	%rs147, %r98;
	selp.b32 	%r20, 0, %r97, %p51;
	selp.b16 	%rs274, 32767, %rs147, %p51;
	setp.gt.u32 	%p52, %r20, -2147483648;
	@%p52 bra 	$L__BB70_30;

	setp.ne.s32 	%p53, %r20, -2147483648;
	and.b16  	%rs148, %rs274, 1;
	setp.eq.b16 	%p54, %rs148, 1;
	not.pred 	%p55, %p54;
	or.pred  	%p56, %p53, %p55;
	@%p56 bra 	$L__BB70_31;

$L__BB70_30:
	add.s16 	%rs274, %rs274, 1;

$L__BB70_31:
	// begin inline asm
	{ mov.b32 %f37, {0,%rs274};}

	// end inline asm
	add.s32 	%r99, %r214, 9;
	mul.wide.u32 	%rd26, %r99, 2;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.nc.u16 	%rs150, [%rd27];
	// begin inline asm
	{ mov.b32 %f38, {0,%rs150};}

	// end inline asm
	ld.shared.u16 	%rs151, [_ZZ18matvec_kernel_TYPEE8x_shared+18];
	// begin inline asm
	{ mov.b32 %f39, {0,%rs151};}

	// end inline asm
	fma.rn.ftz.f32 	%f40, %f38, %f39, %f37;
	mov.b32 	%r100, %f40;
	and.b32  	%r101, %r100, 2147483647;
	setp.gt.u32 	%p57, %r101, 2139095040;
	shl.b32 	%r102, %r100, 16;
	shr.u32 	%r103, %r100, 16;
	cvt.u16.u32 	%rs152, %r103;
	selp.b32 	%r21, 0, %r102, %p57;
	selp.b16 	%rs275, 32767, %rs152, %p57;
	setp.gt.u32 	%p58, %r21, -2147483648;
	@%p58 bra 	$L__BB70_33;

	setp.ne.s32 	%p59, %r21, -2147483648;
	and.b16  	%rs153, %rs275, 1;
	setp.eq.b16 	%p60, %rs153, 1;
	not.pred 	%p61, %p60;
	or.pred  	%p62, %p59, %p61;
	@%p62 bra 	$L__BB70_34;

$L__BB70_33:
	add.s16 	%rs275, %rs275, 1;

$L__BB70_34:
	// begin inline asm
	{ mov.b32 %f41, {0,%rs275};}

	// end inline asm
	add.s32 	%r104, %r214, 10;
	mul.wide.u32 	%rd28, %r104, 2;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.nc.u16 	%rs155, [%rd29];
	// begin inline asm
	{ mov.b32 %f42, {0,%rs155};}

	// end inline asm
	ld.shared.u16 	%rs156, [_ZZ18matvec_kernel_TYPEE8x_shared+20];
	// begin inline asm
	{ mov.b32 %f43, {0,%rs156};}

	// end inline asm
	fma.rn.ftz.f32 	%f44, %f42, %f43, %f41;
	mov.b32 	%r105, %f44;
	and.b32  	%r106, %r105, 2147483647;
	setp.gt.u32 	%p63, %r106, 2139095040;
	shl.b32 	%r107, %r105, 16;
	shr.u32 	%r108, %r105, 16;
	cvt.u16.u32 	%rs157, %r108;
	selp.b32 	%r22, 0, %r107, %p63;
	selp.b16 	%rs276, 32767, %rs157, %p63;
	setp.gt.u32 	%p64, %r22, -2147483648;
	@%p64 bra 	$L__BB70_36;

	setp.ne.s32 	%p65, %r22, -2147483648;
	and.b16  	%rs158, %rs276, 1;
	setp.eq.b16 	%p66, %rs158, 1;
	not.pred 	%p67, %p66;
	or.pred  	%p68, %p65, %p67;
	@%p68 bra 	$L__BB70_37;

$L__BB70_36:
	add.s16 	%rs276, %rs276, 1;

$L__BB70_37:
	// begin inline asm
	{ mov.b32 %f45, {0,%rs276};}

	// end inline asm
	add.s32 	%r109, %r214, 11;
	mul.wide.u32 	%rd30, %r109, 2;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.nc.u16 	%rs160, [%rd31];
	// begin inline asm
	{ mov.b32 %f46, {0,%rs160};}

	// end inline asm
	ld.shared.u16 	%rs161, [_ZZ18matvec_kernel_TYPEE8x_shared+22];
	// begin inline asm
	{ mov.b32 %f47, {0,%rs161};}

	// end inline asm
	fma.rn.ftz.f32 	%f48, %f46, %f47, %f45;
	mov.b32 	%r110, %f48;
	and.b32  	%r111, %r110, 2147483647;
	setp.gt.u32 	%p69, %r111, 2139095040;
	shl.b32 	%r112, %r110, 16;
	shr.u32 	%r113, %r110, 16;
	cvt.u16.u32 	%rs162, %r113;
	selp.b32 	%r23, 0, %r112, %p69;
	selp.b16 	%rs277, 32767, %rs162, %p69;
	setp.gt.u32 	%p70, %r23, -2147483648;
	@%p70 bra 	$L__BB70_39;

	setp.ne.s32 	%p71, %r23, -2147483648;
	and.b16  	%rs163, %rs277, 1;
	setp.eq.b16 	%p72, %rs163, 1;
	not.pred 	%p73, %p72;
	or.pred  	%p74, %p71, %p73;
	@%p74 bra 	$L__BB70_40;

$L__BB70_39:
	add.s16 	%rs277, %rs277, 1;

$L__BB70_40:
	// begin inline asm
	{ mov.b32 %f49, {0,%rs277};}

	// end inline asm
	add.s32 	%r114, %r214, 12;
	mul.wide.u32 	%rd32, %r114, 2;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.nc.u16 	%rs165, [%rd33];
	// begin inline asm
	{ mov.b32 %f50, {0,%rs165};}

	// end inline asm
	ld.shared.u16 	%rs166, [_ZZ18matvec_kernel_TYPEE8x_shared+24];
	// begin inline asm
	{ mov.b32 %f51, {0,%rs166};}

	// end inline asm
	fma.rn.ftz.f32 	%f52, %f50, %f51, %f49;
	mov.b32 	%r115, %f52;
	and.b32  	%r116, %r115, 2147483647;
	setp.gt.u32 	%p75, %r116, 2139095040;
	shl.b32 	%r117, %r115, 16;
	shr.u32 	%r118, %r115, 16;
	cvt.u16.u32 	%rs167, %r118;
	selp.b32 	%r24, 0, %r117, %p75;
	selp.b16 	%rs278, 32767, %rs167, %p75;
	setp.gt.u32 	%p76, %r24, -2147483648;
	@%p76 bra 	$L__BB70_42;

	setp.ne.s32 	%p77, %r24, -2147483648;
	and.b16  	%rs168, %rs278, 1;
	setp.eq.b16 	%p78, %rs168, 1;
	not.pred 	%p79, %p78;
	or.pred  	%p80, %p77, %p79;
	@%p80 bra 	$L__BB70_43;

$L__BB70_42:
	add.s16 	%rs278, %rs278, 1;

$L__BB70_43:
	// begin inline asm
	{ mov.b32 %f53, {0,%rs278};}

	// end inline asm
	add.s32 	%r119, %r214, 13;
	mul.wide.u32 	%rd34, %r119, 2;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.nc.u16 	%rs170, [%rd35];
	// begin inline asm
	{ mov.b32 %f54, {0,%rs170};}

	// end inline asm
	ld.shared.u16 	%rs171, [_ZZ18matvec_kernel_TYPEE8x_shared+26];
	// begin inline asm
	{ mov.b32 %f55, {0,%rs171};}

	// end inline asm
	fma.rn.ftz.f32 	%f56, %f54, %f55, %f53;
	mov.b32 	%r120, %f56;
	and.b32  	%r121, %r120, 2147483647;
	setp.gt.u32 	%p81, %r121, 2139095040;
	shl.b32 	%r122, %r120, 16;
	shr.u32 	%r123, %r120, 16;
	cvt.u16.u32 	%rs172, %r123;
	selp.b32 	%r25, 0, %r122, %p81;
	selp.b16 	%rs279, 32767, %rs172, %p81;
	setp.gt.u32 	%p82, %r25, -2147483648;
	@%p82 bra 	$L__BB70_45;

	setp.ne.s32 	%p83, %r25, -2147483648;
	and.b16  	%rs173, %rs279, 1;
	setp.eq.b16 	%p84, %rs173, 1;
	not.pred 	%p85, %p84;
	or.pred  	%p86, %p83, %p85;
	@%p86 bra 	$L__BB70_46;

$L__BB70_45:
	add.s16 	%rs279, %rs279, 1;

$L__BB70_46:
	// begin inline asm
	{ mov.b32 %f57, {0,%rs279};}

	// end inline asm
	add.s32 	%r124, %r214, 14;
	mul.wide.u32 	%rd36, %r124, 2;
	add.s64 	%rd37, %rd1, %rd36;
	ld.global.nc.u16 	%rs175, [%rd37];
	// begin inline asm
	{ mov.b32 %f58, {0,%rs175};}

	// end inline asm
	ld.shared.u16 	%rs176, [_ZZ18matvec_kernel_TYPEE8x_shared+28];
	// begin inline asm
	{ mov.b32 %f59, {0,%rs176};}

	// end inline asm
	fma.rn.ftz.f32 	%f60, %f58, %f59, %f57;
	mov.b32 	%r125, %f60;
	and.b32  	%r126, %r125, 2147483647;
	setp.gt.u32 	%p87, %r126, 2139095040;
	shl.b32 	%r127, %r125, 16;
	shr.u32 	%r128, %r125, 16;
	cvt.u16.u32 	%rs177, %r128;
	selp.b32 	%r26, 0, %r127, %p87;
	selp.b16 	%rs280, 32767, %rs177, %p87;
	setp.gt.u32 	%p88, %r26, -2147483648;
	@%p88 bra 	$L__BB70_48;

	setp.ne.s32 	%p89, %r26, -2147483648;
	and.b16  	%rs178, %rs280, 1;
	setp.eq.b16 	%p90, %rs178, 1;
	not.pred 	%p91, %p90;
	or.pred  	%p92, %p89, %p91;
	@%p92 bra 	$L__BB70_49;

$L__BB70_48:
	add.s16 	%rs280, %rs280, 1;

$L__BB70_49:
	// begin inline asm
	{ mov.b32 %f61, {0,%rs280};}

	// end inline asm
	add.s32 	%r129, %r214, 15;
	mul.wide.u32 	%rd38, %r129, 2;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.nc.u16 	%rs180, [%rd39];
	// begin inline asm
	{ mov.b32 %f62, {0,%rs180};}

	// end inline asm
	ld.shared.u16 	%rs181, [_ZZ18matvec_kernel_TYPEE8x_shared+30];
	// begin inline asm
	{ mov.b32 %f63, {0,%rs181};}

	// end inline asm
	fma.rn.ftz.f32 	%f64, %f62, %f63, %f61;
	mov.b32 	%r130, %f64;
	and.b32  	%r131, %r130, 2147483647;
	setp.gt.u32 	%p93, %r131, 2139095040;
	shl.b32 	%r132, %r130, 16;
	shr.u32 	%r133, %r130, 16;
	cvt.u16.u32 	%rs182, %r133;
	selp.b32 	%r27, 0, %r132, %p93;
	selp.b16 	%rs281, 32767, %rs182, %p93;
	setp.gt.u32 	%p94, %r27, -2147483648;
	@%p94 bra 	$L__BB70_51;

	setp.ne.s32 	%p95, %r27, -2147483648;
	and.b16  	%rs183, %rs281, 1;
	setp.eq.b16 	%p96, %rs183, 1;
	not.pred 	%p97, %p96;
	or.pred  	%p98, %p95, %p97;
	@%p98 bra 	$L__BB70_52;

$L__BB70_51:
	add.s16 	%rs281, %rs281, 1;

$L__BB70_52:
	// begin inline asm
	{ mov.b32 %f65, {0,%rs281};}

	// end inline asm
	add.s32 	%r134, %r214, 16;
	mul.wide.u32 	%rd40, %r134, 2;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.nc.u16 	%rs185, [%rd41];
	// begin inline asm
	{ mov.b32 %f66, {0,%rs185};}

	// end inline asm
	ld.shared.u16 	%rs186, [_ZZ18matvec_kernel_TYPEE8x_shared+32];
	// begin inline asm
	{ mov.b32 %f67, {0,%rs186};}

	// end inline asm
	fma.rn.ftz.f32 	%f68, %f66, %f67, %f65;
	mov.b32 	%r135, %f68;
	and.b32  	%r136, %r135, 2147483647;
	setp.gt.u32 	%p99, %r136, 2139095040;
	shl.b32 	%r137, %r135, 16;
	shr.u32 	%r138, %r135, 16;
	cvt.u16.u32 	%rs187, %r138;
	selp.b32 	%r28, 0, %r137, %p99;
	selp.b16 	%rs282, 32767, %rs187, %p99;
	setp.gt.u32 	%p100, %r28, -2147483648;
	@%p100 bra 	$L__BB70_54;

	setp.ne.s32 	%p101, %r28, -2147483648;
	and.b16  	%rs188, %rs282, 1;
	setp.eq.b16 	%p102, %rs188, 1;
	not.pred 	%p103, %p102;
	or.pred  	%p104, %p101, %p103;
	@%p104 bra 	$L__BB70_55;

$L__BB70_54:
	add.s16 	%rs282, %rs282, 1;

$L__BB70_55:
	// begin inline asm
	{ mov.b32 %f69, {0,%rs282};}

	// end inline asm
	add.s32 	%r139, %r214, 17;
	mul.wide.u32 	%rd42, %r139, 2;
	add.s64 	%rd43, %rd1, %rd42;
	ld.global.nc.u16 	%rs190, [%rd43];
	// begin inline asm
	{ mov.b32 %f70, {0,%rs190};}

	// end inline asm
	ld.shared.u16 	%rs191, [_ZZ18matvec_kernel_TYPEE8x_shared+34];
	// begin inline asm
	{ mov.b32 %f71, {0,%rs191};}

	// end inline asm
	fma.rn.ftz.f32 	%f72, %f70, %f71, %f69;
	mov.b32 	%r140, %f72;
	and.b32  	%r141, %r140, 2147483647;
	setp.gt.u32 	%p105, %r141, 2139095040;
	shl.b32 	%r142, %r140, 16;
	shr.u32 	%r143, %r140, 16;
	cvt.u16.u32 	%rs192, %r143;
	selp.b32 	%r29, 0, %r142, %p105;
	selp.b16 	%rs283, 32767, %rs192, %p105;
	setp.gt.u32 	%p106, %r29, -2147483648;
	@%p106 bra 	$L__BB70_57;

	setp.ne.s32 	%p107, %r29, -2147483648;
	and.b16  	%rs193, %rs283, 1;
	setp.eq.b16 	%p108, %rs193, 1;
	not.pred 	%p109, %p108;
	or.pred  	%p110, %p107, %p109;
	@%p110 bra 	$L__BB70_58;

$L__BB70_57:
	add.s16 	%rs283, %rs283, 1;

$L__BB70_58:
	// begin inline asm
	{ mov.b32 %f73, {0,%rs283};}

	// end inline asm
	add.s32 	%r144, %r214, 18;
	mul.wide.u32 	%rd44, %r144, 2;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.nc.u16 	%rs195, [%rd45];
	// begin inline asm
	{ mov.b32 %f74, {0,%rs195};}

	// end inline asm
	ld.shared.u16 	%rs196, [_ZZ18matvec_kernel_TYPEE8x_shared+36];
	// begin inline asm
	{ mov.b32 %f75, {0,%rs196};}

	// end inline asm
	fma.rn.ftz.f32 	%f76, %f74, %f75, %f73;
	mov.b32 	%r145, %f76;
	and.b32  	%r146, %r145, 2147483647;
	setp.gt.u32 	%p111, %r146, 2139095040;
	shl.b32 	%r147, %r145, 16;
	shr.u32 	%r148, %r145, 16;
	cvt.u16.u32 	%rs197, %r148;
	selp.b32 	%r30, 0, %r147, %p111;
	selp.b16 	%rs284, 32767, %rs197, %p111;
	setp.gt.u32 	%p112, %r30, -2147483648;
	@%p112 bra 	$L__BB70_60;

	setp.ne.s32 	%p113, %r30, -2147483648;
	and.b16  	%rs198, %rs284, 1;
	setp.eq.b16 	%p114, %rs198, 1;
	not.pred 	%p115, %p114;
	or.pred  	%p116, %p113, %p115;
	@%p116 bra 	$L__BB70_61;

$L__BB70_60:
	add.s16 	%rs284, %rs284, 1;

$L__BB70_61:
	// begin inline asm
	{ mov.b32 %f77, {0,%rs284};}

	// end inline asm
	add.s32 	%r149, %r214, 19;
	mul.wide.u32 	%rd46, %r149, 2;
	add.s64 	%rd47, %rd1, %rd46;
	ld.global.nc.u16 	%rs200, [%rd47];
	// begin inline asm
	{ mov.b32 %f78, {0,%rs200};}

	// end inline asm
	ld.shared.u16 	%rs201, [_ZZ18matvec_kernel_TYPEE8x_shared+38];
	// begin inline asm
	{ mov.b32 %f79, {0,%rs201};}

	// end inline asm
	fma.rn.ftz.f32 	%f80, %f78, %f79, %f77;
	mov.b32 	%r150, %f80;
	and.b32  	%r151, %r150, 2147483647;
	setp.gt.u32 	%p117, %r151, 2139095040;
	shl.b32 	%r152, %r150, 16;
	shr.u32 	%r153, %r150, 16;
	cvt.u16.u32 	%rs202, %r153;
	selp.b32 	%r31, 0, %r152, %p117;
	selp.b16 	%rs285, 32767, %rs202, %p117;
	setp.gt.u32 	%p118, %r31, -2147483648;
	@%p118 bra 	$L__BB70_63;

	setp.ne.s32 	%p119, %r31, -2147483648;
	and.b16  	%rs203, %rs285, 1;
	setp.eq.b16 	%p120, %rs203, 1;
	not.pred 	%p121, %p120;
	or.pred  	%p122, %p119, %p121;
	@%p122 bra 	$L__BB70_64;

$L__BB70_63:
	add.s16 	%rs285, %rs285, 1;

$L__BB70_64:
	// begin inline asm
	{ mov.b32 %f81, {0,%rs285};}

	// end inline asm
	add.s32 	%r154, %r214, 20;
	mul.wide.u32 	%rd48, %r154, 2;
	add.s64 	%rd49, %rd1, %rd48;
	ld.global.nc.u16 	%rs205, [%rd49];
	// begin inline asm
	{ mov.b32 %f82, {0,%rs205};}

	// end inline asm
	ld.shared.u16 	%rs206, [_ZZ18matvec_kernel_TYPEE8x_shared+40];
	// begin inline asm
	{ mov.b32 %f83, {0,%rs206};}

	// end inline asm
	fma.rn.ftz.f32 	%f84, %f82, %f83, %f81;
	mov.b32 	%r155, %f84;
	and.b32  	%r156, %r155, 2147483647;
	setp.gt.u32 	%p123, %r156, 2139095040;
	shl.b32 	%r157, %r155, 16;
	shr.u32 	%r158, %r155, 16;
	cvt.u16.u32 	%rs207, %r158;
	selp.b32 	%r32, 0, %r157, %p123;
	selp.b16 	%rs286, 32767, %rs207, %p123;
	setp.gt.u32 	%p124, %r32, -2147483648;
	@%p124 bra 	$L__BB70_66;

	setp.ne.s32 	%p125, %r32, -2147483648;
	and.b16  	%rs208, %rs286, 1;
	setp.eq.b16 	%p126, %rs208, 1;
	not.pred 	%p127, %p126;
	or.pred  	%p128, %p125, %p127;
	@%p128 bra 	$L__BB70_67;

$L__BB70_66:
	add.s16 	%rs286, %rs286, 1;

$L__BB70_67:
	// begin inline asm
	{ mov.b32 %f85, {0,%rs286};}

	// end inline asm
	add.s32 	%r159, %r214, 21;
	mul.wide.u32 	%rd50, %r159, 2;
	add.s64 	%rd51, %rd1, %rd50;
	ld.global.nc.u16 	%rs210, [%rd51];
	// begin inline asm
	{ mov.b32 %f86, {0,%rs210};}

	// end inline asm
	ld.shared.u16 	%rs211, [_ZZ18matvec_kernel_TYPEE8x_shared+42];
	// begin inline asm
	{ mov.b32 %f87, {0,%rs211};}

	// end inline asm
	fma.rn.ftz.f32 	%f88, %f86, %f87, %f85;
	mov.b32 	%r160, %f88;
	and.b32  	%r161, %r160, 2147483647;
	setp.gt.u32 	%p129, %r161, 2139095040;
	shl.b32 	%r162, %r160, 16;
	shr.u32 	%r163, %r160, 16;
	cvt.u16.u32 	%rs212, %r163;
	selp.b32 	%r33, 0, %r162, %p129;
	selp.b16 	%rs287, 32767, %rs212, %p129;
	setp.gt.u32 	%p130, %r33, -2147483648;
	@%p130 bra 	$L__BB70_69;

	setp.ne.s32 	%p131, %r33, -2147483648;
	and.b16  	%rs213, %rs287, 1;
	setp.eq.b16 	%p132, %rs213, 1;
	not.pred 	%p133, %p132;
	or.pred  	%p134, %p131, %p133;
	@%p134 bra 	$L__BB70_70;

$L__BB70_69:
	add.s16 	%rs287, %rs287, 1;

$L__BB70_70:
	// begin inline asm
	{ mov.b32 %f89, {0,%rs287};}

	// end inline asm
	add.s32 	%r164, %r214, 22;
	mul.wide.u32 	%rd52, %r164, 2;
	add.s64 	%rd53, %rd1, %rd52;
	ld.global.nc.u16 	%rs215, [%rd53];
	// begin inline asm
	{ mov.b32 %f90, {0,%rs215};}

	// end inline asm
	ld.shared.u16 	%rs216, [_ZZ18matvec_kernel_TYPEE8x_shared+44];
	// begin inline asm
	{ mov.b32 %f91, {0,%rs216};}

	// end inline asm
	fma.rn.ftz.f32 	%f92, %f90, %f91, %f89;
	mov.b32 	%r165, %f92;
	and.b32  	%r166, %r165, 2147483647;
	setp.gt.u32 	%p135, %r166, 2139095040;
	shl.b32 	%r167, %r165, 16;
	shr.u32 	%r168, %r165, 16;
	cvt.u16.u32 	%rs217, %r168;
	selp.b32 	%r34, 0, %r167, %p135;
	selp.b16 	%rs288, 32767, %rs217, %p135;
	setp.gt.u32 	%p136, %r34, -2147483648;
	@%p136 bra 	$L__BB70_72;

	setp.ne.s32 	%p137, %r34, -2147483648;
	and.b16  	%rs218, %rs288, 1;
	setp.eq.b16 	%p138, %rs218, 1;
	not.pred 	%p139, %p138;
	or.pred  	%p140, %p137, %p139;
	@%p140 bra 	$L__BB70_73;

$L__BB70_72:
	add.s16 	%rs288, %rs288, 1;

$L__BB70_73:
	// begin inline asm
	{ mov.b32 %f93, {0,%rs288};}

	// end inline asm
	add.s32 	%r169, %r214, 23;
	mul.wide.u32 	%rd54, %r169, 2;
	add.s64 	%rd55, %rd1, %rd54;
	ld.global.nc.u16 	%rs220, [%rd55];
	// begin inline asm
	{ mov.b32 %f94, {0,%rs220};}

	// end inline asm
	ld.shared.u16 	%rs221, [_ZZ18matvec_kernel_TYPEE8x_shared+46];
	// begin inline asm
	{ mov.b32 %f95, {0,%rs221};}

	// end inline asm
	fma.rn.ftz.f32 	%f96, %f94, %f95, %f93;
	mov.b32 	%r170, %f96;
	and.b32  	%r171, %r170, 2147483647;
	setp.gt.u32 	%p141, %r171, 2139095040;
	shl.b32 	%r172, %r170, 16;
	shr.u32 	%r173, %r170, 16;
	cvt.u16.u32 	%rs222, %r173;
	selp.b32 	%r35, 0, %r172, %p141;
	selp.b16 	%rs289, 32767, %rs222, %p141;
	setp.gt.u32 	%p142, %r35, -2147483648;
	@%p142 bra 	$L__BB70_75;

	setp.ne.s32 	%p143, %r35, -2147483648;
	and.b16  	%rs223, %rs289, 1;
	setp.eq.b16 	%p144, %rs223, 1;
	not.pred 	%p145, %p144;
	or.pred  	%p146, %p143, %p145;
	@%p146 bra 	$L__BB70_76;

$L__BB70_75:
	add.s16 	%rs289, %rs289, 1;

$L__BB70_76:
	// begin inline asm
	{ mov.b32 %f97, {0,%rs289};}

	// end inline asm
	add.s32 	%r174, %r214, 24;
	mul.wide.u32 	%rd56, %r174, 2;
	add.s64 	%rd57, %rd1, %rd56;
	ld.global.nc.u16 	%rs225, [%rd57];
	// begin inline asm
	{ mov.b32 %f98, {0,%rs225};}

	// end inline asm
	ld.shared.u16 	%rs226, [_ZZ18matvec_kernel_TYPEE8x_shared+48];
	// begin inline asm
	{ mov.b32 %f99, {0,%rs226};}

	// end inline asm
	fma.rn.ftz.f32 	%f100, %f98, %f99, %f97;
	mov.b32 	%r175, %f100;
	and.b32  	%r176, %r175, 2147483647;
	setp.gt.u32 	%p147, %r176, 2139095040;
	shl.b32 	%r177, %r175, 16;
	shr.u32 	%r178, %r175, 16;
	cvt.u16.u32 	%rs227, %r178;
	selp.b32 	%r36, 0, %r177, %p147;
	selp.b16 	%rs290, 32767, %rs227, %p147;
	setp.gt.u32 	%p148, %r36, -2147483648;
	@%p148 bra 	$L__BB70_78;

	setp.ne.s32 	%p149, %r36, -2147483648;
	and.b16  	%rs228, %rs290, 1;
	setp.eq.b16 	%p150, %rs228, 1;
	not.pred 	%p151, %p150;
	or.pred  	%p152, %p149, %p151;
	@%p152 bra 	$L__BB70_79;

$L__BB70_78:
	add.s16 	%rs290, %rs290, 1;

$L__BB70_79:
	// begin inline asm
	{ mov.b32 %f101, {0,%rs290};}

	// end inline asm
	add.s32 	%r179, %r215, -6;
	mul.wide.u32 	%rd58, %r179, 2;
	add.s64 	%rd59, %rd1, %rd58;
	ld.global.nc.u16 	%rs230, [%rd59];
	// begin inline asm
	{ mov.b32 %f102, {0,%rs230};}

	// end inline asm
	ld.shared.u16 	%rs231, [_ZZ18matvec_kernel_TYPEE8x_shared+50];
	// begin inline asm
	{ mov.b32 %f103, {0,%rs231};}

	// end inline asm
	fma.rn.ftz.f32 	%f104, %f102, %f103, %f101;
	mov.b32 	%r180, %f104;
	and.b32  	%r181, %r180, 2147483647;
	setp.gt.u32 	%p153, %r181, 2139095040;
	shl.b32 	%r182, %r180, 16;
	shr.u32 	%r183, %r180, 16;
	cvt.u16.u32 	%rs232, %r183;
	selp.b32 	%r37, 0, %r182, %p153;
	selp.b16 	%rs291, 32767, %rs232, %p153;
	setp.gt.u32 	%p154, %r37, -2147483648;
	@%p154 bra 	$L__BB70_81;

	setp.ne.s32 	%p155, %r37, -2147483648;
	and.b16  	%rs233, %rs291, 1;
	setp.eq.b16 	%p156, %rs233, 1;
	not.pred 	%p157, %p156;
	or.pred  	%p158, %p155, %p157;
	@%p158 bra 	$L__BB70_82;

$L__BB70_81:
	add.s16 	%rs291, %rs291, 1;

$L__BB70_82:
	// begin inline asm
	{ mov.b32 %f105, {0,%rs291};}

	// end inline asm
	add.s32 	%r184, %r215, -5;
	mul.wide.u32 	%rd60, %r184, 2;
	add.s64 	%rd61, %rd1, %rd60;
	ld.global.nc.u16 	%rs235, [%rd61];
	// begin inline asm
	{ mov.b32 %f106, {0,%rs235};}

	// end inline asm
	ld.shared.u16 	%rs236, [_ZZ18matvec_kernel_TYPEE8x_shared+52];
	// begin inline asm
	{ mov.b32 %f107, {0,%rs236};}

	// end inline asm
	fma.rn.ftz.f32 	%f108, %f106, %f107, %f105;
	mov.b32 	%r185, %f108;
	and.b32  	%r186, %r185, 2147483647;
	setp.gt.u32 	%p159, %r186, 2139095040;
	shl.b32 	%r187, %r185, 16;
	shr.u32 	%r188, %r185, 16;
	cvt.u16.u32 	%rs237, %r188;
	selp.b32 	%r38, 0, %r187, %p159;
	selp.b16 	%rs292, 32767, %rs237, %p159;
	setp.gt.u32 	%p160, %r38, -2147483648;
	@%p160 bra 	$L__BB70_84;

	setp.ne.s32 	%p161, %r38, -2147483648;
	and.b16  	%rs238, %rs292, 1;
	setp.eq.b16 	%p162, %rs238, 1;
	not.pred 	%p163, %p162;
	or.pred  	%p164, %p161, %p163;
	@%p164 bra 	$L__BB70_85;

$L__BB70_84:
	add.s16 	%rs292, %rs292, 1;

$L__BB70_85:
	// begin inline asm
	{ mov.b32 %f109, {0,%rs292};}

	// end inline asm
	add.s32 	%r189, %r215, -4;
	mul.wide.u32 	%rd62, %r189, 2;
	add.s64 	%rd63, %rd1, %rd62;
	ld.global.nc.u16 	%rs240, [%rd63];
	// begin inline asm
	{ mov.b32 %f110, {0,%rs240};}

	// end inline asm
	ld.shared.u16 	%rs241, [_ZZ18matvec_kernel_TYPEE8x_shared+54];
	// begin inline asm
	{ mov.b32 %f111, {0,%rs241};}

	// end inline asm
	fma.rn.ftz.f32 	%f112, %f110, %f111, %f109;
	mov.b32 	%r190, %f112;
	and.b32  	%r191, %r190, 2147483647;
	setp.gt.u32 	%p165, %r191, 2139095040;
	shl.b32 	%r192, %r190, 16;
	shr.u32 	%r193, %r190, 16;
	cvt.u16.u32 	%rs242, %r193;
	selp.b32 	%r39, 0, %r192, %p165;
	selp.b16 	%rs293, 32767, %rs242, %p165;
	setp.gt.u32 	%p166, %r39, -2147483648;
	@%p166 bra 	$L__BB70_87;

	setp.ne.s32 	%p167, %r39, -2147483648;
	and.b16  	%rs243, %rs293, 1;
	setp.eq.b16 	%p168, %rs243, 1;
	not.pred 	%p169, %p168;
	or.pred  	%p170, %p167, %p169;
	@%p170 bra 	$L__BB70_88;

$L__BB70_87:
	add.s16 	%rs293, %rs293, 1;

$L__BB70_88:
	// begin inline asm
	{ mov.b32 %f113, {0,%rs293};}

	// end inline asm
	add.s32 	%r194, %r215, -3;
	mul.wide.u32 	%rd64, %r194, 2;
	add.s64 	%rd65, %rd1, %rd64;
	ld.global.nc.u16 	%rs245, [%rd65];
	// begin inline asm
	{ mov.b32 %f114, {0,%rs245};}

	// end inline asm
	ld.shared.u16 	%rs246, [_ZZ18matvec_kernel_TYPEE8x_shared+56];
	// begin inline asm
	{ mov.b32 %f115, {0,%rs246};}

	// end inline asm
	fma.rn.ftz.f32 	%f116, %f114, %f115, %f113;
	mov.b32 	%r195, %f116;
	and.b32  	%r196, %r195, 2147483647;
	setp.gt.u32 	%p171, %r196, 2139095040;
	shl.b32 	%r197, %r195, 16;
	shr.u32 	%r198, %r195, 16;
	cvt.u16.u32 	%rs247, %r198;
	selp.b32 	%r40, 0, %r197, %p171;
	selp.b16 	%rs294, 32767, %rs247, %p171;
	setp.gt.u32 	%p172, %r40, -2147483648;
	@%p172 bra 	$L__BB70_90;

	setp.ne.s32 	%p173, %r40, -2147483648;
	and.b16  	%rs248, %rs294, 1;
	setp.eq.b16 	%p174, %rs248, 1;
	not.pred 	%p175, %p174;
	or.pred  	%p176, %p173, %p175;
	@%p176 bra 	$L__BB70_91;

$L__BB70_90:
	add.s16 	%rs294, %rs294, 1;

$L__BB70_91:
	// begin inline asm
	{ mov.b32 %f117, {0,%rs294};}

	// end inline asm
	add.s32 	%r199, %r215, -2;
	mul.wide.u32 	%rd66, %r199, 2;
	add.s64 	%rd67, %rd1, %rd66;
	ld.global.nc.u16 	%rs250, [%rd67];
	// begin inline asm
	{ mov.b32 %f118, {0,%rs250};}

	// end inline asm
	ld.shared.u16 	%rs251, [_ZZ18matvec_kernel_TYPEE8x_shared+58];
	// begin inline asm
	{ mov.b32 %f119, {0,%rs251};}

	// end inline asm
	fma.rn.ftz.f32 	%f120, %f118, %f119, %f117;
	mov.b32 	%r200, %f120;
	and.b32  	%r201, %r200, 2147483647;
	setp.gt.u32 	%p177, %r201, 2139095040;
	shl.b32 	%r202, %r200, 16;
	shr.u32 	%r203, %r200, 16;
	cvt.u16.u32 	%rs252, %r203;
	selp.b32 	%r41, 0, %r202, %p177;
	selp.b16 	%rs295, 32767, %rs252, %p177;
	setp.gt.u32 	%p178, %r41, -2147483648;
	@%p178 bra 	$L__BB70_93;

	setp.ne.s32 	%p179, %r41, -2147483648;
	and.b16  	%rs253, %rs295, 1;
	setp.eq.b16 	%p180, %rs253, 1;
	not.pred 	%p181, %p180;
	or.pred  	%p182, %p179, %p181;
	@%p182 bra 	$L__BB70_94;

$L__BB70_93:
	add.s16 	%rs295, %rs295, 1;

$L__BB70_94:
	// begin inline asm
	{ mov.b32 %f121, {0,%rs295};}

	// end inline asm
	add.s32 	%r204, %r215, -1;
	mul.wide.u32 	%rd68, %r204, 2;
	add.s64 	%rd69, %rd1, %rd68;
	ld.global.nc.u16 	%rs255, [%rd69];
	// begin inline asm
	{ mov.b32 %f122, {0,%rs255};}

	// end inline asm
	ld.shared.u16 	%rs256, [_ZZ18matvec_kernel_TYPEE8x_shared+60];
	// begin inline asm
	{ mov.b32 %f123, {0,%rs256};}

	// end inline asm
	fma.rn.ftz.f32 	%f124, %f122, %f123, %f121;
	mov.b32 	%r205, %f124;
	and.b32  	%r206, %r205, 2147483647;
	setp.gt.u32 	%p183, %r206, 2139095040;
	shl.b32 	%r207, %r205, 16;
	shr.u32 	%r208, %r205, 16;
	cvt.u16.u32 	%rs257, %r208;
	selp.b32 	%r42, 0, %r207, %p183;
	selp.b16 	%rs296, 32767, %rs257, %p183;
	setp.gt.u32 	%p184, %r42, -2147483648;
	@%p184 bra 	$L__BB70_96;

	setp.ne.s32 	%p185, %r42, -2147483648;
	and.b16  	%rs258, %rs296, 1;
	setp.eq.b16 	%p186, %rs258, 1;
	not.pred 	%p187, %p186;
	or.pred  	%p188, %p185, %p187;
	@%p188 bra 	$L__BB70_97;

$L__BB70_96:
	add.s16 	%rs296, %rs296, 1;

$L__BB70_97:
	// begin inline asm
	{ mov.b32 %f125, {0,%rs296};}

	// end inline asm
	mul.wide.u32 	%rd70, %r215, 2;
	add.s64 	%rd71, %rd1, %rd70;
	ld.global.nc.u16 	%rs260, [%rd71];
	// begin inline asm
	{ mov.b32 %f126, {0,%rs260};}

	// end inline asm
	ld.shared.u16 	%rs261, [_ZZ18matvec_kernel_TYPEE8x_shared+62];
	// begin inline asm
	{ mov.b32 %f127, {0,%rs261};}

	// end inline asm
	fma.rn.ftz.f32 	%f128, %f126, %f127, %f125;
	mov.b32 	%r209, %f128;
	and.b32  	%r210, %r209, 2147483647;
	setp.gt.u32 	%p189, %r210, 2139095040;
	shl.b32 	%r211, %r209, 16;
	shr.u32 	%r212, %r209, 16;
	cvt.u16.u32 	%rs262, %r212;
	selp.b32 	%r43, 0, %r211, %p189;
	selp.b16 	%rs264, 32767, %rs262, %p189;
	setp.gt.u32 	%p190, %r43, -2147483648;
	@%p190 bra 	$L__BB70_99;

	setp.ne.s32 	%p191, %r43, -2147483648;
	and.b16  	%rs263, %rs264, 1;
	setp.eq.b16 	%p192, %rs263, 1;
	not.pred 	%p193, %p192;
	or.pred  	%p194, %p191, %p193;
	@%p194 bra 	$L__BB70_100;

$L__BB70_99:
	add.s16 	%rs264, %rs264, 1;

$L__BB70_100:
	bar.sync 	0;
	add.s32 	%r215, %r215, 32;
	add.s32 	%r214, %r214, 32;
	add.s32 	%r213, %r213, 32;
	add.s32 	%r216, %r216, 1;
	setp.ne.s32 	%p195, %r216, 0;
	@%p195 bra 	$L__BB70_2;

$L__BB70_101:
	setp.ge.u32 	%p196, %r2, %r48;
	@%p196 bra 	$L__BB70_103;

	cvta.to.global.u64 	%rd72, %rd4;
	mul.wide.u32 	%rd73, %r2, 2;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.u16 	[%rd74], %rs264;

$L__BB70_103:
	ret;

}
	// .globl	Softmax
.visible .entry Softmax(
	.param .u64 Softmax_param_0,
	.param .u64 Softmax_param_1,
	.param .u32 Softmax_param_2,
	.param .u32 Softmax_param_3
)
{
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<82>;
	.reg .b32 	%r<69>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<49>;


	ld.param.u64 	%rd28, [Softmax_param_0];
	ld.param.u64 	%rd29, [Softmax_param_1];
	ld.param.u32 	%r30, [Softmax_param_2];
	ld.param.u32 	%r31, [Softmax_param_3];
	cvta.to.global.u64 	%rd1, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %tid.x;
	mad.lo.s32 	%r1, %r33, %r32, %r34;
	setp.ge.s32 	%p1, %r1, %r31;
	@%p1 bra 	$L__BB71_24;

	mul.lo.s32 	%r67, %r1, %r30;
	cvt.s64.s32 	%rd3, %r67;
	mul.wide.s32 	%rd30, %r67, 4;
	add.s64 	%rd31, %rd2, %rd30;
	ld.global.nc.f32 	%f75, [%rd31];
	setp.lt.s32 	%p2, %r30, 2;
	@%p2 bra 	$L__BB71_8;

	add.s32 	%r35, %r30, -1;
	and.b32  	%r60, %r35, 3;
	add.s32 	%r36, %r30, -2;
	setp.lt.u32 	%p3, %r36, 3;
	mov.u32 	%r59, %r67;
	@%p3 bra 	$L__BB71_5;

	shl.b64 	%rd32, %rd3, 2;
	add.s64 	%rd33, %rd2, %rd32;
	add.s64 	%rd41, %rd33, 8;
	sub.s32 	%r57, %r30, %r60;
	mov.u32 	%r59, %r67;

$L__BB71_4:
	ld.global.nc.f32 	%f20, [%rd41+-8];
	setp.lt.ftz.f32 	%p4, %f75, %f20;
	selp.f32 	%f21, %f20, %f75, %p4;
	ld.global.nc.f32 	%f22, [%rd41+-4];
	setp.lt.ftz.f32 	%p5, %f21, %f22;
	selp.f32 	%f23, %f22, %f21, %p5;
	ld.global.nc.f32 	%f24, [%rd41];
	setp.lt.ftz.f32 	%p6, %f23, %f24;
	selp.f32 	%f25, %f24, %f23, %p6;
	ld.global.nc.f32 	%f26, [%rd41+4];
	setp.lt.ftz.f32 	%p7, %f25, %f26;
	selp.f32 	%f75, %f26, %f25, %p7;
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd41, %rd41, 16;
	add.s32 	%r57, %r57, -4;
	setp.ne.s32 	%p8, %r57, 1;
	@%p8 bra 	$L__BB71_4;

$L__BB71_5:
	setp.eq.s32 	%p9, %r60, 0;
	@%p9 bra 	$L__BB71_8;

	mul.wide.s32 	%rd34, %r59, 4;
	add.s64 	%rd42, %rd2, %rd34;

$L__BB71_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f27, [%rd42];
	setp.lt.ftz.f32 	%p10, %f75, %f27;
	selp.f32 	%f75, %f27, %f75, %p10;
	add.s64 	%rd42, %rd42, 4;
	add.s32 	%r60, %r60, -1;
	setp.ne.s32 	%p11, %r60, 0;
	@%p11 bra 	$L__BB71_7;

$L__BB71_8:
	setp.lt.s32 	%p12, %r30, 1;
	mov.f32 	%f80, 0f00000000;
	@%p12 bra 	$L__BB71_15;

	add.s32 	%r37, %r30, -1;
	and.b32  	%r64, %r30, 3;
	setp.lt.u32 	%p13, %r37, 3;
	mov.f32 	%f80, 0f00000000;
	mov.u32 	%r63, %r67;
	@%p13 bra 	$L__BB71_12;

	sub.s32 	%r62, %r30, %r64;
	shl.b64 	%rd35, %rd3, 2;
	add.s64 	%rd36, %rd35, 8;
	add.s64 	%rd44, %rd2, %rd36;
	add.s64 	%rd43, %rd1, %rd36;
	mov.u32 	%r63, %r67;

$L__BB71_11:
	ld.global.nc.f32 	%f32, [%rd44+-8];
	sub.ftz.f32 	%f33, %f32, %f75;
	mul.ftz.f32 	%f34, %f33, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f35, %f34;
	cvt.ftz.f64.f32 	%fd1, %f35;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd1;
	}
	and.b32  	%r40, %r39, 2147483647;
	setp.eq.s32 	%p14, %r40, 2146435072;
	setp.eq.s32 	%p15, %r38, 0;
	and.pred  	%p16, %p15, %p14;
	selp.f32 	%f36, 0f7F7FFFFF, %f35, %p16;
	st.global.f32 	[%rd43+-8], %f36;
	add.ftz.f32 	%f37, %f80, %f36;
	ld.global.nc.f32 	%f38, [%rd44+-4];
	sub.ftz.f32 	%f39, %f38, %f75;
	mul.ftz.f32 	%f40, %f39, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f41, %f40;
	cvt.ftz.f64.f32 	%fd2, %f41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd2;
	}
	and.b32  	%r43, %r42, 2147483647;
	setp.eq.s32 	%p17, %r43, 2146435072;
	setp.eq.s32 	%p18, %r41, 0;
	and.pred  	%p19, %p18, %p17;
	selp.f32 	%f42, 0f7F7FFFFF, %f41, %p19;
	st.global.f32 	[%rd43+-4], %f42;
	add.ftz.f32 	%f43, %f37, %f42;
	ld.global.nc.f32 	%f44, [%rd44];
	sub.ftz.f32 	%f45, %f44, %f75;
	mul.ftz.f32 	%f46, %f45, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f47, %f46;
	cvt.ftz.f64.f32 	%fd3, %f47;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd3;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd3;
	}
	and.b32  	%r46, %r45, 2147483647;
	setp.eq.s32 	%p20, %r46, 2146435072;
	setp.eq.s32 	%p21, %r44, 0;
	and.pred  	%p22, %p21, %p20;
	selp.f32 	%f48, 0f7F7FFFFF, %f47, %p22;
	st.global.f32 	[%rd43], %f48;
	add.ftz.f32 	%f49, %f43, %f48;
	ld.global.nc.f32 	%f50, [%rd44+4];
	sub.ftz.f32 	%f51, %f50, %f75;
	mul.ftz.f32 	%f52, %f51, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f53, %f52;
	cvt.ftz.f64.f32 	%fd4, %f53;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd4;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd4;
	}
	and.b32  	%r49, %r48, 2147483647;
	setp.eq.s32 	%p23, %r49, 2146435072;
	setp.eq.s32 	%p24, %r47, 0;
	and.pred  	%p25, %p24, %p23;
	selp.f32 	%f54, 0f7F7FFFFF, %f53, %p25;
	st.global.f32 	[%rd43+4], %f54;
	add.ftz.f32 	%f80, %f49, %f54;
	add.s32 	%r63, %r63, 4;
	add.s64 	%rd44, %rd44, 16;
	add.s64 	%rd43, %rd43, 16;
	add.s32 	%r62, %r62, -4;
	setp.ne.s32 	%p26, %r62, 0;
	@%p26 bra 	$L__BB71_11;

$L__BB71_12:
	setp.eq.s32 	%p27, %r64, 0;
	@%p27 bra 	$L__BB71_15;

	mul.wide.s32 	%rd37, %r63, 4;
	add.s64 	%rd46, %rd1, %rd37;
	add.s64 	%rd45, %rd2, %rd37;

$L__BB71_14:
	.pragma "nounroll";
	ld.global.nc.f32 	%f55, [%rd45];
	sub.ftz.f32 	%f56, %f55, %f75;
	mul.ftz.f32 	%f57, %f56, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f58, %f57;
	cvt.ftz.f64.f32 	%fd5, %f58;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %fd5;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd5;
	}
	and.b32  	%r52, %r51, 2147483647;
	setp.eq.s32 	%p28, %r52, 2146435072;
	setp.eq.s32 	%p29, %r50, 0;
	and.pred  	%p30, %p29, %p28;
	selp.f32 	%f59, 0f7F7FFFFF, %f58, %p30;
	st.global.f32 	[%rd46], %f59;
	add.ftz.f32 	%f80, %f80, %f59;
	add.s64 	%rd46, %rd46, 4;
	add.s64 	%rd45, %rd45, 4;
	add.s32 	%r64, %r64, -1;
	setp.ne.s32 	%p31, %r64, 0;
	@%p31 bra 	$L__BB71_14;

$L__BB71_15:
	setp.neu.ftz.f32 	%p32, %f80, 0f00000000;
	@%p32 bra 	$L__BB71_17;

	ld.const.u16 	%rs1, [sh+10];
	// begin inline asm
	{ mov.b32 %f60, {0,%rs1};}

	// end inline asm
	add.ftz.f32 	%f80, %f80, %f60;

$L__BB71_17:
	cvt.ftz.f64.f32 	%fd6, %f80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd6;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd6;
	}
	and.b32  	%r55, %r54, 2147483647;
	setp.eq.s32 	%p33, %r55, 2146435072;
	setp.eq.s32 	%p34, %r53, 0;
	and.pred  	%p35, %p34, %p33;
	selp.f32 	%f18, 0f7F7FFFFF, %f80, %p35;
	@%p12 bra 	$L__BB71_24;

	add.s32 	%r56, %r30, -1;
	and.b32  	%r68, %r30, 3;
	setp.lt.u32 	%p37, %r56, 3;
	@%p37 bra 	$L__BB71_21;

	sub.s32 	%r66, %r30, %r68;
	shl.b64 	%rd38, %rd3, 2;
	add.s64 	%rd39, %rd1, %rd38;
	add.s64 	%rd47, %rd39, 8;

$L__BB71_20:
	ld.global.f32 	%f61, [%rd47+-8];
	div.approx.ftz.f32 	%f62, %f61, %f18;
	st.global.f32 	[%rd47+-8], %f62;
	ld.global.f32 	%f63, [%rd47+-4];
	div.approx.ftz.f32 	%f64, %f63, %f18;
	st.global.f32 	[%rd47+-4], %f64;
	ld.global.f32 	%f65, [%rd47];
	div.approx.ftz.f32 	%f66, %f65, %f18;
	st.global.f32 	[%rd47], %f66;
	ld.global.f32 	%f67, [%rd47+4];
	div.approx.ftz.f32 	%f68, %f67, %f18;
	st.global.f32 	[%rd47+4], %f68;
	add.s32 	%r67, %r67, 4;
	add.s64 	%rd47, %rd47, 16;
	add.s32 	%r66, %r66, -4;
	setp.ne.s32 	%p38, %r66, 0;
	@%p38 bra 	$L__BB71_20;

$L__BB71_21:
	setp.eq.s32 	%p39, %r68, 0;
	@%p39 bra 	$L__BB71_24;

	mul.wide.s32 	%rd40, %r67, 4;
	add.s64 	%rd48, %rd1, %rd40;

$L__BB71_23:
	.pragma "nounroll";
	ld.global.f32 	%f69, [%rd48];
	div.approx.ftz.f32 	%f70, %f69, %f18;
	st.global.f32 	[%rd48], %f70;
	add.s64 	%rd48, %rd48, 4;
	add.s32 	%r68, %r68, -1;
	setp.ne.s32 	%p40, %r68, 0;
	@%p40 bra 	$L__BB71_23;

$L__BB71_24:
	ret;

}
	// .globl	Softmax_TYPE
.visible .entry Softmax_TYPE(
	.param .u64 Softmax_TYPE_param_0,
	.param .u64 Softmax_TYPE_param_1,
	.param .u32 Softmax_TYPE_param_2,
	.param .u32 Softmax_TYPE_param_3
)
{
	.reg .pred 	%p<130>;
	.reg .b16 	%rs<163>;
	.reg .f32 	%f<63>;
	.reg .b32 	%r<139>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<50>;


	ld.param.u64 	%rd28, [Softmax_TYPE_param_0];
	ld.param.u64 	%rd29, [Softmax_TYPE_param_1];
	ld.param.u32 	%r44, [Softmax_TYPE_param_2];
	ld.param.u32 	%r45, [Softmax_TYPE_param_3];
	cvta.to.global.u64 	%rd1, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	mov.u32 	%r46, %ctaid.x;
	mov.u32 	%r47, %ntid.x;
	mov.u32 	%r48, %tid.x;
	mad.lo.s32 	%r1, %r47, %r46, %r48;
	setp.ge.s32 	%p1, %r1, %r45;
	@%p1 bra 	$L__BB72_69;

	ld.const.u16 	%rs1, [sh];
	mul.lo.s32 	%r137, %r1, %r44;
	cvt.s64.s32 	%rd3, %r137;
	mul.wide.s32 	%rd30, %r137, 2;
	add.s64 	%rd31, %rd2, %rd30;
	ld.global.nc.u16 	%rs143, [%rd31];
	setp.lt.s32 	%p2, %r44, 2;
	@%p2 bra 	$L__BB72_8;

	add.s32 	%r49, %r44, -1;
	and.b32  	%r131, %r49, 3;
	add.s32 	%r50, %r44, -2;
	setp.lt.u32 	%p3, %r50, 3;
	mov.u32 	%r130, %r137;
	@%p3 bra 	$L__BB72_5;

	shl.b64 	%rd32, %rd3, 1;
	add.s64 	%rd33, %rd2, %rd32;
	add.s64 	%rd44, %rd33, 4;
	sub.s32 	%r128, %r44, %r131;
	mov.u32 	%r130, %r137;

$L__BB72_4:
	ld.global.nc.u16 	%rs63, [%rd44+-4];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs143};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f2, {0,%rs63};}

	// end inline asm
	setp.lt.ftz.f32 	%p4, %f1, %f2;
	selp.b16 	%rs64, %rs63, %rs143, %p4;
	ld.global.nc.u16 	%rs65, [%rd44+-2];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs64};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f4, {0,%rs65};}

	// end inline asm
	setp.lt.ftz.f32 	%p5, %f3, %f4;
	selp.b16 	%rs66, %rs65, %rs64, %p5;
	ld.global.nc.u16 	%rs67, [%rd44];
	// begin inline asm
	{ mov.b32 %f5, {0,%rs66};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f6, {0,%rs67};}

	// end inline asm
	setp.lt.ftz.f32 	%p6, %f5, %f6;
	selp.b16 	%rs68, %rs67, %rs66, %p6;
	ld.global.nc.u16 	%rs69, [%rd44+2];
	// begin inline asm
	{ mov.b32 %f7, {0,%rs68};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f8, {0,%rs69};}

	// end inline asm
	setp.lt.ftz.f32 	%p7, %f7, %f8;
	selp.b16 	%rs143, %rs69, %rs68, %p7;
	add.s32 	%r130, %r130, 4;
	add.s64 	%rd44, %rd44, 8;
	add.s32 	%r128, %r128, -4;
	setp.ne.s32 	%p8, %r128, 1;
	@%p8 bra 	$L__BB72_4;

$L__BB72_5:
	setp.eq.s32 	%p9, %r131, 0;
	@%p9 bra 	$L__BB72_8;

	mul.wide.s32 	%rd34, %r130, 2;
	add.s64 	%rd45, %rd2, %rd34;

$L__BB72_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs71, [%rd45];
	// begin inline asm
	{ mov.b32 %f9, {0,%rs143};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f10, {0,%rs71};}

	// end inline asm
	setp.lt.ftz.f32 	%p10, %f9, %f10;
	selp.b16 	%rs143, %rs71, %rs143, %p10;
	add.s64 	%rd45, %rd45, 2;
	add.s32 	%r131, %r131, -1;
	setp.ne.s32 	%p11, %r131, 0;
	@%p11 bra 	$L__BB72_7;

$L__BB72_8:
	setp.lt.s32 	%p12, %r44, 1;
	mov.u16 	%rs155, %rs1;
	@%p12 bra 	$L__BB72_40;

	and.b32  	%r12, %r44, 1;
	setp.eq.s32 	%p13, %r44, 1;
	mov.u32 	%r134, %r137;
	mov.u16 	%rs155, %rs1;
	@%p13 bra 	$L__BB72_30;

	sub.s32 	%r133, %r44, %r12;
	shl.b64 	%rd35, %rd3, 1;
	add.s64 	%rd36, %rd35, 2;
	add.s64 	%rd47, %rd1, %rd36;
	add.s64 	%rd46, %rd2, %rd36;
	// begin inline asm
	{ mov.b32 %f12, {0,%rs143};}

	// end inline asm
	mov.u32 	%r134, %r137;
	mov.u16 	%rs155, %rs1;

$L__BB72_11:
	add.s64 	%rd14, %rd46, -2;
	ld.global.nc.u16 	%rs73, [%rd46+-2];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs73};}

	// end inline asm
	sub.ftz.f32 	%f13, %f11, %f12;
	mul.ftz.f32 	%f14, %f13, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f15, %f14;
	mov.b32 	%r51, %f15;
	and.b32  	%r52, %r51, 2147483647;
	setp.gt.u32 	%p14, %r52, 2139095040;
	shl.b32 	%r53, %r51, 16;
	shr.u32 	%r54, %r51, 16;
	cvt.u16.u32 	%rs75, %r54;
	selp.b32 	%r16, 0, %r53, %p14;
	selp.b16 	%rs145, 32767, %rs75, %p14;
	setp.gt.u32 	%p15, %r16, -2147483648;
	@%p15 bra 	$L__BB72_13;

	setp.ne.s32 	%p16, %r16, -2147483648;
	and.b16  	%rs76, %rs145, 1;
	setp.eq.b16 	%p17, %rs76, 1;
	not.pred 	%p18, %p17;
	or.pred  	%p19, %p16, %p18;
	@%p19 bra 	$L__BB72_14;

$L__BB72_13:
	add.s16 	%rs145, %rs145, 1;

$L__BB72_14:
	// begin inline asm
	{ mov.b32 %f16, {0,%rs145};}

	// end inline asm
	cvt.ftz.f64.f32 	%fd1, %f16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r56}, %fd1;
	}
	and.b32  	%r57, %r56, 2147483647;
	setp.eq.s32 	%p20, %r57, 2146435072;
	setp.eq.s32 	%p21, %r55, 0;
	and.pred  	%p22, %p21, %p20;
	selp.b16 	%rs78, 32640, %rs145, %p22;
	// begin inline asm
	{ mov.b32 %f17, {0,%rs78};}

	// end inline asm
	mov.b32 	%r58, %f17;
	and.b32  	%r59, %r58, 2147483647;
	setp.gt.u32 	%p23, %r59, 2139095040;
	shl.b32 	%r60, %r58, 16;
	shr.u32 	%r61, %r58, 16;
	cvt.u16.u32 	%rs79, %r61;
	selp.b32 	%r17, 0, %r60, %p23;
	selp.b16 	%rs146, 32767, %rs79, %p23;
	setp.gt.u32 	%p24, %r17, -2147483648;
	@%p24 bra 	$L__BB72_16;

	setp.ne.s32 	%p25, %r17, -2147483648;
	and.b16  	%rs80, %rs146, 1;
	setp.eq.b16 	%p26, %rs80, 1;
	not.pred 	%p27, %p26;
	or.pred  	%p28, %p25, %p27;
	@%p28 bra 	$L__BB72_17;

$L__BB72_16:
	add.s16 	%rs146, %rs146, 1;

$L__BB72_17:
	add.s64 	%rd15, %rd47, -2;
	st.global.u16 	[%rd47+-2], %rs146;
	// begin inline asm
	{ mov.b32 %f18, {0,%rs155};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f19, {0,%rs146};}

	// end inline asm
	add.ftz.f32 	%f20, %f18, %f19;
	mov.b32 	%r62, %f20;
	and.b32  	%r63, %r62, 2147483647;
	setp.gt.u32 	%p29, %r63, 2139095040;
	shl.b32 	%r64, %r62, 16;
	shr.u32 	%r65, %r62, 16;
	cvt.u16.u32 	%rs83, %r65;
	selp.b32 	%r18, 0, %r64, %p29;
	selp.b16 	%rs147, 32767, %rs83, %p29;
	setp.gt.u32 	%p30, %r18, -2147483648;
	@%p30 bra 	$L__BB72_19;

	setp.ne.s32 	%p31, %r18, -2147483648;
	and.b16  	%rs84, %rs147, 1;
	setp.eq.b16 	%p32, %rs84, 1;
	not.pred 	%p33, %p32;
	or.pred  	%p34, %p31, %p33;
	@%p34 bra 	$L__BB72_20;

$L__BB72_19:
	add.s16 	%rs147, %rs147, 1;

$L__BB72_20:
	ld.global.nc.u16 	%rs85, [%rd14+2];
	// begin inline asm
	{ mov.b32 %f21, {0,%rs85};}

	// end inline asm
	sub.ftz.f32 	%f23, %f21, %f12;
	mul.ftz.f32 	%f24, %f23, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f25, %f24;
	mov.b32 	%r66, %f25;
	and.b32  	%r67, %r66, 2147483647;
	setp.gt.u32 	%p35, %r67, 2139095040;
	shl.b32 	%r68, %r66, 16;
	shr.u32 	%r69, %r66, 16;
	cvt.u16.u32 	%rs87, %r69;
	selp.b32 	%r19, 0, %r68, %p35;
	selp.b16 	%rs148, 32767, %rs87, %p35;
	setp.gt.u32 	%p36, %r19, -2147483648;
	@%p36 bra 	$L__BB72_22;

	setp.ne.s32 	%p37, %r19, -2147483648;
	and.b16  	%rs88, %rs148, 1;
	setp.eq.b16 	%p38, %rs88, 1;
	not.pred 	%p39, %p38;
	or.pred  	%p40, %p37, %p39;
	@%p40 bra 	$L__BB72_23;

$L__BB72_22:
	add.s16 	%rs148, %rs148, 1;

$L__BB72_23:
	// begin inline asm
	{ mov.b32 %f26, {0,%rs148};}

	// end inline asm
	cvt.ftz.f64.f32 	%fd2, %f26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r70, %temp}, %fd2;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd2;
	}
	and.b32  	%r72, %r71, 2147483647;
	setp.eq.s32 	%p41, %r72, 2146435072;
	setp.eq.s32 	%p42, %r70, 0;
	and.pred  	%p43, %p42, %p41;
	selp.b16 	%rs90, 32640, %rs148, %p43;
	// begin inline asm
	{ mov.b32 %f27, {0,%rs90};}

	// end inline asm
	mov.b32 	%r73, %f27;
	and.b32  	%r74, %r73, 2147483647;
	setp.gt.u32 	%p44, %r74, 2139095040;
	shl.b32 	%r75, %r73, 16;
	shr.u32 	%r76, %r73, 16;
	cvt.u16.u32 	%rs91, %r76;
	selp.b32 	%r20, 0, %r75, %p44;
	selp.b16 	%rs149, 32767, %rs91, %p44;
	setp.gt.u32 	%p45, %r20, -2147483648;
	@%p45 bra 	$L__BB72_25;

	setp.ne.s32 	%p46, %r20, -2147483648;
	and.b16  	%rs92, %rs149, 1;
	setp.eq.b16 	%p47, %rs92, 1;
	not.pred 	%p48, %p47;
	or.pred  	%p49, %p46, %p48;
	@%p49 bra 	$L__BB72_26;

$L__BB72_25:
	add.s16 	%rs149, %rs149, 1;

$L__BB72_26:
	st.global.u16 	[%rd15+2], %rs149;
	// begin inline asm
	{ mov.b32 %f28, {0,%rs147};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f29, {0,%rs149};}

	// end inline asm
	add.ftz.f32 	%f30, %f28, %f29;
	mov.b32 	%r77, %f30;
	and.b32  	%r78, %r77, 2147483647;
	setp.gt.u32 	%p50, %r78, 2139095040;
	shl.b32 	%r79, %r77, 16;
	shr.u32 	%r80, %r77, 16;
	cvt.u16.u32 	%rs95, %r80;
	selp.b32 	%r21, 0, %r79, %p50;
	selp.b16 	%rs155, 32767, %rs95, %p50;
	setp.gt.u32 	%p51, %r21, -2147483648;
	@%p51 bra 	$L__BB72_28;

	setp.ne.s32 	%p52, %r21, -2147483648;
	and.b16  	%rs96, %rs155, 1;
	setp.eq.b16 	%p53, %rs96, 1;
	not.pred 	%p54, %p53;
	or.pred  	%p55, %p52, %p54;
	@%p55 bra 	$L__BB72_29;

$L__BB72_28:
	add.s16 	%rs155, %rs155, 1;

$L__BB72_29:
	add.s32 	%r134, %r134, 2;
	add.s64 	%rd47, %rd47, 4;
	add.s64 	%rd46, %rd46, 4;
	add.s32 	%r133, %r133, -2;
	setp.ne.s32 	%p56, %r133, 0;
	@%p56 bra 	$L__BB72_11;

$L__BB72_30:
	setp.eq.s32 	%p57, %r12, 0;
	@%p57 bra 	$L__BB72_40;

	cvt.s64.s32 	%rd18, %r134;
	mul.wide.s32 	%rd37, %r134, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.u16 	%rs97, [%rd38];
	// begin inline asm
	{ mov.b32 %f31, {0,%rs97};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f32, {0,%rs143};}

	// end inline asm
	sub.ftz.f32 	%f33, %f31, %f32;
	mul.ftz.f32 	%f34, %f33, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f35, %f34;
	mov.b32 	%r81, %f35;
	and.b32  	%r82, %r81, 2147483647;
	setp.gt.u32 	%p58, %r82, 2139095040;
	shl.b32 	%r83, %r81, 16;
	shr.u32 	%r84, %r81, 16;
	cvt.u16.u32 	%rs99, %r84;
	selp.b32 	%r25, 0, %r83, %p58;
	selp.b16 	%rs153, 32767, %rs99, %p58;
	setp.gt.u32 	%p59, %r25, -2147483648;
	@%p59 bra 	$L__BB72_33;

	setp.ne.s32 	%p60, %r25, -2147483648;
	and.b16  	%rs100, %rs153, 1;
	setp.eq.b16 	%p61, %rs100, 1;
	not.pred 	%p62, %p61;
	or.pred  	%p63, %p60, %p62;
	@%p63 bra 	$L__BB72_34;

$L__BB72_33:
	add.s16 	%rs153, %rs153, 1;

$L__BB72_34:
	// begin inline asm
	{ mov.b32 %f36, {0,%rs153};}

	// end inline asm
	cvt.ftz.f64.f32 	%fd3, %f36;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r85, %temp}, %fd3;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r86}, %fd3;
	}
	and.b32  	%r87, %r86, 2147483647;
	setp.eq.s32 	%p64, %r87, 2146435072;
	setp.eq.s32 	%p65, %r85, 0;
	and.pred  	%p66, %p65, %p64;
	selp.b16 	%rs102, 32640, %rs153, %p66;
	// begin inline asm
	{ mov.b32 %f37, {0,%rs102};}

	// end inline asm
	mov.b32 	%r88, %f37;
	and.b32  	%r89, %r88, 2147483647;
	setp.gt.u32 	%p67, %r89, 2139095040;
	shl.b32 	%r90, %r88, 16;
	shr.u32 	%r91, %r88, 16;
	cvt.u16.u32 	%rs103, %r91;
	selp.b32 	%r26, 0, %r90, %p67;
	selp.b16 	%rs154, 32767, %rs103, %p67;
	setp.gt.u32 	%p68, %r26, -2147483648;
	@%p68 bra 	$L__BB72_36;

	setp.ne.s32 	%p69, %r26, -2147483648;
	and.b16  	%rs104, %rs154, 1;
	setp.eq.b16 	%p70, %rs104, 1;
	not.pred 	%p71, %p70;
	or.pred  	%p72, %p69, %p71;
	@%p72 bra 	$L__BB72_37;

$L__BB72_36:
	add.s16 	%rs154, %rs154, 1;

$L__BB72_37:
	shl.b64 	%rd39, %rd18, 1;
	add.s64 	%rd40, %rd1, %rd39;
	st.global.u16 	[%rd40], %rs154;
	// begin inline asm
	{ mov.b32 %f38, {0,%rs155};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f39, {0,%rs154};}

	// end inline asm
	add.ftz.f32 	%f40, %f38, %f39;
	mov.b32 	%r92, %f40;
	and.b32  	%r93, %r92, 2147483647;
	setp.gt.u32 	%p73, %r93, 2139095040;
	shl.b32 	%r94, %r92, 16;
	shr.u32 	%r95, %r92, 16;
	cvt.u16.u32 	%rs107, %r95;
	selp.b32 	%r27, 0, %r94, %p73;
	selp.b16 	%rs155, 32767, %rs107, %p73;
	setp.gt.u32 	%p74, %r27, -2147483648;
	@%p74 bra 	$L__BB72_39;

	setp.ne.s32 	%p75, %r27, -2147483648;
	and.b16  	%rs108, %rs155, 1;
	setp.eq.b16 	%p76, %rs108, 1;
	not.pred 	%p77, %p76;
	or.pred  	%p78, %p75, %p77;
	@%p78 bra 	$L__BB72_40;

$L__BB72_39:
	add.s16 	%rs155, %rs155, 1;

$L__BB72_40:
	// begin inline asm
	{ mov.b32 %f41, {0,%rs155};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f42, {0,%rs1};}

	// end inline asm
	setp.neu.ftz.f32 	%p79, %f41, %f42;
	@%p79 bra 	$L__BB72_44;

	// begin inline asm
	{ mov.b32 %f43, {0,%rs155};}

	// end inline asm
	ld.const.u16 	%rs112, [sh+10];
	// begin inline asm
	{ mov.b32 %f44, {0,%rs112};}

	// end inline asm
	add.ftz.f32 	%f45, %f43, %f44;
	mov.b32 	%r96, %f45;
	and.b32  	%r97, %r96, 2147483647;
	setp.gt.u32 	%p80, %r97, 2139095040;
	shl.b32 	%r98, %r96, 16;
	shr.u32 	%r99, %r96, 16;
	cvt.u16.u32 	%rs113, %r99;
	selp.b32 	%r28, 0, %r98, %p80;
	selp.b16 	%rs155, 32767, %rs113, %p80;
	setp.gt.u32 	%p81, %r28, -2147483648;
	@%p81 bra 	$L__BB72_43;

	setp.ne.s32 	%p82, %r28, -2147483648;
	and.b16  	%rs114, %rs155, 1;
	setp.eq.b16 	%p83, %rs114, 1;
	not.pred 	%p84, %p83;
	or.pred  	%p85, %p82, %p84;
	@%p85 bra 	$L__BB72_44;

$L__BB72_43:
	add.s16 	%rs155, %rs155, 1;

$L__BB72_44:
	// begin inline asm
	{ mov.b32 %f46, {0,%rs155};}

	// end inline asm
	cvt.ftz.f64.f32 	%fd4, %f46;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r100, %temp}, %fd4;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r101}, %fd4;
	}
	and.b32  	%r102, %r101, 2147483647;
	setp.eq.s32 	%p86, %r102, 2146435072;
	setp.eq.s32 	%p87, %r100, 0;
	and.pred  	%p88, %p87, %p86;
	selp.b16 	%rs116, 32640, %rs155, %p88;
	// begin inline asm
	{ mov.b32 %f47, {0,%rs116};}

	// end inline asm
	mov.b32 	%r103, %f47;
	and.b32  	%r104, %r103, 2147483647;
	setp.gt.u32 	%p89, %r104, 2139095040;
	shl.b32 	%r105, %r103, 16;
	shr.u32 	%r106, %r103, 16;
	cvt.u16.u32 	%rs117, %r106;
	selp.b32 	%r29, 0, %r105, %p89;
	selp.b16 	%rs157, 32767, %rs117, %p89;
	setp.gt.u32 	%p90, %r29, -2147483648;
	@%p90 bra 	$L__BB72_46;

	setp.ne.s32 	%p91, %r29, -2147483648;
	and.b16  	%rs118, %rs157, 1;
	setp.eq.b16 	%p92, %rs118, 1;
	not.pred 	%p93, %p92;
	or.pred  	%p94, %p91, %p93;
	@%p94 bra 	$L__BB72_47;

$L__BB72_46:
	add.s16 	%rs157, %rs157, 1;

$L__BB72_47:
	@%p12 bra 	$L__BB72_69;

	add.s32 	%r107, %r44, -1;
	and.b32  	%r138, %r44, 3;
	setp.lt.u32 	%p96, %r107, 3;
	@%p96 bra 	$L__BB72_63;

	sub.s32 	%r136, %r44, %r138;
	shl.b64 	%rd41, %rd3, 1;
	add.s64 	%rd42, %rd1, %rd41;
	add.s64 	%rd48, %rd42, 4;
	// begin inline asm
	{ mov.b32 %f49, {0,%rs157};}

	// end inline asm

$L__BB72_50:
	add.s64 	%rd21, %rd48, -4;
	ld.global.u16 	%rs119, [%rd48+-4];
	// begin inline asm
	{ mov.b32 %f48, {0,%rs119};}

	// end inline asm
	div.approx.ftz.f32 	%f50, %f48, %f49;
	mov.b32 	%r108, %f50;
	and.b32  	%r109, %r108, 2147483647;
	setp.gt.u32 	%p97, %r109, 2139095040;
	shl.b32 	%r110, %r108, 16;
	shr.u32 	%r111, %r108, 16;
	cvt.u16.u32 	%rs121, %r111;
	selp.b32 	%r34, 0, %r110, %p97;
	selp.b16 	%rs158, 32767, %rs121, %p97;
	setp.gt.u32 	%p98, %r34, -2147483648;
	@%p98 bra 	$L__BB72_52;

	setp.ne.s32 	%p99, %r34, -2147483648;
	and.b16  	%rs122, %rs158, 1;
	setp.eq.b16 	%p100, %rs122, 1;
	not.pred 	%p101, %p100;
	or.pred  	%p102, %p99, %p101;
	@%p102 bra 	$L__BB72_53;

$L__BB72_52:
	add.s16 	%rs158, %rs158, 1;

$L__BB72_53:
	st.global.u16 	[%rd21], %rs158;
	ld.global.u16 	%rs123, [%rd21+2];
	// begin inline asm
	{ mov.b32 %f51, {0,%rs123};}

	// end inline asm
	div.approx.ftz.f32 	%f53, %f51, %f49;
	mov.b32 	%r112, %f53;
	and.b32  	%r113, %r112, 2147483647;
	setp.gt.u32 	%p103, %r113, 2139095040;
	shl.b32 	%r114, %r112, 16;
	shr.u32 	%r115, %r112, 16;
	cvt.u16.u32 	%rs125, %r115;
	selp.b32 	%r35, 0, %r114, %p103;
	selp.b16 	%rs159, 32767, %rs125, %p103;
	setp.gt.u32 	%p104, %r35, -2147483648;
	@%p104 bra 	$L__BB72_55;

	setp.ne.s32 	%p105, %r35, -2147483648;
	and.b16  	%rs126, %rs159, 1;
	setp.eq.b16 	%p106, %rs126, 1;
	not.pred 	%p107, %p106;
	or.pred  	%p108, %p105, %p107;
	@%p108 bra 	$L__BB72_56;

$L__BB72_55:
	add.s16 	%rs159, %rs159, 1;

$L__BB72_56:
	st.global.u16 	[%rd21+2], %rs159;
	ld.global.u16 	%rs127, [%rd21+4];
	// begin inline asm
	{ mov.b32 %f54, {0,%rs127};}

	// end inline asm
	div.approx.ftz.f32 	%f56, %f54, %f49;
	mov.b32 	%r116, %f56;
	and.b32  	%r117, %r116, 2147483647;
	setp.gt.u32 	%p109, %r117, 2139095040;
	shl.b32 	%r118, %r116, 16;
	shr.u32 	%r119, %r116, 16;
	cvt.u16.u32 	%rs129, %r119;
	selp.b32 	%r36, 0, %r118, %p109;
	selp.b16 	%rs160, 32767, %rs129, %p109;
	setp.gt.u32 	%p110, %r36, -2147483648;
	@%p110 bra 	$L__BB72_58;

	setp.ne.s32 	%p111, %r36, -2147483648;
	and.b16  	%rs130, %rs160, 1;
	setp.eq.b16 	%p112, %rs130, 1;
	not.pred 	%p113, %p112;
	or.pred  	%p114, %p111, %p113;
	@%p114 bra 	$L__BB72_59;

$L__BB72_58:
	add.s16 	%rs160, %rs160, 1;

$L__BB72_59:
	st.global.u16 	[%rd21+4], %rs160;
	ld.global.u16 	%rs131, [%rd21+6];
	// begin inline asm
	{ mov.b32 %f57, {0,%rs131};}

	// end inline asm
	div.approx.ftz.f32 	%f59, %f57, %f49;
	mov.b32 	%r120, %f59;
	and.b32  	%r121, %r120, 2147483647;
	setp.gt.u32 	%p115, %r121, 2139095040;
	shl.b32 	%r122, %r120, 16;
	shr.u32 	%r123, %r120, 16;
	cvt.u16.u32 	%rs133, %r123;
	selp.b32 	%r37, 0, %r122, %p115;
	selp.b16 	%rs161, 32767, %rs133, %p115;
	setp.gt.u32 	%p116, %r37, -2147483648;
	@%p116 bra 	$L__BB72_61;

	setp.ne.s32 	%p117, %r37, -2147483648;
	and.b16  	%rs134, %rs161, 1;
	setp.eq.b16 	%p118, %rs134, 1;
	not.pred 	%p119, %p118;
	or.pred  	%p120, %p117, %p119;
	@%p120 bra 	$L__BB72_62;

$L__BB72_61:
	add.s16 	%rs161, %rs161, 1;

$L__BB72_62:
	st.global.u16 	[%rd21+6], %rs161;
	add.s32 	%r137, %r137, 4;
	add.s64 	%rd48, %rd48, 8;
	add.s32 	%r136, %r136, -4;
	setp.ne.s32 	%p121, %r136, 0;
	@%p121 bra 	$L__BB72_50;

$L__BB72_63:
	setp.eq.s32 	%p122, %r138, 0;
	@%p122 bra 	$L__BB72_69;

	mul.wide.s32 	%rd43, %r137, 2;
	add.s64 	%rd49, %rd1, %rd43;
	// begin inline asm
	{ mov.b32 %f61, {0,%rs157};}

	// end inline asm

$L__BB72_65:
	.pragma "nounroll";
	ld.global.u16 	%rs135, [%rd49];
	// begin inline asm
	{ mov.b32 %f60, {0,%rs135};}

	// end inline asm
	div.approx.ftz.f32 	%f62, %f60, %f61;
	mov.b32 	%r124, %f62;
	and.b32  	%r125, %r124, 2147483647;
	setp.gt.u32 	%p123, %r125, 2139095040;
	shl.b32 	%r126, %r124, 16;
	shr.u32 	%r127, %r124, 16;
	cvt.u16.u32 	%rs137, %r127;
	selp.b32 	%r42, 0, %r126, %p123;
	selp.b16 	%rs162, 32767, %rs137, %p123;
	setp.gt.u32 	%p124, %r42, -2147483648;
	@%p124 bra 	$L__BB72_67;

	setp.ne.s32 	%p125, %r42, -2147483648;
	and.b16  	%rs138, %rs162, 1;
	setp.eq.b16 	%p126, %rs138, 1;
	not.pred 	%p127, %p126;
	or.pred  	%p128, %p125, %p127;
	@%p128 bra 	$L__BB72_68;

$L__BB72_67:
	add.s16 	%rs162, %rs162, 1;

$L__BB72_68:
	st.global.u16 	[%rd49], %rs162;
	add.s64 	%rd49, %rd49, 2;
	add.s32 	%r138, %r138, -1;
	setp.ne.s32 	%p129, %r138, 0;
	@%p129 bra 	$L__BB72_65;

$L__BB72_69:
	ret;

}
	// .globl	matrixMulti
.visible .entry matrixMulti(
	.param .u64 matrixMulti_param_0,
	.param .u64 matrixMulti_param_1,
	.param .u64 matrixMulti_param_2,
	.param .u32 matrixMulti_param_3,
	.param .u32 matrixMulti_param_4,
	.param .u32 matrixMulti_param_5
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<111>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<16>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11matrixMultiE4ds_A[4096];
	// demoted variable
	.shared .align 4 .b8 _ZZ11matrixMultiE4ds_B[4096];

	ld.param.u64 	%rd4, [matrixMulti_param_0];
	ld.param.u64 	%rd5, [matrixMulti_param_1];
	ld.param.u64 	%rd6, [matrixMulti_param_2];
	ld.param.u32 	%r19, [matrixMulti_param_3];
	ld.param.u32 	%r20, [matrixMulti_param_4];
	ld.param.u32 	%r21, [matrixMulti_param_5];
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r2, %r23, %r22, %r39;
	mov.u32 	%r24, %ntid.y;
	mov.u32 	%r25, %ctaid.y;
	mov.u32 	%r40, %tid.y;
	mad.lo.s32 	%r4, %r25, %r24, %r40;
	setp.lt.s32 	%p1, %r21, -30;
	mov.f32 	%f110, 0f00000000;
	@%p1 bra 	$L__BB73_7;

	shl.b32 	%r27, %r40, 7;
	mov.u32 	%r28, _ZZ11matrixMultiE4ds_A;
	add.s32 	%r7, %r28, %r27;
	shl.b32 	%r29, %r39, 2;
	add.s32 	%r5, %r7, %r29;
	mov.u32 	%r30, _ZZ11matrixMultiE4ds_B;
	add.s32 	%r31, %r30, %r27;
	add.s32 	%r6, %r31, %r29;
	add.s32 	%r8, %r30, %r29;
	mad.lo.s32 	%r41, %r40, %r20, %r2;
	mad.lo.s32 	%r32, %r21, %r4, %r39;
	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r32, 4;
	add.s64 	%rd15, %rd7, %rd8;
	add.s32 	%r33, %r21, -1;
	mov.u32 	%r42, -1;
	shr.s32 	%r34, %r33, 31;
	shr.u32 	%r35, %r34, 27;
	add.s32 	%r36, %r33, %r35;
	shr.s32 	%r10, %r36, 5;
	cvta.to.global.u64 	%rd9, %rd5;

$L__BB73_2:
	setp.ge.s32 	%p2, %r39, %r21;
	setp.ge.s32 	%p3, %r4, %r19;
	mov.f32 	%f109, 0f00000000;
	or.pred  	%p4, %p3, %p2;
	mov.f32 	%f108, %f109;
	@%p4 bra 	$L__BB73_4;

	ld.global.f32 	%f108, [%rd15];

$L__BB73_4:
	st.shared.f32 	[%r5], %f108;
	setp.ge.s32 	%p5, %r40, %r21;
	setp.ge.s32 	%p6, %r2, %r20;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB73_6;

	mul.wide.s32 	%rd10, %r41, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f32 	%f109, [%rd11];

$L__BB73_6:
	st.shared.f32 	[%r6], %f109;
	bar.sync 	0;
	ld.shared.f32 	%f12, [%r8];
	ld.shared.f32 	%f13, [%r7];
	fma.rn.ftz.f32 	%f14, %f13, %f12, %f110;
	ld.shared.f32 	%f15, [%r8+128];
	ld.shared.f32 	%f16, [%r7+4];
	fma.rn.ftz.f32 	%f17, %f16, %f15, %f14;
	ld.shared.f32 	%f18, [%r8+256];
	ld.shared.f32 	%f19, [%r7+8];
	fma.rn.ftz.f32 	%f20, %f19, %f18, %f17;
	ld.shared.f32 	%f21, [%r8+384];
	ld.shared.f32 	%f22, [%r7+12];
	fma.rn.ftz.f32 	%f23, %f22, %f21, %f20;
	ld.shared.f32 	%f24, [%r8+512];
	ld.shared.f32 	%f25, [%r7+16];
	fma.rn.ftz.f32 	%f26, %f25, %f24, %f23;
	ld.shared.f32 	%f27, [%r8+640];
	ld.shared.f32 	%f28, [%r7+20];
	fma.rn.ftz.f32 	%f29, %f28, %f27, %f26;
	ld.shared.f32 	%f30, [%r8+768];
	ld.shared.f32 	%f31, [%r7+24];
	fma.rn.ftz.f32 	%f32, %f31, %f30, %f29;
	ld.shared.f32 	%f33, [%r8+896];
	ld.shared.f32 	%f34, [%r7+28];
	fma.rn.ftz.f32 	%f35, %f34, %f33, %f32;
	ld.shared.f32 	%f36, [%r8+1024];
	ld.shared.f32 	%f37, [%r7+32];
	fma.rn.ftz.f32 	%f38, %f37, %f36, %f35;
	ld.shared.f32 	%f39, [%r8+1152];
	ld.shared.f32 	%f40, [%r7+36];
	fma.rn.ftz.f32 	%f41, %f40, %f39, %f38;
	ld.shared.f32 	%f42, [%r8+1280];
	ld.shared.f32 	%f43, [%r7+40];
	fma.rn.ftz.f32 	%f44, %f43, %f42, %f41;
	ld.shared.f32 	%f45, [%r8+1408];
	ld.shared.f32 	%f46, [%r7+44];
	fma.rn.ftz.f32 	%f47, %f46, %f45, %f44;
	ld.shared.f32 	%f48, [%r8+1536];
	ld.shared.f32 	%f49, [%r7+48];
	fma.rn.ftz.f32 	%f50, %f49, %f48, %f47;
	ld.shared.f32 	%f51, [%r8+1664];
	ld.shared.f32 	%f52, [%r7+52];
	fma.rn.ftz.f32 	%f53, %f52, %f51, %f50;
	ld.shared.f32 	%f54, [%r8+1792];
	ld.shared.f32 	%f55, [%r7+56];
	fma.rn.ftz.f32 	%f56, %f55, %f54, %f53;
	ld.shared.f32 	%f57, [%r8+1920];
	ld.shared.f32 	%f58, [%r7+60];
	fma.rn.ftz.f32 	%f59, %f58, %f57, %f56;
	ld.shared.f32 	%f60, [%r8+2048];
	ld.shared.f32 	%f61, [%r7+64];
	fma.rn.ftz.f32 	%f62, %f61, %f60, %f59;
	ld.shared.f32 	%f63, [%r8+2176];
	ld.shared.f32 	%f64, [%r7+68];
	fma.rn.ftz.f32 	%f65, %f64, %f63, %f62;
	ld.shared.f32 	%f66, [%r8+2304];
	ld.shared.f32 	%f67, [%r7+72];
	fma.rn.ftz.f32 	%f68, %f67, %f66, %f65;
	ld.shared.f32 	%f69, [%r8+2432];
	ld.shared.f32 	%f70, [%r7+76];
	fma.rn.ftz.f32 	%f71, %f70, %f69, %f68;
	ld.shared.f32 	%f72, [%r8+2560];
	ld.shared.f32 	%f73, [%r7+80];
	fma.rn.ftz.f32 	%f74, %f73, %f72, %f71;
	ld.shared.f32 	%f75, [%r8+2688];
	ld.shared.f32 	%f76, [%r7+84];
	fma.rn.ftz.f32 	%f77, %f76, %f75, %f74;
	ld.shared.f32 	%f78, [%r8+2816];
	ld.shared.f32 	%f79, [%r7+88];
	fma.rn.ftz.f32 	%f80, %f79, %f78, %f77;
	ld.shared.f32 	%f81, [%r8+2944];
	ld.shared.f32 	%f82, [%r7+92];
	fma.rn.ftz.f32 	%f83, %f82, %f81, %f80;
	ld.shared.f32 	%f84, [%r8+3072];
	ld.shared.f32 	%f85, [%r7+96];
	fma.rn.ftz.f32 	%f86, %f85, %f84, %f83;
	ld.shared.f32 	%f87, [%r8+3200];
	ld.shared.f32 	%f88, [%r7+100];
	fma.rn.ftz.f32 	%f89, %f88, %f87, %f86;
	ld.shared.f32 	%f90, [%r8+3328];
	ld.shared.f32 	%f91, [%r7+104];
	fma.rn.ftz.f32 	%f92, %f91, %f90, %f89;
	ld.shared.f32 	%f93, [%r8+3456];
	ld.shared.f32 	%f94, [%r7+108];
	fma.rn.ftz.f32 	%f95, %f94, %f93, %f92;
	ld.shared.f32 	%f96, [%r8+3584];
	ld.shared.f32 	%f97, [%r7+112];
	fma.rn.ftz.f32 	%f98, %f97, %f96, %f95;
	ld.shared.f32 	%f99, [%r8+3712];
	ld.shared.f32 	%f100, [%r7+116];
	fma.rn.ftz.f32 	%f101, %f100, %f99, %f98;
	ld.shared.f32 	%f102, [%r8+3840];
	ld.shared.f32 	%f103, [%r7+120];
	fma.rn.ftz.f32 	%f104, %f103, %f102, %f101;
	ld.shared.f32 	%f105, [%r8+3968];
	ld.shared.f32 	%f106, [%r7+124];
	fma.rn.ftz.f32 	%f110, %f106, %f105, %f104;
	bar.sync 	0;
	shl.b32 	%r37, %r20, 5;
	add.s32 	%r41, %r41, %r37;
	add.s32 	%r40, %r40, 32;
	add.s64 	%rd15, %rd15, 128;
	add.s32 	%r39, %r39, 32;
	add.s32 	%r42, %r42, 1;
	setp.lt.s32 	%p8, %r42, %r10;
	@%p8 bra 	$L__BB73_2;

$L__BB73_7:
	setp.ge.s32 	%p9, %r2, %r20;
	setp.ge.s32 	%p10, %r4, %r19;
	or.pred  	%p11, %p10, %p9;
	@%p11 bra 	$L__BB73_9;

	mad.lo.s32 	%r38, %r4, %r20, %r2;
	cvta.to.global.u64 	%rd12, %rd6;
	mul.wide.s32 	%rd13, %r38, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f110;

$L__BB73_9:
	ret;

}
	// .globl	matrixMulti_TYPE
.visible .entry matrixMulti_TYPE(
	.param .u64 matrixMulti_TYPE_param_0,
	.param .u64 matrixMulti_TYPE_param_1,
	.param .u64 matrixMulti_TYPE_param_2,
	.param .u32 matrixMulti_TYPE_param_3,
	.param .u32 matrixMulti_TYPE_param_4,
	.param .u32 matrixMulti_TYPE_param_5
)
{
	.reg .pred 	%p<61>;
	.reg .b16 	%rs<88>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<105>;
	.reg .b64 	%rd<13>;
	// demoted variable
	.shared .align 2 .b8 _ZZ16matrixMulti_TYPEE4ds_A[2048];
	// demoted variable
	.shared .align 2 .b8 _ZZ16matrixMulti_TYPEE4ds_B[2048];

	ld.param.u64 	%rd3, [matrixMulti_TYPE_param_0];
	ld.param.u64 	%rd4, [matrixMulti_TYPE_param_1];
	ld.param.u64 	%rd5, [matrixMulti_TYPE_param_2];
	ld.param.u32 	%r34, [matrixMulti_TYPE_param_3];
	ld.param.u32 	%r35, [matrixMulti_TYPE_param_4];
	ld.param.u32 	%r36, [matrixMulti_TYPE_param_5];
	mov.u32 	%r37, %ntid.x;
	mov.u32 	%r38, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r38, %r37, %r1;
	mov.u32 	%r39, %ntid.y;
	mov.u32 	%r40, %ctaid.y;
	mov.u32 	%r3, %tid.y;
	mad.lo.s32 	%r4, %r40, %r39, %r3;
	ld.const.u16 	%rs78, [sh];
	setp.lt.s32 	%p1, %r36, -30;
	@%p1 bra 	$L__BB74_33;

	mul.lo.s32 	%r5, %r4, %r36;
	shl.b32 	%r42, %r3, 6;
	mov.u32 	%r43, _ZZ16matrixMulti_TYPEE4ds_A;
	add.s32 	%r44, %r43, %r42;
	shl.b32 	%r45, %r1, 1;
	add.s32 	%r6, %r44, %r45;
	mov.u32 	%r46, _ZZ16matrixMulti_TYPEE4ds_B;
	add.s32 	%r47, %r46, %r42;
	add.s32 	%r7, %r47, %r45;
	add.s32 	%r48, %r46, %r45;
	add.s32 	%r8, %r48, 256;
	add.s32 	%r9, %r44, 8;
	add.s32 	%r49, %r36, -1;
	shr.s32 	%r50, %r49, 31;
	shr.u32 	%r51, %r50, 27;
	add.s32 	%r52, %r49, %r51;
	shr.s32 	%r10, %r52, 5;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r101, 0;

$L__BB74_2:
	shl.b32 	%r12, %r101, 5;
	add.s32 	%r13, %r12, %r1;
	setp.ge.s32 	%p2, %r13, %r36;
	setp.ge.s32 	%p3, %r4, %r34;
	mov.u16 	%rs77, 0;
	or.pred  	%p4, %p3, %p2;
	mov.u16 	%rs76, %rs77;
	@%p4 bra 	$L__BB74_4;

	add.s32 	%r53, %r13, %r5;
	mul.wide.s32 	%rd6, %r53, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u16 	%rs76, [%rd7];

$L__BB74_4:
	st.shared.u16 	[%r6], %rs76;
	add.s32 	%r14, %r12, %r3;
	setp.ge.s32 	%p5, %r14, %r36;
	setp.ge.s32 	%p6, %r2, %r35;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB74_6;

	mad.lo.s32 	%r54, %r14, %r35, %r2;
	mul.wide.s32 	%rd8, %r54, 2;
	add.s64 	%rd9, %rd2, %rd8;
	ld.global.u16 	%rs77, [%rd9];

$L__BB74_6:
	st.shared.u16 	[%r7], %rs77;
	bar.sync 	0;
	mov.u32 	%r104, 256;
	mov.u32 	%r102, %r9;
	mov.u32 	%r103, %r8;

$L__BB74_7:
	// begin inline asm
	{ mov.b32 %f1, {0,%rs78};}

	// end inline asm
	ld.shared.u16 	%rs36, [%r102+-8];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs36};}

	// end inline asm
	ld.shared.u16 	%rs37, [%r103+-256];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs37};}

	// end inline asm
	fma.rn.ftz.f32 	%f4, %f2, %f3, %f1;
	mov.b32 	%r56, %f4;
	and.b32  	%r57, %r56, 2147483647;
	setp.gt.u32 	%p8, %r57, 2139095040;
	shl.b32 	%r58, %r56, 16;
	shr.u32 	%r59, %r56, 16;
	cvt.u16.u32 	%rs38, %r59;
	selp.b32 	%r20, 0, %r58, %p8;
	selp.b16 	%rs79, 32767, %rs38, %p8;
	setp.gt.u32 	%p9, %r20, -2147483648;
	@%p9 bra 	$L__BB74_9;

	setp.ne.s32 	%p10, %r20, -2147483648;
	and.b16  	%rs39, %rs79, 1;
	setp.eq.b16 	%p11, %rs39, 1;
	not.pred 	%p12, %p11;
	or.pred  	%p13, %p10, %p12;
	@%p13 bra 	$L__BB74_10;

$L__BB74_9:
	add.s16 	%rs79, %rs79, 1;

$L__BB74_10:
	// begin inline asm
	{ mov.b32 %f5, {0,%rs79};}

	// end inline asm
	ld.shared.u16 	%rs41, [%r102+-6];
	// begin inline asm
	{ mov.b32 %f6, {0,%rs41};}

	// end inline asm
	ld.shared.u16 	%rs42, [%r103+-192];
	// begin inline asm
	{ mov.b32 %f7, {0,%rs42};}

	// end inline asm
	fma.rn.ftz.f32 	%f8, %f6, %f7, %f5;
	mov.b32 	%r60, %f8;
	and.b32  	%r61, %r60, 2147483647;
	setp.gt.u32 	%p14, %r61, 2139095040;
	shl.b32 	%r62, %r60, 16;
	shr.u32 	%r63, %r60, 16;
	cvt.u16.u32 	%rs43, %r63;
	selp.b32 	%r23, 0, %r62, %p14;
	selp.b16 	%rs80, 32767, %rs43, %p14;
	setp.gt.u32 	%p15, %r23, -2147483648;
	@%p15 bra 	$L__BB74_12;

	setp.ne.s32 	%p16, %r23, -2147483648;
	and.b16  	%rs44, %rs80, 1;
	setp.eq.b16 	%p17, %rs44, 1;
	not.pred 	%p18, %p17;
	or.pred  	%p19, %p16, %p18;
	@%p19 bra 	$L__BB74_13;

$L__BB74_12:
	add.s16 	%rs80, %rs80, 1;

$L__BB74_13:
	// begin inline asm
	{ mov.b32 %f9, {0,%rs80};}

	// end inline asm
	add.s32 	%r89, %r102, -8;
	ld.shared.u16 	%rs46, [%r89+4];
	// begin inline asm
	{ mov.b32 %f10, {0,%rs46};}

	// end inline asm
	add.s32 	%r90, %r103, -256;
	ld.shared.u16 	%rs47, [%r90+128];
	// begin inline asm
	{ mov.b32 %f11, {0,%rs47};}

	// end inline asm
	fma.rn.ftz.f32 	%f12, %f10, %f11, %f9;
	mov.b32 	%r64, %f12;
	and.b32  	%r65, %r64, 2147483647;
	setp.gt.u32 	%p20, %r65, 2139095040;
	shl.b32 	%r66, %r64, 16;
	shr.u32 	%r67, %r64, 16;
	cvt.u16.u32 	%rs48, %r67;
	selp.b32 	%r24, 0, %r66, %p20;
	selp.b16 	%rs81, 32767, %rs48, %p20;
	setp.gt.u32 	%p21, %r24, -2147483648;
	@%p21 bra 	$L__BB74_15;

	setp.ne.s32 	%p22, %r24, -2147483648;
	and.b16  	%rs49, %rs81, 1;
	setp.eq.b16 	%p23, %rs49, 1;
	not.pred 	%p24, %p23;
	or.pred  	%p25, %p22, %p24;
	@%p25 bra 	$L__BB74_16;

$L__BB74_15:
	add.s16 	%rs81, %rs81, 1;

$L__BB74_16:
	// begin inline asm
	{ mov.b32 %f13, {0,%rs81};}

	// end inline asm
	add.s32 	%r91, %r102, -8;
	ld.shared.u16 	%rs51, [%r91+6];
	// begin inline asm
	{ mov.b32 %f14, {0,%rs51};}

	// end inline asm
	add.s32 	%r92, %r103, -256;
	ld.shared.u16 	%rs52, [%r92+192];
	// begin inline asm
	{ mov.b32 %f15, {0,%rs52};}

	// end inline asm
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	mov.b32 	%r68, %f16;
	and.b32  	%r69, %r68, 2147483647;
	setp.gt.u32 	%p26, %r69, 2139095040;
	shl.b32 	%r70, %r68, 16;
	shr.u32 	%r71, %r68, 16;
	cvt.u16.u32 	%rs53, %r71;
	selp.b32 	%r25, 0, %r70, %p26;
	selp.b16 	%rs82, 32767, %rs53, %p26;
	setp.gt.u32 	%p27, %r25, -2147483648;
	@%p27 bra 	$L__BB74_18;

	setp.ne.s32 	%p28, %r25, -2147483648;
	and.b16  	%rs54, %rs82, 1;
	setp.eq.b16 	%p29, %rs54, 1;
	not.pred 	%p30, %p29;
	or.pred  	%p31, %p28, %p30;
	@%p31 bra 	$L__BB74_19;

$L__BB74_18:
	add.s16 	%rs82, %rs82, 1;

$L__BB74_19:
	// begin inline asm
	{ mov.b32 %f17, {0,%rs82};}

	// end inline asm
	add.s32 	%r93, %r102, -8;
	ld.shared.u16 	%rs56, [%r93+8];
	// begin inline asm
	{ mov.b32 %f18, {0,%rs56};}

	// end inline asm
	add.s32 	%r94, %r103, -256;
	ld.shared.u16 	%rs57, [%r94+256];
	// begin inline asm
	{ mov.b32 %f19, {0,%rs57};}

	// end inline asm
	fma.rn.ftz.f32 	%f20, %f18, %f19, %f17;
	mov.b32 	%r72, %f20;
	and.b32  	%r73, %r72, 2147483647;
	setp.gt.u32 	%p32, %r73, 2139095040;
	shl.b32 	%r74, %r72, 16;
	shr.u32 	%r75, %r72, 16;
	cvt.u16.u32 	%rs58, %r75;
	selp.b32 	%r26, 0, %r74, %p32;
	selp.b16 	%rs83, 32767, %rs58, %p32;
	setp.gt.u32 	%p33, %r26, -2147483648;
	@%p33 bra 	$L__BB74_21;

	setp.ne.s32 	%p34, %r26, -2147483648;
	and.b16  	%rs59, %rs83, 1;
	setp.eq.b16 	%p35, %rs59, 1;
	not.pred 	%p36, %p35;
	or.pred  	%p37, %p34, %p36;
	@%p37 bra 	$L__BB74_22;

$L__BB74_21:
	add.s16 	%rs83, %rs83, 1;

$L__BB74_22:
	// begin inline asm
	{ mov.b32 %f21, {0,%rs83};}

	// end inline asm
	add.s32 	%r95, %r102, -8;
	ld.shared.u16 	%rs61, [%r95+10];
	// begin inline asm
	{ mov.b32 %f22, {0,%rs61};}

	// end inline asm
	add.s32 	%r96, %r103, -256;
	ld.shared.u16 	%rs62, [%r96+320];
	// begin inline asm
	{ mov.b32 %f23, {0,%rs62};}

	// end inline asm
	fma.rn.ftz.f32 	%f24, %f22, %f23, %f21;
	mov.b32 	%r76, %f24;
	and.b32  	%r77, %r76, 2147483647;
	setp.gt.u32 	%p38, %r77, 2139095040;
	shl.b32 	%r78, %r76, 16;
	shr.u32 	%r79, %r76, 16;
	cvt.u16.u32 	%rs63, %r79;
	selp.b32 	%r27, 0, %r78, %p38;
	selp.b16 	%rs84, 32767, %rs63, %p38;
	setp.gt.u32 	%p39, %r27, -2147483648;
	@%p39 bra 	$L__BB74_24;

	setp.ne.s32 	%p40, %r27, -2147483648;
	and.b16  	%rs64, %rs84, 1;
	setp.eq.b16 	%p41, %rs64, 1;
	not.pred 	%p42, %p41;
	or.pred  	%p43, %p40, %p42;
	@%p43 bra 	$L__BB74_25;

$L__BB74_24:
	add.s16 	%rs84, %rs84, 1;

$L__BB74_25:
	// begin inline asm
	{ mov.b32 %f25, {0,%rs84};}

	// end inline asm
	add.s32 	%r97, %r102, -8;
	ld.shared.u16 	%rs66, [%r97+12];
	// begin inline asm
	{ mov.b32 %f26, {0,%rs66};}

	// end inline asm
	add.s32 	%r98, %r103, -256;
	ld.shared.u16 	%rs67, [%r98+384];
	// begin inline asm
	{ mov.b32 %f27, {0,%rs67};}

	// end inline asm
	fma.rn.ftz.f32 	%f28, %f26, %f27, %f25;
	mov.b32 	%r80, %f28;
	and.b32  	%r81, %r80, 2147483647;
	setp.gt.u32 	%p44, %r81, 2139095040;
	shl.b32 	%r82, %r80, 16;
	shr.u32 	%r83, %r80, 16;
	cvt.u16.u32 	%rs68, %r83;
	selp.b32 	%r28, 0, %r82, %p44;
	selp.b16 	%rs85, 32767, %rs68, %p44;
	setp.gt.u32 	%p45, %r28, -2147483648;
	@%p45 bra 	$L__BB74_27;

	setp.ne.s32 	%p46, %r28, -2147483648;
	and.b16  	%rs69, %rs85, 1;
	setp.eq.b16 	%p47, %rs69, 1;
	not.pred 	%p48, %p47;
	or.pred  	%p49, %p46, %p48;
	@%p49 bra 	$L__BB74_28;

$L__BB74_27:
	add.s16 	%rs85, %rs85, 1;

$L__BB74_28:
	// begin inline asm
	{ mov.b32 %f29, {0,%rs85};}

	// end inline asm
	add.s32 	%r99, %r102, -8;
	ld.shared.u16 	%rs71, [%r99+14];
	// begin inline asm
	{ mov.b32 %f30, {0,%rs71};}

	// end inline asm
	add.s32 	%r100, %r103, -256;
	ld.shared.u16 	%rs72, [%r100+448];
	// begin inline asm
	{ mov.b32 %f31, {0,%rs72};}

	// end inline asm
	fma.rn.ftz.f32 	%f32, %f30, %f31, %f29;
	mov.b32 	%r84, %f32;
	and.b32  	%r85, %r84, 2147483647;
	setp.gt.u32 	%p50, %r85, 2139095040;
	shl.b32 	%r86, %r84, 16;
	shr.u32 	%r87, %r84, 16;
	cvt.u16.u32 	%rs73, %r87;
	selp.b32 	%r29, 0, %r86, %p50;
	selp.b16 	%rs78, 32767, %rs73, %p50;
	setp.gt.u32 	%p51, %r29, -2147483648;
	@%p51 bra 	$L__BB74_30;

	setp.ne.s32 	%p52, %r29, -2147483648;
	and.b16  	%rs74, %rs78, 1;
	setp.eq.b16 	%p53, %rs74, 1;
	not.pred 	%p54, %p53;
	or.pred  	%p55, %p52, %p54;
	@%p55 bra 	$L__BB74_31;

$L__BB74_30:
	add.s16 	%rs78, %rs78, 1;

$L__BB74_31:
	add.s32 	%r103, %r103, 512;
	add.s32 	%r102, %r102, 16;
	add.s32 	%r104, %r104, 512;
	setp.ne.s32 	%p56, %r104, 2304;
	@%p56 bra 	$L__BB74_7;

	bar.sync 	0;
	add.s32 	%r33, %r101, 1;
	setp.lt.s32 	%p57, %r101, %r10;
	mov.u32 	%r101, %r33;
	@%p57 bra 	$L__BB74_2;

$L__BB74_33:
	setp.ge.s32 	%p58, %r2, %r35;
	setp.ge.s32 	%p59, %r4, %r34;
	or.pred  	%p60, %p59, %p58;
	@%p60 bra 	$L__BB74_35;

	mad.lo.s32 	%r88, %r4, %r35, %r2;
	cvta.to.global.u64 	%rd10, %rd5;
	mul.wide.s32 	%rd11, %r88, 2;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.u16 	[%rd12], %rs78;

$L__BB74_35:
	ret;

}
	// .globl	convolution2D
.visible .entry convolution2D(
	.param .u64 convolution2D_param_0,
	.param .u64 convolution2D_param_1,
	.param .u64 convolution2D_param_2,
	.param .u32 convolution2D_param_3,
	.param .u32 convolution2D_param_4,
	.param .u32 convolution2D_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<82>;
	.reg .b32 	%r<61>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<22>;
	// demoted variable
	.shared .align 4 .b8 _ZZ13convolution2DE4N_ds[1600];

	ld.param.u64 	%rd10, [convolution2D_param_0];
	ld.param.u64 	%rd11, [convolution2D_param_1];
	ld.param.u64 	%rd12, [convolution2D_param_2];
	ld.param.u32 	%r7, [convolution2D_param_3];
	ld.param.u32 	%r8, [convolution2D_param_4];
	ld.param.u32 	%r9, [convolution2D_param_5];
	setp.lt.s32 	%p4, %r7, 1;
	@%p4 bra 	$L__BB75_11;

	cvta.to.global.u64 	%rd13, %rd11;
	cvta.to.global.u64 	%rd14, %rd10;
	mov.u32 	%r11, %tid.y;
	shl.b32 	%r12, %r11, 4;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r14, %r12, %r13;
	mul.hi.s32 	%r15, %r14, 1717986919;
	shr.u32 	%r16, %r15, 31;
	shr.s32 	%r17, %r15, 3;
	add.s32 	%r18, %r17, %r16;
	mul.lo.s32 	%r19, %r18, 20;
	sub.s32 	%r20, %r14, %r19;
	mov.u32 	%r21, %ctaid.y;
	shl.b32 	%r22, %r21, 4;
	add.s32 	%r23, %r22, -2;
	add.s32 	%r24, %r23, %r18;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 4;
	add.s32 	%r27, %r26, -2;
	add.s32 	%r28, %r27, %r20;
	setp.lt.s32 	%p5, %r24, %r9;
	or.b32  	%r29, %r24, %r28;
	setp.gt.s32 	%p6, %r29, -1;
	and.pred  	%p7, %p6, %p5;
	setp.lt.s32 	%p8, %r28, %r8;
	and.pred  	%p1, %p8, %p7;
	mov.u32 	%r30, _ZZ13convolution2DE4N_ds;
	mad.lo.s32 	%r31, %r18, 80, %r30;
	shl.b32 	%r32, %r20, 2;
	add.s32 	%r1, %r31, %r32;
	add.s32 	%r2, %r14, 256;
	mul.hi.s32 	%r33, %r2, 1717986919;
	shr.u32 	%r34, %r33, 31;
	shr.s32 	%r35, %r33, 3;
	add.s32 	%r36, %r35, %r34;
	mul.lo.s32 	%r37, %r36, 20;
	sub.s32 	%r38, %r2, %r37;
	add.s32 	%r39, %r23, %r36;
	add.s32 	%r40, %r27, %r38;
	setp.lt.s32 	%p9, %r39, %r9;
	or.b32  	%r41, %r39, %r40;
	setp.gt.s32 	%p10, %r41, -1;
	and.pred  	%p11, %p10, %p9;
	setp.lt.s32 	%p12, %r40, %r8;
	and.pred  	%p2, %p12, %p11;
	mad.lo.s32 	%r42, %r36, 80, %r30;
	shl.b32 	%r43, %r38, 2;
	add.s32 	%r3, %r42, %r43;
	add.s32 	%r44, %r22, %r11;
	add.s32 	%r45, %r26, %r13;
	setp.lt.s32 	%p13, %r44, %r9;
	setp.lt.s32 	%p14, %r45, %r8;
	and.pred  	%p3, %p14, %p13;
	mad.lo.s32 	%r46, %r11, 80, %r30;
	shl.b32 	%r47, %r13, 2;
	add.s32 	%r4, %r46, %r47;
	ld.global.nc.f32 	%f1, [%rd13];
	ld.global.nc.f32 	%f2, [%rd13+4];
	ld.global.nc.f32 	%f3, [%rd13+8];
	ld.global.nc.f32 	%f4, [%rd13+12];
	ld.global.nc.f32 	%f5, [%rd13+16];
	ld.global.nc.f32 	%f6, [%rd13+20];
	ld.global.nc.f32 	%f7, [%rd13+24];
	ld.global.nc.f32 	%f8, [%rd13+28];
	ld.global.nc.f32 	%f9, [%rd13+32];
	ld.global.nc.f32 	%f10, [%rd13+36];
	ld.global.nc.f32 	%f11, [%rd13+40];
	ld.global.nc.f32 	%f12, [%rd13+44];
	ld.global.nc.f32 	%f13, [%rd13+48];
	ld.global.nc.f32 	%f14, [%rd13+52];
	ld.global.nc.f32 	%f15, [%rd13+56];
	ld.global.nc.f32 	%f16, [%rd13+60];
	ld.global.nc.f32 	%f17, [%rd13+64];
	ld.global.nc.f32 	%f18, [%rd13+68];
	ld.global.nc.f32 	%f19, [%rd13+72];
	ld.global.nc.f32 	%f20, [%rd13+76];
	ld.global.nc.f32 	%f21, [%rd13+80];
	ld.global.nc.f32 	%f22, [%rd13+84];
	ld.global.nc.f32 	%f23, [%rd13+88];
	ld.global.nc.f32 	%f24, [%rd13+92];
	ld.global.nc.f32 	%f25, [%rd13+96];
	mad.lo.s32 	%r48, %r8, %r24, %r20;
	add.s32 	%r49, %r48, %r26;
	add.s32 	%r50, %r49, -2;
	mul.lo.s32 	%r51, %r7, %r50;
	mul.wide.s32 	%rd15, %r51, 4;
	add.s64 	%rd21, %rd14, %rd15;
	mad.lo.s32 	%r52, %r8, %r39, %r38;
	add.s32 	%r53, %r52, %r26;
	add.s32 	%r54, %r53, -2;
	mul.lo.s32 	%r55, %r7, %r54;
	mul.wide.s32 	%rd16, %r55, 4;
	add.s64 	%rd20, %rd14, %rd16;
	mad.lo.s32 	%r56, %r8, %r44, %r13;
	add.s32 	%r57, %r56, %r26;
	mul.lo.s32 	%r58, %r7, %r57;
	cvta.to.global.u64 	%rd17, %rd12;
	mul.wide.s32 	%rd18, %r58, 4;
	add.s64 	%rd19, %rd17, %rd18;
	mov.u32 	%r60, 0;
	not.pred 	%p15, %p1;
	not.pred 	%p17, %p3;

$L__BB75_2:
	mov.f32 	%f81, 0f00000000;
	@%p15 bra 	$L__BB75_4;

	ld.global.f32 	%f81, [%rd21];

$L__BB75_4:
	st.shared.f32 	[%r1], %f81;
	setp.gt.s32 	%p16, %r2, 399;
	@%p16 bra 	$L__BB75_8;

	@%p2 bra 	$L__BB75_7;
	bra.uni 	$L__BB75_6;

$L__BB75_7:
	ld.global.f32 	%f30, [%rd20];
	st.shared.f32 	[%r3], %f30;
	bra.uni 	$L__BB75_8;

$L__BB75_6:
	mov.u32 	%r59, 0;
	st.shared.u32 	[%r3], %r59;

$L__BB75_8:
	bar.sync 	0;
	ld.shared.f32 	%f31, [%r4];
	fma.rn.ftz.f32 	%f32, %f31, %f1, 0f00000000;
	ld.shared.f32 	%f33, [%r4+4];
	fma.rn.ftz.f32 	%f34, %f33, %f2, %f32;
	ld.shared.f32 	%f35, [%r4+8];
	fma.rn.ftz.f32 	%f36, %f35, %f3, %f34;
	ld.shared.f32 	%f37, [%r4+12];
	fma.rn.ftz.f32 	%f38, %f37, %f4, %f36;
	ld.shared.f32 	%f39, [%r4+16];
	fma.rn.ftz.f32 	%f40, %f39, %f5, %f38;
	ld.shared.f32 	%f41, [%r4+80];
	fma.rn.ftz.f32 	%f42, %f41, %f6, %f40;
	ld.shared.f32 	%f43, [%r4+84];
	fma.rn.ftz.f32 	%f44, %f43, %f7, %f42;
	ld.shared.f32 	%f45, [%r4+88];
	fma.rn.ftz.f32 	%f46, %f45, %f8, %f44;
	ld.shared.f32 	%f47, [%r4+92];
	fma.rn.ftz.f32 	%f48, %f47, %f9, %f46;
	ld.shared.f32 	%f49, [%r4+96];
	fma.rn.ftz.f32 	%f50, %f49, %f10, %f48;
	ld.shared.f32 	%f51, [%r4+160];
	fma.rn.ftz.f32 	%f52, %f51, %f11, %f50;
	ld.shared.f32 	%f53, [%r4+164];
	fma.rn.ftz.f32 	%f54, %f53, %f12, %f52;
	ld.shared.f32 	%f55, [%r4+168];
	fma.rn.ftz.f32 	%f56, %f55, %f13, %f54;
	ld.shared.f32 	%f57, [%r4+172];
	fma.rn.ftz.f32 	%f58, %f57, %f14, %f56;
	ld.shared.f32 	%f59, [%r4+176];
	fma.rn.ftz.f32 	%f60, %f59, %f15, %f58;
	ld.shared.f32 	%f61, [%r4+240];
	fma.rn.ftz.f32 	%f62, %f61, %f16, %f60;
	ld.shared.f32 	%f63, [%r4+244];
	fma.rn.ftz.f32 	%f64, %f63, %f17, %f62;
	ld.shared.f32 	%f65, [%r4+248];
	fma.rn.ftz.f32 	%f66, %f65, %f18, %f64;
	ld.shared.f32 	%f67, [%r4+252];
	fma.rn.ftz.f32 	%f68, %f67, %f19, %f66;
	ld.shared.f32 	%f69, [%r4+256];
	fma.rn.ftz.f32 	%f70, %f69, %f20, %f68;
	ld.shared.f32 	%f71, [%r4+320];
	fma.rn.ftz.f32 	%f72, %f71, %f21, %f70;
	ld.shared.f32 	%f73, [%r4+324];
	fma.rn.ftz.f32 	%f74, %f73, %f22, %f72;
	ld.shared.f32 	%f75, [%r4+328];
	fma.rn.ftz.f32 	%f76, %f75, %f23, %f74;
	ld.shared.f32 	%f77, [%r4+332];
	fma.rn.ftz.f32 	%f78, %f77, %f24, %f76;
	ld.shared.f32 	%f79, [%r4+336];
	fma.rn.ftz.f32 	%f28, %f79, %f25, %f78;
	@%p17 bra 	$L__BB75_10;

	cvt.ftz.f64.f32 	%fd1, %f28;
	cvt.sat.f64.f64 	%fd2, %fd1;
	cvt.rn.ftz.f32.f64 	%f80, %fd2;
	st.global.f32 	[%rd19], %f80;

$L__BB75_10:
	bar.sync 	0;
	add.s64 	%rd21, %rd21, 4;
	add.s64 	%rd20, %rd20, 4;
	add.s64 	%rd19, %rd19, 4;
	add.s32 	%r60, %r60, 1;
	setp.lt.s32 	%p18, %r60, %r7;
	@%p18 bra 	$L__BB75_2;

$L__BB75_11:
	ret;

}
	// .globl	transposeConvolution2D
.visible .entry transposeConvolution2D(
	.param .u64 transposeConvolution2D_param_0,
	.param .u64 transposeConvolution2D_param_1,
	.param .u64 transposeConvolution2D_param_2,
	.param .u32 transposeConvolution2D_param_3,
	.param .u32 transposeConvolution2D_param_4,
	.param .u32 transposeConvolution2D_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<82>;
	.reg .b32 	%r<61>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<22>;
	// demoted variable
	.shared .align 4 .b8 _ZZ22transposeConvolution2DE4N_ds[1600];

	ld.param.u64 	%rd10, [transposeConvolution2D_param_0];
	ld.param.u64 	%rd11, [transposeConvolution2D_param_1];
	ld.param.u64 	%rd12, [transposeConvolution2D_param_2];
	ld.param.u32 	%r7, [transposeConvolution2D_param_3];
	ld.param.u32 	%r8, [transposeConvolution2D_param_4];
	ld.param.u32 	%r9, [transposeConvolution2D_param_5];
	setp.lt.s32 	%p4, %r7, 1;
	@%p4 bra 	$L__BB76_11;

	cvta.to.global.u64 	%rd13, %rd11;
	cvta.to.global.u64 	%rd14, %rd10;
	mov.u32 	%r11, %tid.y;
	shl.b32 	%r12, %r11, 4;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r14, %r12, %r13;
	mul.hi.s32 	%r15, %r14, 1717986919;
	shr.u32 	%r16, %r15, 31;
	shr.s32 	%r17, %r15, 3;
	add.s32 	%r18, %r17, %r16;
	mul.lo.s32 	%r19, %r18, 20;
	sub.s32 	%r20, %r14, %r19;
	mov.u32 	%r21, %ctaid.y;
	shl.b32 	%r22, %r21, 4;
	add.s32 	%r23, %r22, -2;
	add.s32 	%r24, %r23, %r18;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 4;
	add.s32 	%r27, %r26, -2;
	add.s32 	%r28, %r27, %r20;
	setp.lt.s32 	%p5, %r24, %r9;
	or.b32  	%r29, %r24, %r28;
	setp.gt.s32 	%p6, %r29, -1;
	and.pred  	%p7, %p6, %p5;
	setp.lt.s32 	%p8, %r28, %r8;
	and.pred  	%p1, %p8, %p7;
	mov.u32 	%r30, _ZZ22transposeConvolution2DE4N_ds;
	mad.lo.s32 	%r31, %r18, 80, %r30;
	shl.b32 	%r32, %r20, 2;
	add.s32 	%r1, %r31, %r32;
	add.s32 	%r2, %r14, 256;
	mul.hi.s32 	%r33, %r2, 1717986919;
	shr.u32 	%r34, %r33, 31;
	shr.s32 	%r35, %r33, 3;
	add.s32 	%r36, %r35, %r34;
	mul.lo.s32 	%r37, %r36, 20;
	sub.s32 	%r38, %r2, %r37;
	add.s32 	%r39, %r23, %r36;
	add.s32 	%r40, %r27, %r38;
	setp.lt.s32 	%p9, %r39, %r9;
	or.b32  	%r41, %r39, %r40;
	setp.gt.s32 	%p10, %r41, -1;
	and.pred  	%p11, %p10, %p9;
	setp.lt.s32 	%p12, %r40, %r8;
	and.pred  	%p2, %p12, %p11;
	mad.lo.s32 	%r42, %r36, 80, %r30;
	shl.b32 	%r43, %r38, 2;
	add.s32 	%r3, %r42, %r43;
	add.s32 	%r44, %r22, %r11;
	add.s32 	%r45, %r26, %r13;
	setp.lt.s32 	%p13, %r44, %r9;
	setp.lt.s32 	%p14, %r45, %r8;
	and.pred  	%p3, %p14, %p13;
	mad.lo.s32 	%r46, %r11, 80, %r30;
	shl.b32 	%r47, %r13, 2;
	add.s32 	%r4, %r46, %r47;
	ld.global.nc.f32 	%f1, [%rd13];
	ld.global.nc.f32 	%f2, [%rd13+4];
	ld.global.nc.f32 	%f3, [%rd13+8];
	ld.global.nc.f32 	%f4, [%rd13+12];
	ld.global.nc.f32 	%f5, [%rd13+16];
	ld.global.nc.f32 	%f6, [%rd13+20];
	ld.global.nc.f32 	%f7, [%rd13+24];
	ld.global.nc.f32 	%f8, [%rd13+28];
	ld.global.nc.f32 	%f9, [%rd13+32];
	ld.global.nc.f32 	%f10, [%rd13+36];
	ld.global.nc.f32 	%f11, [%rd13+40];
	ld.global.nc.f32 	%f12, [%rd13+44];
	ld.global.nc.f32 	%f13, [%rd13+48];
	ld.global.nc.f32 	%f14, [%rd13+52];
	ld.global.nc.f32 	%f15, [%rd13+56];
	ld.global.nc.f32 	%f16, [%rd13+60];
	ld.global.nc.f32 	%f17, [%rd13+64];
	ld.global.nc.f32 	%f18, [%rd13+68];
	ld.global.nc.f32 	%f19, [%rd13+72];
	ld.global.nc.f32 	%f20, [%rd13+76];
	ld.global.nc.f32 	%f21, [%rd13+80];
	ld.global.nc.f32 	%f22, [%rd13+84];
	ld.global.nc.f32 	%f23, [%rd13+88];
	ld.global.nc.f32 	%f24, [%rd13+92];
	ld.global.nc.f32 	%f25, [%rd13+96];
	mad.lo.s32 	%r48, %r8, %r24, %r20;
	add.s32 	%r49, %r48, %r26;
	add.s32 	%r50, %r49, -2;
	mul.lo.s32 	%r51, %r7, %r50;
	mul.wide.s32 	%rd15, %r51, 4;
	add.s64 	%rd21, %rd14, %rd15;
	mad.lo.s32 	%r52, %r8, %r39, %r38;
	add.s32 	%r53, %r52, %r26;
	add.s32 	%r54, %r53, -2;
	mul.lo.s32 	%r55, %r7, %r54;
	mul.wide.s32 	%rd16, %r55, 4;
	add.s64 	%rd20, %rd14, %rd16;
	mad.lo.s32 	%r56, %r8, %r44, %r13;
	add.s32 	%r57, %r56, %r26;
	mul.lo.s32 	%r58, %r7, %r57;
	cvta.to.global.u64 	%rd17, %rd12;
	mul.wide.s32 	%rd18, %r58, 4;
	add.s64 	%rd19, %rd17, %rd18;
	mov.u32 	%r60, 0;
	not.pred 	%p15, %p1;
	not.pred 	%p17, %p3;

$L__BB76_2:
	mov.f32 	%f81, 0f00000000;
	@%p15 bra 	$L__BB76_4;

	ld.global.f32 	%f81, [%rd21];

$L__BB76_4:
	st.shared.f32 	[%r1], %f81;
	setp.gt.s32 	%p16, %r2, 399;
	@%p16 bra 	$L__BB76_8;

	@%p2 bra 	$L__BB76_7;
	bra.uni 	$L__BB76_6;

$L__BB76_7:
	ld.global.f32 	%f30, [%rd20];
	st.shared.f32 	[%r3], %f30;
	bra.uni 	$L__BB76_8;

$L__BB76_6:
	mov.u32 	%r59, 0;
	st.shared.u32 	[%r3], %r59;

$L__BB76_8:
	bar.sync 	0;
	ld.shared.f32 	%f31, [%r4];
	fma.rn.ftz.f32 	%f32, %f31, %f1, 0f00000000;
	ld.shared.f32 	%f33, [%r4+4];
	fma.rn.ftz.f32 	%f34, %f33, %f2, %f32;
	ld.shared.f32 	%f35, [%r4+8];
	fma.rn.ftz.f32 	%f36, %f35, %f3, %f34;
	ld.shared.f32 	%f37, [%r4+12];
	fma.rn.ftz.f32 	%f38, %f37, %f4, %f36;
	ld.shared.f32 	%f39, [%r4+16];
	fma.rn.ftz.f32 	%f40, %f39, %f5, %f38;
	ld.shared.f32 	%f41, [%r4+80];
	fma.rn.ftz.f32 	%f42, %f41, %f6, %f40;
	ld.shared.f32 	%f43, [%r4+84];
	fma.rn.ftz.f32 	%f44, %f43, %f7, %f42;
	ld.shared.f32 	%f45, [%r4+88];
	fma.rn.ftz.f32 	%f46, %f45, %f8, %f44;
	ld.shared.f32 	%f47, [%r4+92];
	fma.rn.ftz.f32 	%f48, %f47, %f9, %f46;
	ld.shared.f32 	%f49, [%r4+96];
	fma.rn.ftz.f32 	%f50, %f49, %f10, %f48;
	ld.shared.f32 	%f51, [%r4+160];
	fma.rn.ftz.f32 	%f52, %f51, %f11, %f50;
	ld.shared.f32 	%f53, [%r4+164];
	fma.rn.ftz.f32 	%f54, %f53, %f12, %f52;
	ld.shared.f32 	%f55, [%r4+168];
	fma.rn.ftz.f32 	%f56, %f55, %f13, %f54;
	ld.shared.f32 	%f57, [%r4+172];
	fma.rn.ftz.f32 	%f58, %f57, %f14, %f56;
	ld.shared.f32 	%f59, [%r4+176];
	fma.rn.ftz.f32 	%f60, %f59, %f15, %f58;
	ld.shared.f32 	%f61, [%r4+240];
	fma.rn.ftz.f32 	%f62, %f61, %f16, %f60;
	ld.shared.f32 	%f63, [%r4+244];
	fma.rn.ftz.f32 	%f64, %f63, %f17, %f62;
	ld.shared.f32 	%f65, [%r4+248];
	fma.rn.ftz.f32 	%f66, %f65, %f18, %f64;
	ld.shared.f32 	%f67, [%r4+252];
	fma.rn.ftz.f32 	%f68, %f67, %f19, %f66;
	ld.shared.f32 	%f69, [%r4+256];
	fma.rn.ftz.f32 	%f70, %f69, %f20, %f68;
	ld.shared.f32 	%f71, [%r4+320];
	fma.rn.ftz.f32 	%f72, %f71, %f21, %f70;
	ld.shared.f32 	%f73, [%r4+324];
	fma.rn.ftz.f32 	%f74, %f73, %f22, %f72;
	ld.shared.f32 	%f75, [%r4+328];
	fma.rn.ftz.f32 	%f76, %f75, %f23, %f74;
	ld.shared.f32 	%f77, [%r4+332];
	fma.rn.ftz.f32 	%f78, %f77, %f24, %f76;
	ld.shared.f32 	%f79, [%r4+336];
	fma.rn.ftz.f32 	%f28, %f79, %f25, %f78;
	@%p17 bra 	$L__BB76_10;

	cvt.ftz.f64.f32 	%fd1, %f28;
	cvt.sat.f64.f64 	%fd2, %fd1;
	cvt.rn.ftz.f32.f64 	%f80, %fd2;
	st.global.f32 	[%rd19], %f80;

$L__BB76_10:
	bar.sync 	0;
	add.s64 	%rd21, %rd21, 4;
	add.s64 	%rd20, %rd20, 4;
	add.s64 	%rd19, %rd19, 4;
	add.s32 	%r60, %r60, 1;
	setp.lt.s32 	%p18, %r60, %r7;
	@%p18 bra 	$L__BB76_2;

$L__BB76_11:
	ret;

}
	// .globl	Conv2D
.visible .entry Conv2D(
	.param .u64 Conv2D_param_0,
	.param .u64 Conv2D_param_1,
	.param .u64 Conv2D_param_2,
	.param .u32 Conv2D_param_3,
	.param .u32 Conv2D_param_4,
	.param .u32 Conv2D_param_5,
	.param .u32 Conv2D_param_6,
	.param .u32 Conv2D_param_7,
	.param .u32 Conv2D_param_8,
	.param .u32 Conv2D_param_9,
	.param .u32 Conv2D_param_10,
	.param .u32 Conv2D_param_11
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd14, [Conv2D_param_0];
	ld.param.u64 	%rd15, [Conv2D_param_1];
	ld.param.u64 	%rd13, [Conv2D_param_2];
	ld.param.u32 	%r21, [Conv2D_param_3];
	ld.param.u32 	%r22, [Conv2D_param_4];
	ld.param.u32 	%r23, [Conv2D_param_5];
	ld.param.u32 	%r24, [Conv2D_param_6];
	ld.param.u32 	%r28, [Conv2D_param_7];
	ld.param.u32 	%r25, [Conv2D_param_8];
	ld.param.u32 	%r26, [Conv2D_param_9];
	ld.param.u32 	%r29, [Conv2D_param_10];
	ld.param.u32 	%r27, [Conv2D_param_11];
	cvta.to.global.u64 	%rd1, %rd15;
	cvta.to.global.u64 	%rd2, %rd14;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r31, %ctaid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r1, %r31, %r30, %r32;
	mov.u32 	%r33, %ntid.y;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %tid.y;
	mad.lo.s32 	%r2, %r34, %r33, %r35;
	setp.ge.s32 	%p1, %r1, %r29;
	setp.ge.s32 	%p2, %r2, %r28;
	or.pred  	%p3, %p2, %p1;
	mov.f32 	%f37, 0f00000000;
	@%p3 bra 	$L__BB77_14;

	setp.lt.s32 	%p4, %r25, 1;
	@%p4 bra 	$L__BB77_13;

	setp.lt.s32 	%p5, %r26, 1;
	@%p5 bra 	$L__BB77_13;

	add.s32 	%r3, %r26, -1;
	and.b32  	%r4, %r26, 3;
	sub.s32 	%r5, %r26, %r4;
	add.s64 	%rd3, %rd2, 8;
	add.s64 	%rd4, %rd1, 8;
	mul.lo.s32 	%r37, %r1, %r22;
	sub.s32 	%r6, %r37, %r21;
	mul.lo.s32 	%r7, %r2, %r25;
	mov.f32 	%f37, 0f00000000;
	mov.u32 	%r40, 0;

$L__BB77_4:
	add.s32 	%r9, %r40, %r6;
	setp.lt.s32 	%p6, %r9, 0;
	setp.ge.s32 	%p7, %r9, %r23;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB77_12;

	setp.lt.u32 	%p9, %r3, 3;
	add.s32 	%r38, %r40, %r7;
	mul.lo.s32 	%r44, %r38, %r26;
	mul.lo.s32 	%r45, %r9, %r24;
	@%p9 bra 	$L__BB77_8;

	mul.wide.s32 	%rd16, %r45, 4;
	add.s64 	%rd24, %rd3, %rd16;
	mul.wide.s32 	%rd17, %r44, 4;
	add.s64 	%rd23, %rd4, %rd17;
	mov.u32 	%r43, %r5;

$L__BB77_7:
	ld.global.nc.f32 	%f15, [%rd23+-8];
	ld.global.f32 	%f16, [%rd24+-8];
	fma.rn.ftz.f32 	%f17, %f16, %f15, %f37;
	ld.global.nc.f32 	%f18, [%rd23+-4];
	ld.global.f32 	%f19, [%rd24+-4];
	fma.rn.ftz.f32 	%f20, %f19, %f18, %f17;
	ld.global.nc.f32 	%f21, [%rd23];
	ld.global.f32 	%f22, [%rd24];
	fma.rn.ftz.f32 	%f23, %f22, %f21, %f20;
	ld.global.nc.f32 	%f24, [%rd23+4];
	ld.global.f32 	%f25, [%rd24+4];
	fma.rn.ftz.f32 	%f37, %f25, %f24, %f23;
	add.s32 	%r45, %r45, 4;
	add.s32 	%r44, %r44, 4;
	add.s64 	%rd24, %rd24, 16;
	add.s64 	%rd23, %rd23, 16;
	add.s32 	%r43, %r43, -4;
	setp.ne.s32 	%p10, %r43, 0;
	@%p10 bra 	$L__BB77_7;

$L__BB77_8:
	setp.eq.s32 	%p11, %r4, 0;
	@%p11 bra 	$L__BB77_12;

	setp.eq.s32 	%p12, %r4, 1;
	mul.wide.s32 	%rd18, %r45, 4;
	add.s64 	%rd11, %rd2, %rd18;
	mul.wide.s32 	%rd19, %r44, 4;
	add.s64 	%rd12, %rd1, %rd19;
	ld.global.nc.f32 	%f26, [%rd12];
	ld.global.f32 	%f27, [%rd11];
	fma.rn.ftz.f32 	%f37, %f27, %f26, %f37;
	@%p12 bra 	$L__BB77_12;

	setp.eq.s32 	%p13, %r4, 2;
	ld.global.nc.f32 	%f28, [%rd12+4];
	ld.global.f32 	%f29, [%rd11+4];
	fma.rn.ftz.f32 	%f37, %f29, %f28, %f37;
	@%p13 bra 	$L__BB77_12;

	ld.global.nc.f32 	%f30, [%rd12+8];
	ld.global.f32 	%f31, [%rd11+8];
	fma.rn.ftz.f32 	%f37, %f31, %f30, %f37;

$L__BB77_12:
	add.s32 	%r40, %r40, 1;
	setp.lt.s32 	%p14, %r40, %r25;
	@%p14 bra 	$L__BB77_4;

$L__BB77_13:
	mad.lo.s32 	%r39, %r1, %r27, %r2;
	cvta.to.global.u64 	%rd20, %rd13;
	mul.wide.s32 	%rd21, %r39, 4;
	add.s64 	%rd22, %rd20, %rd21;
	st.global.f32 	[%rd22], %f37;

$L__BB77_14:
	ret;

}
	// .globl	TransposeConv2D
.visible .entry TransposeConv2D(
	.param .u64 TransposeConv2D_param_0,
	.param .u64 TransposeConv2D_param_1,
	.param .u64 TransposeConv2D_param_2,
	.param .u32 TransposeConv2D_param_3,
	.param .u32 TransposeConv2D_param_4,
	.param .u32 TransposeConv2D_param_5,
	.param .u32 TransposeConv2D_param_6,
	.param .u32 TransposeConv2D_param_7,
	.param .u32 TransposeConv2D_param_8,
	.param .u32 TransposeConv2D_param_9
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd10, [TransposeConv2D_param_0];
	ld.param.u64 	%rd11, [TransposeConv2D_param_1];
	ld.param.u64 	%rd9, [TransposeConv2D_param_2];
	ld.param.u32 	%r27, [TransposeConv2D_param_3];
	ld.param.u32 	%r28, [TransposeConv2D_param_4];
	ld.param.u32 	%r29, [TransposeConv2D_param_5];
	ld.param.u32 	%r30, [TransposeConv2D_param_6];
	ld.param.u32 	%r31, [TransposeConv2D_param_7];
	ld.param.u32 	%r32, [TransposeConv2D_param_8];
	ld.param.u32 	%r33, [TransposeConv2D_param_9];
	cvta.to.global.u64 	%rd1, %rd11;
	cvta.to.global.u64 	%rd2, %rd10;
	mov.u32 	%r34, %ntid.x;
	mov.u32 	%r35, %ctaid.x;
	mul.lo.s32 	%r1, %r35, %r34;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.u32 	%r36, %ntid.y;
	mov.u32 	%r37, %ctaid.y;
	mov.u32 	%r38, %tid.y;
	mad.lo.s32 	%r4, %r37, %r36, %r38;
	setp.ge.s32 	%p1, %r3, %r33;
	setp.ge.s32 	%p2, %r4, %r32;
	or.pred  	%p3, %p2, %p1;
	mov.f32 	%f37, 0f00000000;
	@%p3 bra 	$L__BB78_15;

	setp.lt.s32 	%p4, %r31, 1;
	@%p4 bra 	$L__BB78_14;

	setp.lt.s32 	%p5, %r30, 1;
	@%p5 bra 	$L__BB78_14;

	add.s32 	%r5, %r30, -1;
	mul.lo.s32 	%r40, %r32, %r31;
	shl.b32 	%r6, %r40, 2;
	add.s32 	%r7, %r31, -1;
	add.s32 	%r41, %r2, %r27;
	add.s32 	%r42, %r41, %r1;
	add.s32 	%r43, %r42, 1;
	sub.s32 	%r8, %r43, %r31;
	and.b32  	%r9, %r30, 3;
	sub.s32 	%r10, %r9, %r30;
	add.s64 	%rd3, %rd2, 8;
	mul.wide.s32 	%rd4, %r40, 4;
	add.s32 	%r44, %r27, %r3;
	add.s32 	%r45, %r44, 1;
	sub.s32 	%r11, %r45, %r31;
	mov.f32 	%f37, 0f00000000;
	mov.u32 	%r58, 0;

$L__BB78_4:
	add.s32 	%r13, %r11, %r58;
	setp.lt.s32 	%p6, %r13, 0;
	setp.ge.s32 	%p7, %r13, %r28;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB78_13;

	setp.lt.u32 	%p9, %r5, 3;
	mul.lo.s32 	%r62, %r13, %r29;
	mov.u32 	%r60, 0;
	@%p9 bra 	$L__BB78_9;

	sub.s32 	%r48, %r7, %r58;
	mad.lo.s32 	%r59, %r32, %r48, %r4;
	add.s32 	%r49, %r8, %r58;
	mul.lo.s32 	%r16, %r29, %r49;
	mul.wide.s32 	%rd12, %r16, 4;
	add.s64 	%rd28, %rd3, %rd12;

$L__BB78_7:
	mul.wide.s32 	%rd13, %r59, 4;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.nc.f32 	%f15, [%rd14];
	ld.global.nc.f32 	%f16, [%rd28+-8];
	fma.rn.ftz.f32 	%f17, %f16, %f15, %f37;
	add.s64 	%rd15, %rd14, %rd4;
	ld.global.nc.f32 	%f18, [%rd15];
	ld.global.nc.f32 	%f19, [%rd28+-4];
	fma.rn.ftz.f32 	%f20, %f19, %f18, %f17;
	add.s64 	%rd16, %rd15, %rd4;
	ld.global.nc.f32 	%f21, [%rd16];
	ld.global.nc.f32 	%f22, [%rd28];
	fma.rn.ftz.f32 	%f23, %f22, %f21, %f20;
	add.s64 	%rd17, %rd16, %rd4;
	ld.global.nc.f32 	%f24, [%rd17];
	ld.global.nc.f32 	%f25, [%rd28+4];
	fma.rn.ftz.f32 	%f37, %f25, %f24, %f23;
	add.s32 	%r59, %r59, %r6;
	add.s32 	%r60, %r60, 4;
	add.s32 	%r50, %r10, %r60;
	add.s64 	%rd28, %rd28, 16;
	setp.ne.s32 	%p10, %r50, 0;
	@%p10 bra 	$L__BB78_7;

	add.s32 	%r62, %r16, %r60;

$L__BB78_9:
	setp.eq.s32 	%p11, %r9, 0;
	@%p11 bra 	$L__BB78_13;

	setp.eq.s32 	%p12, %r9, 1;
	mul.wide.s32 	%rd18, %r62, 4;
	add.s64 	%rd8, %rd2, %rd18;
	not.b32 	%r51, %r58;
	add.s32 	%r52, %r51, %r31;
	mad.lo.s32 	%r24, %r60, %r31, %r52;
	mad.lo.s32 	%r53, %r24, %r32, %r4;
	mul.wide.s32 	%rd19, %r53, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.nc.f32 	%f26, [%rd20];
	ld.global.nc.f32 	%f27, [%rd8];
	fma.rn.ftz.f32 	%f37, %f27, %f26, %f37;
	@%p12 bra 	$L__BB78_13;

	setp.eq.s32 	%p13, %r9, 2;
	add.s32 	%r25, %r24, %r31;
	mad.lo.s32 	%r54, %r25, %r32, %r4;
	mul.wide.s32 	%rd21, %r54, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.f32 	%f28, [%rd22];
	ld.global.nc.f32 	%f29, [%rd8+4];
	fma.rn.ftz.f32 	%f37, %f29, %f28, %f37;
	@%p13 bra 	$L__BB78_13;

	add.s32 	%r55, %r25, %r31;
	mad.lo.s32 	%r56, %r55, %r32, %r4;
	mul.wide.s32 	%rd23, %r56, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.nc.f32 	%f30, [%rd24];
	ld.global.nc.f32 	%f31, [%rd8+8];
	fma.rn.ftz.f32 	%f37, %f31, %f30, %f37;

$L__BB78_13:
	add.s32 	%r58, %r58, 1;
	setp.lt.s32 	%p14, %r58, %r31;
	@%p14 bra 	$L__BB78_4;

$L__BB78_14:
	mad.lo.s32 	%r57, %r3, %r33, %r4;
	cvta.to.global.u64 	%rd25, %rd9;
	mul.wide.s32 	%rd26, %r57, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f37;

$L__BB78_15:
	ret;

}
	// .globl	Convolution
.visible .entry Convolution(
	.param .u64 Convolution_param_0,
	.param .u64 Convolution_param_1,
	.param .u64 Convolution_param_2,
	.param .u32 Convolution_param_3,
	.param .u32 Convolution_param_4,
	.param .u32 Convolution_param_5,
	.param .u32 Convolution_param_6,
	.param .u32 Convolution_param_7,
	.param .u32 Convolution_param_8,
	.param .u32 Convolution_param_9,
	.param .u32 Convolution_param_10,
	.param .u32 Convolution_param_11
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd14, [Convolution_param_0];
	ld.param.u64 	%rd13, [Convolution_param_1];
	ld.param.u64 	%rd15, [Convolution_param_2];
	ld.param.u32 	%r21, [Convolution_param_3];
	ld.param.u32 	%r22, [Convolution_param_4];
	ld.param.u32 	%r23, [Convolution_param_5];
	ld.param.u32 	%r24, [Convolution_param_6];
	ld.param.u32 	%r28, [Convolution_param_7];
	ld.param.u32 	%r25, [Convolution_param_8];
	ld.param.u32 	%r29, [Convolution_param_9];
	ld.param.u32 	%r26, [Convolution_param_10];
	ld.param.u32 	%r27, [Convolution_param_11];
	cvta.to.global.u64 	%rd1, %rd15;
	cvta.to.global.u64 	%rd2, %rd14;
	mov.u32 	%r30, %ntid.x;
	mov.u32 	%r31, %ctaid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r1, %r31, %r30, %r32;
	mov.u32 	%r33, %ntid.y;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %tid.y;
	mad.lo.s32 	%r2, %r34, %r33, %r35;
	setp.ge.s32 	%p1, %r1, %r28;
	setp.ge.s32 	%p2, %r2, %r29;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB79_13;

	setp.lt.s32 	%p4, %r26, 1;
	@%p4 bra 	$L__BB79_13;

	setp.lt.s32 	%p5, %r27, 1;
	@%p5 bra 	$L__BB79_13;

	add.s32 	%r3, %r27, -1;
	and.b32  	%r4, %r27, 3;
	sub.s32 	%r5, %r27, %r4;
	add.s64 	%rd3, %rd2, 8;
	mul.lo.s32 	%r37, %r1, %r22;
	sub.s32 	%r6, %r37, %r21;
	cvta.to.global.u64 	%rd16, %rd13;
	mad.lo.s32 	%r38, %r1, %r25, %r2;
	mul.wide.s32 	%rd17, %r38, 4;
	add.s64 	%rd4, %rd16, %rd17;
	mul.lo.s32 	%r7, %r2, %r26;
	mov.u32 	%r40, 0;

$L__BB79_4:
	add.s32 	%r9, %r40, %r6;
	setp.lt.s32 	%p6, %r9, 0;
	setp.ge.s32 	%p7, %r9, %r23;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB79_12;

	setp.lt.u32 	%p9, %r3, 3;
	mul.lo.s32 	%r44, %r9, %r24;
	add.s32 	%r39, %r40, %r7;
	mul.lo.s32 	%r45, %r39, %r27;
	ld.global.nc.f32 	%f1, [%rd4];
	@%p9 bra 	$L__BB79_8;

	mul.wide.s32 	%rd18, %r45, 4;
	add.s64 	%rd28, %rd1, %rd18;
	mul.wide.s32 	%rd19, %r44, 4;
	add.s64 	%rd27, %rd3, %rd19;
	mov.u32 	%r43, %r5;

$L__BB79_7:
	ld.global.nc.f32 	%f2, [%rd27+-8];
	mul.ftz.f32 	%f3, %f2, %f1;
	atom.global.add.f32 	%f4, [%rd28], %f3;
	ld.global.nc.f32 	%f5, [%rd27+-4];
	mul.ftz.f32 	%f6, %f5, %f1;
	add.s64 	%rd20, %rd28, 4;
	atom.global.add.f32 	%f7, [%rd20], %f6;
	ld.global.nc.f32 	%f8, [%rd27];
	mul.ftz.f32 	%f9, %f8, %f1;
	add.s64 	%rd21, %rd28, 8;
	atom.global.add.f32 	%f10, [%rd21], %f9;
	ld.global.nc.f32 	%f11, [%rd27+4];
	mul.ftz.f32 	%f12, %f11, %f1;
	add.s64 	%rd22, %rd28, 12;
	atom.global.add.f32 	%f13, [%rd22], %f12;
	add.s32 	%r44, %r44, 4;
	add.s32 	%r45, %r45, 4;
	add.s64 	%rd28, %rd28, 16;
	add.s64 	%rd27, %rd27, 16;
	add.s32 	%r43, %r43, -4;
	setp.ne.s32 	%p10, %r43, 0;
	@%p10 bra 	$L__BB79_7;

$L__BB79_8:
	setp.eq.s32 	%p11, %r4, 0;
	@%p11 bra 	$L__BB79_12;

	setp.eq.s32 	%p12, %r4, 1;
	mul.wide.s32 	%rd23, %r45, 4;
	add.s64 	%rd11, %rd1, %rd23;
	mul.wide.s32 	%rd24, %r44, 4;
	add.s64 	%rd12, %rd2, %rd24;
	ld.global.nc.f32 	%f14, [%rd12];
	mul.ftz.f32 	%f15, %f14, %f1;
	atom.global.add.f32 	%f16, [%rd11], %f15;
	@%p12 bra 	$L__BB79_12;

	setp.eq.s32 	%p13, %r4, 2;
	ld.global.nc.f32 	%f17, [%rd12+4];
	mul.ftz.f32 	%f18, %f17, %f1;
	add.s64 	%rd25, %rd11, 4;
	atom.global.add.f32 	%f19, [%rd25], %f18;
	@%p13 bra 	$L__BB79_12;

	add.s64 	%rd26, %rd11, 8;
	ld.global.nc.f32 	%f20, [%rd12+8];
	mul.ftz.f32 	%f21, %f20, %f1;
	atom.global.add.f32 	%f22, [%rd26], %f21;

$L__BB79_12:
	add.s32 	%r40, %r40, 1;
	setp.lt.s32 	%p14, %r40, %r26;
	@%p14 bra 	$L__BB79_4;

$L__BB79_13:
	ret;

}
	// .globl	derSoftmax
.visible .entry derSoftmax(
	.param .u64 derSoftmax_param_0,
	.param .u64 derSoftmax_param_1,
	.param .u64 derSoftmax_param_2,
	.param .u32 derSoftmax_param_3,
	.param .u32 derSoftmax_param_4
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<101>;
	.reg .b32 	%r<94>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd35, [derSoftmax_param_0];
	ld.param.u64 	%rd36, [derSoftmax_param_1];
	ld.param.u64 	%rd34, [derSoftmax_param_2];
	ld.param.u32 	%r42, [derSoftmax_param_3];
	ld.param.u32 	%r41, [derSoftmax_param_4];
	cvta.to.global.u64 	%rd1, %rd36;
	cvta.to.global.u64 	%rd2, %rd35;
	mov.u32 	%r43, %ctaid.x;
	mov.u32 	%r44, %ntid.x;
	mov.u32 	%r45, %tid.x;
	mad.lo.s32 	%r1, %r44, %r43, %r45;
	mov.u32 	%r46, %ctaid.y;
	mov.u32 	%r47, %ntid.y;
	mul.lo.s32 	%r2, %r47, %r46;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r42;
	setp.ge.s32 	%p2, %r4, %r41;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB80_23;

	cvta.to.global.u64 	%rd37, %rd34;
	mul.lo.s32 	%r5, %r1, %r41;
	add.s32 	%r49, %r5, %r4;
	mul.wide.s32 	%rd38, %r49, 4;
	add.s64 	%rd3, %rd37, %rd38;
	mov.f32 	%f95, 0f00000000;
	mov.u32 	%r89, 0;
	st.global.u32 	[%rd3], %r89;
	add.s64 	%rd39, %rd2, %rd38;
	ld.global.nc.f32 	%f1, [%rd39];
	max.s32 	%r6, %r4, 0;
	min.s32 	%r7, %r6, %r41;
	setp.lt.s32 	%p4, %r7, 1;
	@%p4 bra 	$L__BB80_8;

	add.s32 	%r52, %r7, -1;
	and.b32  	%r82, %r7, 3;
	setp.lt.u32 	%p5, %r52, 3;
	mov.f32 	%f95, 0f00000000;
	mov.u32 	%r89, 0;
	@%p5 bra 	$L__BB80_5;

	not.b32 	%r54, %r6;
	not.b32 	%r55, %r41;
	max.s32 	%r56, %r54, %r55;
	add.s32 	%r57, %r56, %r82;
	neg.s32 	%r77, %r57;
	mul.wide.s32 	%rd40, %r5, 4;
	add.s64 	%rd41, %rd40, 8;
	add.s64 	%rd50, %rd2, %rd41;
	add.s64 	%rd49, %rd1, %rd41;

$L__BB80_4:
	ld.global.nc.f32 	%f28, [%rd50+-8];
	mul.ftz.f32 	%f29, %f28, %f1;
	ld.global.nc.f32 	%f30, [%rd49+-8];
	mul.ftz.f32 	%f31, %f29, %f30;
	sub.ftz.f32 	%f32, %f95, %f31;
	ld.global.nc.f32 	%f33, [%rd50+-4];
	mul.ftz.f32 	%f34, %f33, %f1;
	ld.global.nc.f32 	%f35, [%rd49+-4];
	mul.ftz.f32 	%f36, %f34, %f35;
	sub.ftz.f32 	%f37, %f32, %f36;
	ld.global.nc.f32 	%f38, [%rd50];
	mul.ftz.f32 	%f39, %f38, %f1;
	ld.global.nc.f32 	%f40, [%rd49];
	mul.ftz.f32 	%f41, %f39, %f40;
	sub.ftz.f32 	%f42, %f37, %f41;
	ld.global.nc.f32 	%f43, [%rd50+4];
	mul.ftz.f32 	%f44, %f43, %f1;
	ld.global.nc.f32 	%f45, [%rd49+4];
	mul.ftz.f32 	%f46, %f44, %f45;
	sub.ftz.f32 	%f95, %f42, %f46;
	add.s32 	%r89, %r89, 4;
	add.s64 	%rd50, %rd50, 16;
	add.s64 	%rd49, %rd49, 16;
	add.s32 	%r77, %r77, -4;
	setp.ne.s32 	%p6, %r77, 1;
	@%p6 bra 	$L__BB80_4;

$L__BB80_5:
	setp.eq.s32 	%p7, %r82, 0;
	@%p7 bra 	$L__BB80_8;

	add.s32 	%r58, %r89, %r5;
	mul.wide.s32 	%rd42, %r58, 4;
	add.s64 	%rd52, %rd1, %rd42;
	add.s64 	%rd51, %rd2, %rd42;

$L__BB80_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f47, [%rd51];
	mul.ftz.f32 	%f48, %f47, %f1;
	ld.global.nc.f32 	%f49, [%rd52];
	mul.ftz.f32 	%f50, %f48, %f49;
	sub.ftz.f32 	%f95, %f95, %f50;
	add.s32 	%r89, %r89, 1;
	add.s64 	%rd52, %rd52, 4;
	add.s64 	%rd51, %rd51, 4;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p8, %r82, 0;
	@%p8 bra 	$L__BB80_7;

$L__BB80_8:
	add.s32 	%r59, %r4, 1;
	min.s32 	%r21, %r59, %r41;
	setp.ge.s32 	%p9, %r89, %r21;
	@%p9 bra 	$L__BB80_15;

	mov.f32 	%f52, 0f3F800000;
	sub.ftz.f32 	%f53, %f52, %f1;
	mul.ftz.f32 	%f9, %f1, %f53;
	mov.u32 	%r61, -2;
	sub.s32 	%r62, %r61, %r2;
	sub.s32 	%r63, %r62, %r3;
	not.b32 	%r64, %r41;
	max.s32 	%r22, %r63, %r64;
	not.b32 	%r65, %r89;
	sub.s32 	%r66, %r65, %r22;
	and.b32  	%r85, %r66, 3;
	setp.eq.s32 	%p10, %r85, 0;
	mov.u32 	%r86, %r89;
	@%p10 bra 	$L__BB80_12;

	add.s32 	%r67, %r89, %r5;
	mul.wide.s32 	%rd43, %r67, 4;
	add.s64 	%rd53, %rd1, %rd43;
	mov.u32 	%r86, %r89;

$L__BB80_11:
	.pragma "nounroll";
	ld.global.nc.f32 	%f54, [%rd53];
	fma.rn.ftz.f32 	%f95, %f54, %f9, %f95;
	add.s32 	%r86, %r86, 1;
	add.s64 	%rd53, %rd53, 4;
	add.s32 	%r85, %r85, -1;
	setp.ne.s32 	%p11, %r85, 0;
	@%p11 bra 	$L__BB80_11;

$L__BB80_12:
	sub.s32 	%r69, %r61, %r89;
	sub.s32 	%r70, %r69, %r22;
	setp.lt.u32 	%p12, %r70, 3;
	mov.u32 	%r89, %r86;
	@%p12 bra 	$L__BB80_15;

	add.s32 	%r71, %r86, %r5;
	mul.wide.s32 	%rd44, %r71, 4;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd54, %rd45, 8;
	mov.u32 	%r89, %r86;

$L__BB80_14:
	ld.global.nc.f32 	%f55, [%rd54+-8];
	fma.rn.ftz.f32 	%f56, %f55, %f9, %f95;
	ld.global.nc.f32 	%f57, [%rd54+-4];
	fma.rn.ftz.f32 	%f58, %f57, %f9, %f56;
	ld.global.nc.f32 	%f59, [%rd54];
	fma.rn.ftz.f32 	%f60, %f59, %f9, %f58;
	ld.global.nc.f32 	%f61, [%rd54+4];
	fma.rn.ftz.f32 	%f95, %f61, %f9, %f60;
	add.s64 	%rd54, %rd54, 16;
	add.s32 	%r89, %r89, 4;
	setp.lt.s32 	%p13, %r89, %r21;
	@%p13 bra 	$L__BB80_14;

$L__BB80_15:
	setp.ge.s32 	%p14, %r89, %r41;
	@%p14 bra 	$L__BB80_22;

	sub.s32 	%r72, %r41, %r89;
	and.b32  	%r91, %r72, 3;
	setp.eq.s32 	%p15, %r91, 0;
	mov.u32 	%r92, %r89;
	@%p15 bra 	$L__BB80_19;

	add.s32 	%r73, %r89, %r5;
	mul.wide.s32 	%rd46, %r73, 4;
	add.s64 	%rd56, %rd1, %rd46;
	add.s64 	%rd55, %rd2, %rd46;
	mov.u32 	%r92, %r89;

$L__BB80_18:
	.pragma "nounroll";
	ld.global.nc.f32 	%f63, [%rd55];
	mul.ftz.f32 	%f64, %f63, %f1;
	ld.global.nc.f32 	%f65, [%rd56];
	mul.ftz.f32 	%f66, %f64, %f65;
	sub.ftz.f32 	%f95, %f95, %f66;
	add.s32 	%r92, %r92, 1;
	add.s64 	%rd56, %rd56, 4;
	add.s64 	%rd55, %rd55, 4;
	add.s32 	%r91, %r91, -1;
	setp.ne.s32 	%p16, %r91, 0;
	@%p16 bra 	$L__BB80_18;

$L__BB80_19:
	not.b32 	%r74, %r89;
	add.s32 	%r75, %r74, %r41;
	setp.lt.u32 	%p17, %r75, 3;
	@%p17 bra 	$L__BB80_22;

	add.s32 	%r76, %r92, %r5;
	mul.wide.s32 	%rd47, %r76, 4;
	add.s64 	%rd48, %rd47, 8;
	add.s64 	%rd58, %rd1, %rd48;
	add.s64 	%rd57, %rd2, %rd48;

$L__BB80_21:
	ld.global.nc.f32 	%f67, [%rd57+-8];
	mul.ftz.f32 	%f68, %f67, %f1;
	ld.global.nc.f32 	%f69, [%rd58+-8];
	mul.ftz.f32 	%f70, %f68, %f69;
	sub.ftz.f32 	%f71, %f95, %f70;
	ld.global.nc.f32 	%f72, [%rd57+-4];
	mul.ftz.f32 	%f73, %f72, %f1;
	ld.global.nc.f32 	%f74, [%rd58+-4];
	mul.ftz.f32 	%f75, %f73, %f74;
	sub.ftz.f32 	%f76, %f71, %f75;
	ld.global.nc.f32 	%f77, [%rd57];
	mul.ftz.f32 	%f78, %f77, %f1;
	ld.global.nc.f32 	%f79, [%rd58];
	mul.ftz.f32 	%f80, %f78, %f79;
	sub.ftz.f32 	%f81, %f76, %f80;
	ld.global.nc.f32 	%f82, [%rd57+4];
	mul.ftz.f32 	%f83, %f82, %f1;
	ld.global.nc.f32 	%f84, [%rd58+4];
	mul.ftz.f32 	%f85, %f83, %f84;
	sub.ftz.f32 	%f95, %f81, %f85;
	add.s64 	%rd58, %rd58, 16;
	add.s64 	%rd57, %rd57, 16;
	add.s32 	%r92, %r92, 4;
	setp.lt.s32 	%p18, %r92, %r41;
	@%p18 bra 	$L__BB80_21;

$L__BB80_22:
	st.global.f32 	[%rd3], %f95;

$L__BB80_23:
	ret;

}
	// .globl	derSoftmax_TYPE
.visible .entry derSoftmax_TYPE(
	.param .u64 derSoftmax_TYPE_param_0,
	.param .u64 derSoftmax_TYPE_param_1,
	.param .u64 derSoftmax_TYPE_param_2,
	.param .u32 derSoftmax_TYPE_param_3,
	.param .u32 derSoftmax_TYPE_param_4
)
{
	.reg .pred 	%p<65>;
	.reg .b16 	%rs<80>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<76>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd13, [derSoftmax_TYPE_param_0];
	ld.param.u64 	%rd14, [derSoftmax_TYPE_param_1];
	ld.param.u64 	%rd12, [derSoftmax_TYPE_param_2];
	ld.param.u32 	%r25, [derSoftmax_TYPE_param_3];
	ld.param.u32 	%r24, [derSoftmax_TYPE_param_4];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r26, %ctaid.x;
	mov.u32 	%r27, %ntid.x;
	mov.u32 	%r28, %tid.x;
	mad.lo.s32 	%r1, %r27, %r26, %r28;
	mov.u32 	%r29, %ctaid.y;
	mov.u32 	%r30, %ntid.y;
	mov.u32 	%r31, %tid.y;
	mad.lo.s32 	%r2, %r30, %r29, %r31;
	setp.ge.s32 	%p1, %r1, %r25;
	setp.ge.s32 	%p2, %r2, %r24;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB81_37;

	cvta.to.global.u64 	%rd15, %rd12;
	ld.const.u16 	%rs71, [sh];
	mul.lo.s32 	%r3, %r1, %r24;
	add.s32 	%r32, %r3, %r2;
	mul.wide.s32 	%rd16, %r32, 2;
	add.s64 	%rd3, %rd15, %rd16;
	st.global.u16 	[%rd3], %rs71;
	add.s64 	%rd17, %rd2, %rd16;
	ld.global.nc.u16 	%rs2, [%rd17];
	ld.const.u16 	%rs3, [sh+8];
	setp.lt.s32 	%p4, %r24, 1;
	@%p4 bra 	$L__BB81_36;

	and.b32  	%r4, %r24, 1;
	setp.eq.s32 	%p5, %r24, 1;
	mov.u32 	%r75, 0;
	@%p5 bra 	$L__BB81_25;

	sub.s32 	%r74, %r24, %r4;
	neg.s32 	%r72, %r2;
	mul.wide.s32 	%rd18, %r3, 2;
	add.s64 	%rd19, %rd18, 2;
	add.s64 	%rd25, %rd1, %rd19;
	add.s64 	%rd24, %rd2, %rd19;
	// begin inline asm
	{ mov.b32 %f4, {0,%rs2};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f8, {0,%rs3};}

	// end inline asm
	sub.ftz.f32 	%f10, %f8, %f4;
	mul.ftz.f32 	%f11, %f4, %f10;

$L__BB81_4:
	add.s64 	%rd8, %rd24, -2;
	setp.eq.s32 	%p6, %r72, 0;
	@%p6 bra 	$L__BB81_8;

	ld.global.nc.u16 	%rs33, [%rd8];
	// begin inline asm
	{ mov.b32 %f5, {0,%rs33};}

	// end inline asm
	mul.ftz.f32 	%f6, %f4, %f5;
	neg.ftz.f32 	%f7, %f6;
	mov.b32 	%r35, %f7;
	and.b32  	%r36, %r35, 2147483647;
	setp.gt.u32 	%p7, %r36, 2139095040;
	shl.b32 	%r37, %r35, 16;
	shr.u32 	%r38, %r35, 16;
	cvt.u16.u32 	%rs34, %r38;
	selp.b32 	%r10, 0, %r37, %p7;
	selp.b16 	%rs72, 32767, %rs34, %p7;
	setp.gt.u32 	%p8, %r10, -2147483648;
	@%p8 bra 	$L__BB81_7;

	setp.ne.s32 	%p9, %r10, -2147483648;
	and.b16  	%rs35, %rs72, 1;
	setp.eq.b16 	%p10, %rs35, 1;
	not.pred 	%p11, %p10;
	or.pred  	%p12, %p9, %p11;
	@%p12 bra 	$L__BB81_11;

$L__BB81_7:
	add.s16 	%rs72, %rs72, 1;
	bra.uni 	$L__BB81_11;

$L__BB81_8:
	mov.b32 	%r39, %f11;
	and.b32  	%r40, %r39, 2147483647;
	setp.gt.u32 	%p13, %r40, 2139095040;
	shl.b32 	%r41, %r39, 16;
	shr.u32 	%r42, %r39, 16;
	cvt.u16.u32 	%rs38, %r42;
	selp.b32 	%r11, 0, %r41, %p13;
	selp.b16 	%rs72, 32767, %rs38, %p13;
	setp.gt.u32 	%p14, %r11, -2147483648;
	@%p14 bra 	$L__BB81_10;

	setp.ne.s32 	%p15, %r11, -2147483648;
	and.b16  	%rs39, %rs72, 1;
	setp.eq.b16 	%p16, %rs39, 1;
	not.pred 	%p17, %p16;
	or.pred  	%p18, %p15, %p17;
	@%p18 bra 	$L__BB81_11;

$L__BB81_10:
	add.s16 	%rs72, %rs72, 1;

$L__BB81_11:
	// begin inline asm
	{ mov.b32 %f12, {0,%rs71};}

	// end inline asm
	add.s64 	%rd9, %rd25, -2;
	ld.global.nc.u16 	%rs41, [%rd25+-2];
	// begin inline asm
	{ mov.b32 %f13, {0,%rs41};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f14, {0,%rs72};}

	// end inline asm
	fma.rn.ftz.f32 	%f15, %f13, %f14, %f12;
	mov.b32 	%r43, %f15;
	and.b32  	%r44, %r43, 2147483647;
	setp.gt.u32 	%p19, %r44, 2139095040;
	shl.b32 	%r45, %r43, 16;
	shr.u32 	%r46, %r43, 16;
	cvt.u16.u32 	%rs43, %r46;
	selp.b32 	%r12, 0, %r45, %p19;
	selp.b16 	%rs73, 32767, %rs43, %p19;
	setp.gt.u32 	%p20, %r12, -2147483648;
	@%p20 bra 	$L__BB81_13;

	setp.ne.s32 	%p21, %r12, -2147483648;
	and.b16  	%rs44, %rs73, 1;
	setp.eq.b16 	%p22, %rs44, 1;
	not.pred 	%p23, %p22;
	or.pred  	%p24, %p21, %p23;
	@%p24 bra 	$L__BB81_14;

$L__BB81_13:
	add.s16 	%rs73, %rs73, 1;

$L__BB81_14:
	add.s32 	%r47, %r75, 1;
	setp.eq.s32 	%p25, %r2, %r47;
	@%p25 bra 	$L__BB81_18;
	bra.uni 	$L__BB81_15;

$L__BB81_18:
	mov.b32 	%r52, %f11;
	and.b32  	%r53, %r52, 2147483647;
	setp.gt.u32 	%p32, %r53, 2139095040;
	shl.b32 	%r54, %r52, 16;
	shr.u32 	%r55, %r52, 16;
	cvt.u16.u32 	%rs51, %r55;
	selp.b32 	%r14, 0, %r54, %p32;
	selp.b16 	%rs74, 32767, %rs51, %p32;
	setp.gt.u32 	%p33, %r14, -2147483648;
	@%p33 bra 	$L__BB81_20;

	setp.ne.s32 	%p34, %r14, -2147483648;
	and.b16  	%rs52, %rs74, 1;
	setp.eq.b16 	%p35, %rs52, 1;
	not.pred 	%p36, %p35;
	or.pred  	%p37, %p34, %p36;
	@%p37 bra 	$L__BB81_21;

$L__BB81_20:
	add.s16 	%rs74, %rs74, 1;
	bra.uni 	$L__BB81_21;

$L__BB81_15:
	ld.global.nc.u16 	%rs46, [%rd8+2];
	// begin inline asm
	{ mov.b32 %f17, {0,%rs46};}

	// end inline asm
	mul.ftz.f32 	%f18, %f4, %f17;
	neg.ftz.f32 	%f19, %f18;
	mov.b32 	%r48, %f19;
	and.b32  	%r49, %r48, 2147483647;
	setp.gt.u32 	%p26, %r49, 2139095040;
	shl.b32 	%r50, %r48, 16;
	shr.u32 	%r51, %r48, 16;
	cvt.u16.u32 	%rs47, %r51;
	selp.b32 	%r13, 0, %r50, %p26;
	selp.b16 	%rs74, 32767, %rs47, %p26;
	setp.gt.u32 	%p27, %r13, -2147483648;
	@%p27 bra 	$L__BB81_17;

	setp.ne.s32 	%p28, %r13, -2147483648;
	and.b16  	%rs48, %rs74, 1;
	setp.eq.b16 	%p29, %rs48, 1;
	not.pred 	%p30, %p29;
	or.pred  	%p31, %p28, %p30;
	@%p31 bra 	$L__BB81_21;

$L__BB81_17:
	add.s16 	%rs74, %rs74, 1;

$L__BB81_21:
	// begin inline asm
	{ mov.b32 %f24, {0,%rs73};}

	// end inline asm
	ld.global.nc.u16 	%rs54, [%rd9+2];
	// begin inline asm
	{ mov.b32 %f25, {0,%rs54};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f26, {0,%rs74};}

	// end inline asm
	fma.rn.ftz.f32 	%f27, %f25, %f26, %f24;
	mov.b32 	%r56, %f27;
	and.b32  	%r57, %r56, 2147483647;
	setp.gt.u32 	%p38, %r57, 2139095040;
	shl.b32 	%r58, %r56, 16;
	shr.u32 	%r59, %r56, 16;
	cvt.u16.u32 	%rs56, %r59;
	selp.b32 	%r15, 0, %r58, %p38;
	selp.b16 	%rs71, 32767, %rs56, %p38;
	setp.gt.u32 	%p39, %r15, -2147483648;
	@%p39 bra 	$L__BB81_23;

	setp.ne.s32 	%p40, %r15, -2147483648;
	and.b16  	%rs57, %rs71, 1;
	setp.eq.b16 	%p41, %rs57, 1;
	not.pred 	%p42, %p41;
	or.pred  	%p43, %p40, %p42;
	@%p43 bra 	$L__BB81_24;

$L__BB81_23:
	add.s16 	%rs71, %rs71, 1;

$L__BB81_24:
	add.s32 	%r75, %r75, 2;
	add.s32 	%r72, %r72, 2;
	add.s64 	%rd25, %rd25, 4;
	add.s64 	%rd24, %rd24, 4;
	add.s32 	%r74, %r74, -2;
	setp.ne.s32 	%p44, %r74, 0;
	@%p44 bra 	$L__BB81_4;

$L__BB81_25:
	setp.eq.s32 	%p45, %r4, 0;
	@%p45 bra 	$L__BB81_36;

	add.s32 	%r20, %r75, %r3;
	// begin inline asm
	{ mov.b32 %f28, {0,%rs2};}

	// end inline asm
	setp.eq.s32 	%p46, %r2, %r75;
	@%p46 bra 	$L__BB81_30;
	bra.uni 	$L__BB81_27;

$L__BB81_30:
	// begin inline asm
	{ mov.b32 %f32, {0,%rs3};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f33, {0,%rs2};}

	// end inline asm
	sub.ftz.f32 	%f34, %f32, %f33;
	mul.ftz.f32 	%f35, %f28, %f34;
	mov.b32 	%r64, %f35;
	and.b32  	%r65, %r64, 2147483647;
	setp.gt.u32 	%p53, %r65, 2139095040;
	shl.b32 	%r66, %r64, 16;
	shr.u32 	%r67, %r64, 16;
	cvt.u16.u32 	%rs64, %r67;
	selp.b32 	%r22, 0, %r66, %p53;
	selp.b16 	%rs78, 32767, %rs64, %p53;
	setp.gt.u32 	%p54, %r22, -2147483648;
	@%p54 bra 	$L__BB81_32;

	setp.ne.s32 	%p55, %r22, -2147483648;
	and.b16  	%rs65, %rs78, 1;
	setp.eq.b16 	%p56, %rs65, 1;
	not.pred 	%p57, %p56;
	or.pred  	%p58, %p55, %p57;
	@%p58 bra 	$L__BB81_33;

$L__BB81_32:
	add.s16 	%rs78, %rs78, 1;
	bra.uni 	$L__BB81_33;

$L__BB81_27:
	mul.wide.s32 	%rd20, %r20, 2;
	add.s64 	%rd21, %rd2, %rd20;
	ld.global.nc.u16 	%rs59, [%rd21];
	// begin inline asm
	{ mov.b32 %f29, {0,%rs59};}

	// end inline asm
	mul.ftz.f32 	%f30, %f28, %f29;
	neg.ftz.f32 	%f31, %f30;
	mov.b32 	%r60, %f31;
	and.b32  	%r61, %r60, 2147483647;
	setp.gt.u32 	%p47, %r61, 2139095040;
	shl.b32 	%r62, %r60, 16;
	shr.u32 	%r63, %r60, 16;
	cvt.u16.u32 	%rs60, %r63;
	selp.b32 	%r21, 0, %r62, %p47;
	selp.b16 	%rs78, 32767, %rs60, %p47;
	setp.gt.u32 	%p48, %r21, -2147483648;
	@%p48 bra 	$L__BB81_29;

	setp.ne.s32 	%p49, %r21, -2147483648;
	and.b16  	%rs61, %rs78, 1;
	setp.eq.b16 	%p50, %rs61, 1;
	not.pred 	%p51, %p50;
	or.pred  	%p52, %p49, %p51;
	@%p52 bra 	$L__BB81_33;

$L__BB81_29:
	add.s16 	%rs78, %rs78, 1;

$L__BB81_33:
	// begin inline asm
	{ mov.b32 %f36, {0,%rs71};}

	// end inline asm
	mul.wide.s32 	%rd22, %r20, 2;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.u16 	%rs67, [%rd23];
	// begin inline asm
	{ mov.b32 %f37, {0,%rs67};}

	// end inline asm
	// begin inline asm
	{ mov.b32 %f38, {0,%rs78};}

	// end inline asm
	fma.rn.ftz.f32 	%f39, %f37, %f38, %f36;
	mov.b32 	%r68, %f39;
	and.b32  	%r69, %r68, 2147483647;
	setp.gt.u32 	%p59, %r69, 2139095040;
	shl.b32 	%r70, %r68, 16;
	shr.u32 	%r71, %r68, 16;
	cvt.u16.u32 	%rs69, %r71;
	selp.b32 	%r23, 0, %r70, %p59;
	selp.b16 	%rs71, 32767, %rs69, %p59;
	setp.gt.u32 	%p60, %r23, -2147483648;
	@%p60 bra 	$L__BB81_35;

	setp.ne.s32 	%p61, %r23, -2147483648;
	and.b16  	%rs70, %rs71, 1;
	setp.eq.b16 	%p62, %rs70, 1;
	not.pred 	%p63, %p62;
	or.pred  	%p64, %p61, %p63;
	@%p64 bra 	$L__BB81_36;

$L__BB81_35:
	add.s16 	%rs71, %rs71, 1;

$L__BB81_36:
	st.global.u16 	[%rd3], %rs71;

$L__BB81_37:
	ret;

}

